{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Logistic Regression\n",
    "\n",
    "_Authors: Kiefer Katovich, Matt Brems, Noelle Brown_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Distinguish between regression and classification problems.\n",
    "- Understand how logistic regression is similar to and different from linear regression.\n",
    "- Fit, generate predictions from, and evaluate a logistic regression model in `sklearn`.\n",
    "- Understand how to interpret the coefficients of logistic regression.\n",
    "- Know the benefits of logistic regression as a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "---\n",
    "\n",
    "Logistic regression is a natural **bridge** to connect _regression and classification_.\n",
    "- Logistic regression is the most common **binary classification** algorithm.\n",
    "- Because it is a regression model, logistic regression will predict continuous values.\n",
    "    - Logistic regression will **predict continuous probabilities between 0 and 1**.\n",
    "    - Example: What is the probability that someone shows up to vote?\n",
    "- However, logistic regression almost always operates as a classification model.\n",
    "    - Logistic regression will **use these continuous predictions (/probabilities) to classify the predicted response as 0 or 1** _(with reference to a threshold probability, like 0.5 mid-point)_.\n",
    "    - Example: Based on the predicted probability, do we predict that someone votes?\n",
    "\n",
    "In this lecture, we'll only be reviewing the binary outcome case with two classes, but logistic regression can be generalized to predicting outcomes with 3 or more classes.\n",
    "\n",
    "**Some examples of when logistic regression could be used:**\n",
    "- Will a user will purchase a product, given characteristics like income, age, and number of family members?\n",
    "- Does this patient have a specific disease based on their symptoms?\n",
    "- Will a person default on their loan?\n",
    "- Given a certain set of employee demographics, can we predict if they will leave the company?\n",
    "- Given one's GPA and the prestige of a college, will a student be admitted to a specific graduate program?\n",
    "\n",
    "And many more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Import sklearn train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import sklearn logistic regression\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graduate School Admissions\n",
    "\n",
    "---\n",
    "\n",
    "Today, we'll be applying logistic regression to solve the following problem: **\"Given one's GPA, will a student be admitted to a specific graduate program?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read in the data.\n",
    "admissions = pd.read_csv('../data/grad_admissions.csv')\n",
    "print(admissions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>2.915018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660.0</td>\n",
       "      <td>4.044540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800.0</td>\n",
       "      <td>4.950714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640.0</td>\n",
       "      <td>3.921994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>2.069878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit    gre       gpa\n",
       "0      0  380.0  2.915018\n",
       "1      1  660.0  4.044540\n",
       "2      1  800.0  4.950714\n",
       "3      1  640.0  3.921994\n",
       "4      0  520.0  2.069878"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check first five rows.\n",
    "admissions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are:\n",
    "- `admit`: A binary 0/1 variable indicating whether or not a student was admitted, where **1 means admitted and 0 means not admitted**.\n",
    "- `gre`: The student's [GRE (Graduate Record Exam)](https://en.wikipedia.org/wiki/Graduate_Record_Examinations) score.\n",
    "- `gpa`: The student's GPA.\n",
    "\n",
    "In this classificatiom machine learning task, `admit` will be our response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "admit     0\n",
       "gre      20\n",
       "gpa      20\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many missing values do we have in each column?\n",
    "admissions.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop every row that has an NA.\n",
    "# Since we only have 20 missing out of 4000 rows, the impact of these missing rows on our model is probably minimal\n",
    "admissions.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What assumption are we making when we drop rows that have at least one NA in it?</summary>\n",
    "    \n",
    "- We assume that there's nothing special about the rows we happened to drop.\n",
    "- We might say that what we dropped is a random sample of our whole data.\n",
    "- We are assuming that our data is missing completely at random.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap of Notation\n",
    "\n",
    "You're quite familiar with **linear** regression:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\hat{\\mathbf{y}} &=& \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2 + \\cdots + \\hat{\\beta}_px_p \\\\\n",
    "&=& \\hat{\\beta}_0 + \\sum_{j=1}^p\\hat{\\beta}_jX_j\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{\\mathbf{y}}$ is the predicted values of the response variable $\\mathbf{y}$ based on all of the inputs $x_j$.\n",
    "- $x_1$, $x_2$, $\\ldots$, $x_p$ are the predictors.\n",
    "- $\\hat{\\beta}_0$ is the estimated intercept.\n",
    "- $\\hat{\\beta}_j$ is the estimated coefficient for the predictor $x_j$, the $j$th column in variable matrix $X$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plot-reg'></a>\n",
    "### What if we predicted `admit` with `gpa` using Linear Regression?\n",
    "\n",
    "Looking at the plot below, what are problems with using a regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12, 5))\n",
    "\n",
    "# using seaborn's auto linear regression best fit\n",
    "sns.regplot(x=admissions['gpa'], y=admissions['admit'], data=admissions,\n",
    "            ci = False, \n",
    "            line_kws = {'color': 'orange'})\n",
    "plt.ylim(-0.1, 1.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear regression line by itself isn't completely representative of the relation between the response and the feature in solving a classification problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pred-binary'></a>\n",
    "\n",
    "## Predicting a Binary Class\n",
    "\n",
    "---\n",
    "\n",
    "In our case we have two classes: `1=admitted` and `0=rejected`.\n",
    "\n",
    "The logistic regression is still solving for, **predicted response** $\\hat{y}$. However, in our binary classification case, **$\\hat{y}$ will be the probability of $y$ being one of the classes**.\n",
    "\n",
    "$$\n",
    "\\hat{y} = P(y = 1)\n",
    "$$\n",
    "\n",
    "We'll still try to fit a \"line\" of best fit to this... except it won't be perfectly linear. We need to *guarantee* that the right-hand side of the regression equation will evaluate to a probability. (That is, some number between 0 and 1!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Logit Link Function (advanced)\n",
    "\n",
    "---\n",
    "\n",
    "We will use something called a **link function** to effectively \"bend\" our line of best fit so that it is a curve of best fit that matches the range or set of values in which we're interested.\n",
    "\n",
    "For logistic regression, that specific link function that transforms (\"bends\") our line is known as the **logit** link.\n",
    "\n",
    "$$\n",
    "\\text{logit}\\left(P(y = 1)\\right) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_px_p\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log\\left(\\frac{P(y = 1)}{1 - P(y = 1)}\\right) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_px_p\n",
    "$$\n",
    "\n",
    "Equivalently, we assume that each independent variable $x_i$ is linearly related to the **log of the odds of success**.\n",
    "\n",
    "Remember, the purpose of the link function is to bend our line of best fit.\n",
    "- This is convenient because we can have any values of $X$ inputs that we want, and we'll only ever predict between 0 and 1!\n",
    "- However, interpreting a one-unit change gets a little harder. (More on this later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/logregmeme.png\" style=\"height: 400px\">\n",
    "\n",
    "[*image source*](https://twitter.com/ChelseaParlett/status/1279111984433127425?s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and making predictions with the logistic regression model.\n",
    "\n",
    "We can follow the same steps to build a logistic regression model that we follow to build a linear regression model.\n",
    "\n",
    "1. Define X & y\n",
    "2. Instantiate the model.\n",
    "3. Fit the model.\n",
    "4. Generate predictions.\n",
    "5. Evaluate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split into training & testing sets\n",
    "X = admissions[['gpa']]\n",
    "y = admissions['admit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Instantiate our model.\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Step 3: Fit our model.\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Intercept: [-17.50520394]\n",
      "Logistic Regression Coefficient: [[4.92045156]]\n"
     ]
    }
   ],
   "source": [
    "# referencing to our logistic regression equation above, the RHS still resembles that of linear regression,\n",
    "# so, we can obtain the best fit line's intercepts, coefficients attributed to the predictors with similar methods\n",
    "print(f'Logistic Regression Intercept: {logreg.intercept_}')\n",
    "print(f'Logistic Regression Coefficient: {logreg.coef_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are two methods in `sklearn` to be aware of when using logistic regression:\n",
    "- `.predict()`\n",
    "- `.predict_proba()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4 (part 1, using .predict()): Generate predicted classes, 'either' 0 or 1.\n",
    "logreg.predict(X_test)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.895, 0.105],\n",
       "       [0.82 , 0.18 ],\n",
       "       [0.995, 0.005],\n",
       "       ...,\n",
       "       [0.58 , 0.42 ],\n",
       "       [0.993, 0.007],\n",
       "       [0.98 , 0.02 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4 (part 2, using .predict_proba()): Generate predicted probabilities, 'between' 0 and 1.\n",
    "# below will generate the prediction probabilities and np.round will round the results to 3 decimals\n",
    "np.round(logreg.predict_proba(X_test), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.05097514e-01, 1.80209008e-01, 5.28865177e-03, 6.76723910e-01,\n",
       "       3.92682635e-01, 3.26048107e-01, 4.02932246e-02, 9.44740761e-01,\n",
       "       9.85939165e-01, 2.62783644e-02, 2.15329742e-03, 7.63886656e-01,\n",
       "       9.20474287e-02, 1.47584501e-02, 3.32620363e-02, 2.17513763e-01,\n",
       "       1.10153739e-01, 2.17773542e-02, 3.01994104e-01, 4.79277165e-03,\n",
       "       3.96889627e-01, 1.43526431e-01, 2.43729269e-03, 3.37571888e-01,\n",
       "       1.90161688e-02, 5.04037982e-02, 9.78607147e-04, 8.67570092e-01,\n",
       "       9.90753994e-01, 9.94767900e-01, 9.96791243e-01, 5.83977120e-04,\n",
       "       1.03377716e-02, 1.78462855e-02, 9.74703501e-01, 5.56949835e-01,\n",
       "       4.20089482e-02, 6.59694282e-02, 2.85297095e-03, 2.21787180e-02,\n",
       "       9.21463516e-01, 9.78666746e-01, 1.41695289e-02, 1.13280684e-02,\n",
       "       1.69751954e-03, 5.15988759e-01, 3.89521904e-03, 1.30274524e-01,\n",
       "       8.60574550e-02, 9.87081338e-01, 3.34752465e-02, 4.58934177e-02,\n",
       "       5.90915439e-01, 3.96091089e-02, 5.10929053e-01, 3.62328667e-03,\n",
       "       9.91225319e-01, 5.45393954e-01, 4.85941917e-01, 9.97998508e-01,\n",
       "       5.96250391e-03, 4.42999918e-01, 3.38685716e-01, 9.61578089e-01,\n",
       "       8.81140764e-02, 5.37907360e-03, 1.21360924e-01, 6.67149264e-01,\n",
       "       4.22606826e-01, 1.99006728e-01, 9.94275664e-01, 2.17605207e-03,\n",
       "       9.77053833e-01, 9.57439458e-03, 5.52288232e-03, 2.04808742e-01,\n",
       "       9.66513379e-01, 1.49145645e-01, 6.19895023e-01, 1.60216214e-01,\n",
       "       1.95523877e-03, 6.29097464e-02, 5.73939738e-02, 4.41841842e-02,\n",
       "       5.36969719e-01, 9.92409024e-01, 4.36592613e-01, 5.24507007e-01,\n",
       "       8.88066407e-03, 3.51420240e-01, 6.44010331e-01, 2.07285269e-03,\n",
       "       7.60855544e-01, 2.32642653e-03, 1.52144231e-01, 7.29269613e-01,\n",
       "       1.67878644e-04, 3.64366251e-02, 1.19681223e-02, 3.55154114e-01,\n",
       "       6.15752785e-01, 7.60734924e-01, 1.15102043e-03, 1.73577789e-03,\n",
       "       1.17241436e-03, 1.42450351e-02, 9.34824382e-03, 3.06545973e-02,\n",
       "       1.09177015e-02, 4.34769203e-04, 9.94577213e-01, 1.73470952e-01,\n",
       "       3.78705471e-02, 8.27624192e-04, 6.32437534e-01, 6.72515856e-04,\n",
       "       3.33786979e-02, 1.77137395e-02, 6.33712566e-03, 5.01156621e-01,\n",
       "       1.37492672e-01, 9.83954261e-01, 2.55524497e-02, 9.05831882e-01,\n",
       "       8.96657681e-01, 3.12813565e-02, 3.81745527e-02, 9.45325992e-01,\n",
       "       1.07601836e-03, 7.00366909e-02, 4.68569515e-02, 8.01231205e-02,\n",
       "       1.12252178e-01, 2.29475796e-02, 7.89896357e-04, 4.09256375e-01,\n",
       "       5.03225364e-01, 6.36871479e-01, 9.79591422e-01, 3.59675648e-02,\n",
       "       2.50005154e-01, 5.24962421e-01, 4.22019169e-03, 3.22591428e-04,\n",
       "       2.28264921e-01, 4.12947521e-01, 1.64416037e-01, 5.98544426e-03,\n",
       "       2.60036102e-02, 5.41719918e-01, 4.50358090e-03, 4.32947922e-02,\n",
       "       3.69303301e-01, 2.18692805e-01, 1.76270646e-03, 2.40875684e-01,\n",
       "       8.89701358e-01, 1.99360374e-02, 8.01461740e-01, 2.83149498e-03,\n",
       "       6.40950840e-01, 9.74942642e-01, 9.70337646e-01, 8.13787189e-02,\n",
       "       5.19820579e-02, 5.70032573e-02, 6.52850698e-01, 2.28182320e-03,\n",
       "       7.09290257e-01, 9.65039711e-01, 3.76325003e-01, 4.03646367e-02,\n",
       "       1.59325559e-02, 7.82754368e-04, 9.76019614e-01, 3.77456378e-03,\n",
       "       4.01310497e-02, 9.62696073e-01, 9.94951959e-01, 4.85676502e-01,\n",
       "       6.71360069e-04, 2.51306381e-01, 6.67513496e-02, 8.27978902e-01,\n",
       "       9.67506015e-01, 9.28572421e-03, 1.11772366e-01, 4.85361260e-03,\n",
       "       8.52069933e-01, 4.66453149e-02, 1.02645782e-01, 2.91129103e-02,\n",
       "       6.54350579e-01, 6.46473023e-02, 6.63946598e-01, 2.27014774e-04,\n",
       "       9.93701453e-01, 2.88136860e-01, 9.94003169e-03, 1.39042730e-02,\n",
       "       4.48207528e-01, 1.87701926e-02, 3.20698652e-01, 4.68912844e-02,\n",
       "       4.68163364e-02, 7.36291709e-02, 9.96749135e-01, 9.80356463e-01,\n",
       "       9.60217209e-01, 9.45829896e-02, 6.13557339e-01, 9.72313879e-01,\n",
       "       3.92857441e-04, 2.85649358e-01, 1.77995225e-03, 1.42413445e-02,\n",
       "       2.27975386e-01, 8.35578329e-02, 3.38222230e-05, 1.93994066e-01,\n",
       "       2.02632074e-04, 9.72261243e-03, 1.12810862e-01, 6.74585372e-02,\n",
       "       9.98009862e-01, 1.65742801e-02, 1.77583585e-02, 2.89043793e-04,\n",
       "       9.82064365e-01, 8.07726554e-02, 3.22080322e-02, 4.80962650e-04,\n",
       "       1.10848487e-02, 3.21132153e-03, 6.47782909e-03, 1.64997668e-01,\n",
       "       9.02905086e-01, 3.02724199e-02, 1.17379243e-02, 7.39871051e-02,\n",
       "       2.29786662e-01, 4.09731592e-01, 8.64173871e-04, 1.35048385e-01,\n",
       "       2.11296190e-02, 2.74988294e-02, 5.45162045e-03, 2.20349461e-02,\n",
       "       4.60099912e-03, 2.06473887e-02, 1.98351804e-02, 9.54746824e-02,\n",
       "       4.36093178e-04, 1.43205849e-01, 1.58448488e-02, 8.64500617e-01,\n",
       "       9.17581638e-02, 7.68494166e-01, 5.38988747e-02, 8.71479740e-01,\n",
       "       8.31949242e-02, 1.69078815e-01, 1.92057191e-01, 1.93586557e-01,\n",
       "       1.39834830e-01, 6.22988328e-02, 1.28139183e-02, 1.14558686e-02,\n",
       "       2.64029144e-01, 1.07860651e-01, 6.76971348e-02, 2.08849388e-01,\n",
       "       2.36238479e-02, 9.54768924e-01, 3.61963016e-03, 9.64484693e-01,\n",
       "       9.34521722e-01, 3.45597312e-01, 6.42827603e-01, 8.68021261e-02,\n",
       "       2.80320186e-01, 1.24765961e-02, 2.62541432e-01, 1.70141146e-02,\n",
       "       9.67536437e-01, 1.33305668e-01, 9.30850202e-03, 7.43180203e-03,\n",
       "       1.35199399e-01, 1.40699394e-01, 5.39421997e-03, 3.73347895e-02,\n",
       "       9.63382060e-01, 3.47422130e-01, 9.42311830e-01, 9.53734111e-01,\n",
       "       5.92259192e-02, 6.52986579e-01, 8.04076113e-01, 4.02661083e-01,\n",
       "       2.99395846e-02, 5.27906722e-05, 2.05204511e-04, 5.31215710e-02,\n",
       "       9.92419210e-01, 1.86511070e-03, 7.20307067e-02, 9.82043048e-01,\n",
       "       8.60114665e-01, 8.21463452e-04, 1.80228132e-02, 2.22952251e-01,\n",
       "       2.26978177e-02, 4.32614107e-01, 5.91917031e-01, 1.24845207e-01,\n",
       "       9.87714626e-01, 9.48748381e-01, 6.89198052e-01, 6.24698850e-01,\n",
       "       9.94003229e-01, 5.64686713e-01, 6.14672899e-02, 2.17490766e-01,\n",
       "       1.87273686e-02, 1.96839827e-02, 6.33508811e-04, 4.98681691e-01,\n",
       "       5.86591530e-01, 5.62956169e-01, 9.36764011e-04, 9.22148180e-03,\n",
       "       7.44870143e-04, 5.00913919e-03, 9.21179488e-01, 1.73830296e-03,\n",
       "       3.95249572e-01, 6.86491224e-01, 4.08669720e-01, 8.60753018e-01,\n",
       "       5.47806204e-02, 6.65993334e-01, 2.46071405e-01, 1.28905978e-02,\n",
       "       2.91485205e-03, 5.62780392e-03, 3.43391035e-03, 4.01793328e-02,\n",
       "       9.41125619e-01, 1.70524892e-02, 4.19340983e-03, 1.46254949e-02,\n",
       "       6.87795123e-03, 2.32127028e-02, 4.39081487e-03, 7.55640014e-02,\n",
       "       8.88582289e-03, 6.88766849e-02, 2.30412552e-01, 2.35269056e-02,\n",
       "       7.31518202e-02, 1.29510891e-02, 9.26710141e-03, 1.69115138e-03,\n",
       "       8.35583217e-01, 1.79988316e-01, 5.31855079e-02, 2.42928529e-02,\n",
       "       1.08453241e-02, 4.60051679e-03, 1.70214804e-02, 4.18592496e-01,\n",
       "       9.18035419e-01, 3.01478748e-01, 2.08345014e-02, 2.20263749e-01,\n",
       "       8.91320820e-01, 5.59005089e-03, 2.77225676e-01, 9.94969420e-01,\n",
       "       2.58697443e-02, 7.18703078e-02, 5.14194232e-03, 6.27886657e-02,\n",
       "       9.00798660e-01, 1.97170006e-01, 6.81258462e-03, 8.08163612e-01,\n",
       "       1.28354889e-01, 3.54784346e-03, 1.25183659e-01, 7.05118960e-01,\n",
       "       1.02000741e-03, 9.00187004e-01, 2.43394360e-02, 9.57770320e-01,\n",
       "       3.24426206e-03, 4.43457186e-01, 7.17518037e-02, 9.85026352e-01,\n",
       "       3.89030299e-01, 1.75941005e-01, 8.87097270e-03, 4.34461765e-01,\n",
       "       5.46476799e-03, 3.91443992e-01, 4.32665498e-03, 6.45352701e-01,\n",
       "       2.81249158e-01, 5.25202271e-01, 1.20349802e-01, 9.89864456e-03,\n",
       "       3.35575144e-02, 1.81748346e-02, 3.48924727e-01, 4.61110579e-03,\n",
       "       5.11164448e-03, 1.68947053e-02, 1.54130571e-01, 9.78543358e-01,\n",
       "       2.34981004e-02, 2.10582988e-02, 5.71210326e-03, 1.78635945e-02,\n",
       "       8.08463586e-01, 1.76605717e-01, 4.64310535e-01, 7.06991931e-01,\n",
       "       9.73493509e-01, 6.42820300e-01, 6.68354451e-01, 9.95286582e-01,\n",
       "       5.02767851e-01, 3.27679132e-04, 9.77426452e-03, 8.24178431e-01,\n",
       "       3.59330579e-03, 9.96819306e-01, 3.64871310e-02, 9.92448547e-01,\n",
       "       9.39778947e-01, 3.04512310e-01, 1.23436671e-01, 9.97346389e-01,\n",
       "       9.70301966e-01, 1.60026200e-03, 6.71620251e-03, 1.07866654e-02,\n",
       "       5.38888291e-04, 6.80475037e-01, 6.49585365e-04, 2.81582408e-01,\n",
       "       6.20251489e-02, 6.60515421e-02, 9.90048314e-01, 1.25792875e-03,\n",
       "       2.67248811e-02, 9.87884789e-01, 7.75265294e-01, 8.87819022e-03,\n",
       "       1.82005165e-02, 9.82611416e-01, 2.21369157e-03, 4.76679320e-02,\n",
       "       6.85894769e-01, 5.16361079e-01, 7.96632347e-03, 6.08711254e-03,\n",
       "       2.32429934e-05, 9.27994944e-02, 9.60274184e-01, 6.53040863e-01,\n",
       "       9.88484534e-01, 2.13747509e-02, 9.36397263e-01, 5.94837776e-01,\n",
       "       2.38365945e-02, 1.11592556e-01, 4.26047690e-02, 3.25730456e-03,\n",
       "       1.53427323e-02, 3.10827309e-03, 7.49679080e-01, 8.29524190e-01,\n",
       "       2.31462401e-01, 7.81108751e-03, 7.32741001e-01, 1.12287701e-02,\n",
       "       1.61209425e-01, 2.48364459e-01, 6.88091249e-01, 8.97695981e-03,\n",
       "       1.53328717e-01, 6.52800610e-01, 1.29048840e-02, 8.22013728e-01,\n",
       "       9.27785169e-01, 1.60932021e-01, 7.36528872e-02, 3.42178047e-01,\n",
       "       4.81690756e-02, 2.92506188e-04, 7.34701592e-03, 3.02742379e-02,\n",
       "       1.05315626e-02, 2.22452717e-03, 9.88454872e-01, 1.10887825e-01,\n",
       "       4.95401021e-04, 6.70965427e-01, 9.81142882e-01, 1.06692626e-01,\n",
       "       9.95817724e-01, 3.42932558e-01, 9.63566049e-01, 9.89253511e-03,\n",
       "       3.27645067e-04, 2.39080036e-01, 2.39721089e-02, 1.64611208e-01,\n",
       "       7.14198629e-01, 9.86851208e-01, 3.21420358e-01, 5.74014804e-01,\n",
       "       1.64251222e-03, 9.90028932e-01, 1.79239085e-02, 5.77397624e-02,\n",
       "       1.79558705e-03, 2.43220226e-02, 9.98460879e-01, 2.12281002e-02,\n",
       "       7.99419407e-01, 9.68043597e-01, 1.13976313e-02, 9.98203663e-01,\n",
       "       7.33628941e-02, 7.58874279e-03, 1.94850330e-02, 9.92480981e-01,\n",
       "       8.47471125e-02, 4.78171256e-01, 6.98749962e-01, 9.17219881e-01,\n",
       "       5.41456138e-01, 4.35842923e-01, 7.33289515e-01, 4.80742298e-03,\n",
       "       6.27028011e-02, 4.20156704e-02, 5.39838936e-02, 9.92460305e-04,\n",
       "       6.29060165e-03, 9.34494263e-01, 3.32977817e-03, 3.87097673e-01,\n",
       "       2.22074014e-01, 3.83815041e-02, 2.10137181e-02, 8.80797111e-01,\n",
       "       5.03546317e-01, 9.83314708e-01, 4.56658893e-04, 9.44381843e-01,\n",
       "       9.25023570e-01, 9.81677171e-01, 9.54918248e-03, 2.11223078e-02,\n",
       "       6.70250669e-01, 2.32605983e-03, 1.81988273e-01, 5.41619649e-05,\n",
       "       3.24087069e-03, 5.29906246e-02, 8.42326996e-01, 2.27441520e-02,\n",
       "       1.63644675e-01, 8.12489845e-03, 9.97790480e-01, 5.32098314e-02,\n",
       "       1.17189906e-02, 4.89135669e-03, 6.91201259e-01, 7.43590792e-03,\n",
       "       1.91096265e-01, 1.00565991e-01, 9.97259616e-01, 4.19696734e-03,\n",
       "       7.73813473e-01, 8.23043833e-03, 2.69613009e-02, 3.69412462e-01,\n",
       "       9.55397274e-02, 4.22037531e-02, 8.19925839e-01, 4.21197056e-03,\n",
       "       9.03670397e-01, 7.35021478e-03, 9.75413167e-01, 2.06286362e-01,\n",
       "       1.99087850e-02, 8.93161336e-03, 1.53768948e-01, 5.05538836e-01,\n",
       "       5.69536236e-02, 3.41049084e-03, 5.46941681e-03, 1.69430861e-03,\n",
       "       1.35452397e-01, 2.67827032e-02, 4.66041590e-03, 1.07886286e-02,\n",
       "       9.53248166e-01, 8.05298413e-01, 3.48114251e-01, 9.25137745e-01,\n",
       "       4.11785759e-02, 1.21395233e-02, 1.19935668e-02, 1.59858900e-03,\n",
       "       2.55816430e-02, 9.92514660e-02, 9.98032819e-01, 5.75638839e-01,\n",
       "       9.29669333e-02, 5.40386861e-03, 4.54015991e-04, 3.87552632e-02,\n",
       "       2.53164831e-01, 9.89787965e-01, 2.69664156e-02, 9.72322509e-01,\n",
       "       1.28574756e-02, 1.08580974e-03, 9.04742308e-01, 1.15045916e-01,\n",
       "       1.74558317e-02, 1.36443072e-03, 4.33578738e-03, 4.78508515e-04,\n",
       "       5.10760957e-03, 2.20476874e-03, 5.92360905e-03, 9.61492656e-01,\n",
       "       5.96748126e-02, 9.37124824e-01, 2.67645413e-01, 2.22233025e-03,\n",
       "       2.53602345e-02, 9.75448539e-01, 9.45481352e-01, 5.49343488e-01,\n",
       "       9.31466642e-01, 9.97184977e-01, 9.95918658e-01, 2.01682627e-01,\n",
       "       1.37001636e-02, 1.85683872e-01, 1.17560140e-02, 3.76784101e-02,\n",
       "       9.69172454e-01, 9.79112834e-01, 6.59592035e-02, 4.50950923e-01,\n",
       "       5.87578749e-02, 1.57322421e-01, 5.04049580e-02, 9.53758191e-01,\n",
       "       1.09470341e-01, 4.14177814e-01, 4.50431449e-02, 7.74928994e-04,\n",
       "       9.38457950e-01, 2.95118137e-02, 8.27909746e-01, 9.50223999e-01,\n",
       "       9.98674900e-01, 2.48365759e-03, 2.93814998e-02, 9.58063211e-01,\n",
       "       9.96500512e-01, 1.90052440e-01, 8.21178535e-04, 8.26316643e-01,\n",
       "       2.84117062e-01, 1.43003383e-02, 5.11909280e-03, 8.62554749e-01,\n",
       "       9.89125134e-01, 7.38916974e-03, 1.88549982e-01, 3.49472893e-02,\n",
       "       3.20669274e-03, 8.24310038e-02, 1.86527184e-01, 6.79397814e-02,\n",
       "       2.09592783e-02, 6.09238011e-03, 5.18626053e-04, 7.55617711e-02,\n",
       "       2.00052688e-02, 9.86990253e-01, 9.69319313e-01, 1.08242624e-02,\n",
       "       2.79701422e-03, 2.79943347e-03, 9.90518571e-01, 5.60417203e-01,\n",
       "       9.95322021e-01, 6.99947077e-02, 7.04655213e-01, 3.84016598e-02,\n",
       "       7.30037948e-02, 9.97234967e-01, 5.80160019e-01, 1.91111687e-02,\n",
       "       5.21384657e-01, 1.46535626e-01, 4.10458178e-02, 1.13895954e-02,\n",
       "       9.01969215e-01, 7.86459679e-02, 2.89659822e-03, 9.81014178e-01,\n",
       "       2.01122998e-01, 1.47083602e-02, 7.96890775e-03, 1.96796980e-01,\n",
       "       6.36956456e-01, 7.00680792e-01, 1.90548464e-01, 7.67094055e-01,\n",
       "       1.95904532e-03, 1.35682487e-03, 9.78879267e-01, 7.48367306e-02,\n",
       "       5.85638104e-04, 1.75965020e-01, 9.34703448e-01, 9.19156499e-03,\n",
       "       3.84041840e-01, 4.91718994e-04, 2.42790821e-01, 9.72554243e-01,\n",
       "       4.55061462e-03, 5.46289207e-03, 1.64227554e-03, 7.10932493e-02,\n",
       "       4.44445956e-02, 9.96590195e-01, 1.16450076e-02, 1.84957518e-01,\n",
       "       2.20227030e-03, 2.03756363e-03, 9.85914981e-01, 5.21485404e-01,\n",
       "       2.87716156e-02, 3.17563803e-02, 3.92134089e-01, 9.93815330e-01,\n",
       "       1.08470353e-02, 3.89434521e-01, 2.30268843e-02, 1.42556394e-01,\n",
       "       9.53919678e-01, 5.65375006e-03, 4.93891515e-02, 9.99115148e-01,\n",
       "       1.71961534e-01, 6.64451474e-01, 2.98459546e-01, 9.93562987e-01,\n",
       "       1.66004650e-01, 4.81250830e-01, 9.28052181e-01, 4.94327084e-02,\n",
       "       1.41407050e-02, 2.21988012e-01, 7.56473169e-04, 4.09685327e-03,\n",
       "       2.97880712e-02, 9.91815701e-01, 9.87950384e-01, 9.97091352e-01,\n",
       "       1.06844724e-01, 8.32684891e-01, 2.87495372e-02, 4.13793940e-03,\n",
       "       2.12138184e-01, 6.08365624e-04, 9.74989986e-01, 4.59651260e-03,\n",
       "       3.11072680e-02, 9.85165756e-01, 4.15901047e-03, 9.24065078e-02,\n",
       "       7.37791986e-01, 2.45000440e-02, 8.49979786e-02, 4.23296914e-02,\n",
       "       3.53265776e-02, 1.44998370e-02, 6.26378104e-01, 6.22805750e-03,\n",
       "       9.38302440e-01, 6.36130050e-02, 2.89755452e-02, 1.69504666e-02,\n",
       "       4.69138458e-01, 1.02773407e-02, 1.82101413e-02, 3.12659772e-01,\n",
       "       2.48823670e-03, 1.35278719e-02, 4.77804638e-03, 9.70991442e-01,\n",
       "       1.04680522e-02, 1.84710552e-01, 3.40632128e-03, 1.99674312e-02,\n",
       "       1.90922249e-01, 1.94303506e-02, 7.06452458e-02, 2.18214439e-01,\n",
       "       1.05063458e-01, 8.30337865e-01, 9.92261052e-01, 8.34917810e-02,\n",
       "       9.83339742e-01, 1.26261189e-03, 9.80837868e-01, 2.88778419e-02,\n",
       "       2.82019359e-01, 9.81643924e-01, 4.27702505e-03, 7.72270719e-04,\n",
       "       6.35105852e-02, 3.30240095e-04, 8.36846110e-01, 1.94240126e-01,\n",
       "       4.92843419e-01, 9.78615241e-01, 3.54664173e-02, 9.86907564e-01,\n",
       "       9.92175534e-01, 1.47882132e-03, 1.03408249e-03, 9.09196346e-01,\n",
       "       3.45099466e-01, 9.23280302e-01, 3.03360212e-03, 8.48794564e-01,\n",
       "       1.23810704e-02, 1.90984422e-03, 9.49355532e-01, 3.21138349e-01,\n",
       "       2.53142362e-01, 9.94528498e-01, 3.44980254e-03, 1.89977912e-02,\n",
       "       3.36391814e-01, 2.35674320e-03, 1.64472084e-02, 1.43469754e-01,\n",
       "       6.94284482e-03, 1.01111674e-01, 1.20241749e-03, 4.29090705e-02,\n",
       "       9.96783069e-04, 1.29901435e-02, 1.06774664e-01, 4.63593633e-04,\n",
       "       5.10158677e-02, 8.84487064e-01, 3.46149529e-02, 9.56019641e-01,\n",
       "       4.68994421e-04, 5.15229877e-03, 3.02123178e-03, 1.93544449e-01,\n",
       "       8.98072616e-01, 2.65485971e-01, 1.05383534e-01, 2.91562603e-02,\n",
       "       1.08627267e-02, 4.32214175e-01, 1.40647679e-02, 9.86825895e-01,\n",
       "       7.62297536e-01, 3.60871610e-01, 9.74696429e-03, 6.08594482e-01,\n",
       "       2.49391144e-03, 9.97922795e-01, 4.51528065e-02, 2.83663270e-02,\n",
       "       5.50082034e-03, 1.85458713e-01, 5.53857247e-02, 9.61220844e-01,\n",
       "       7.52542193e-02, 2.58303799e-01, 4.40081384e-02, 9.82508427e-01,\n",
       "       7.16715335e-01, 3.46968540e-03, 9.35753609e-01, 7.96389296e-01,\n",
       "       2.26359762e-03, 8.38213582e-02, 6.24563284e-02, 3.64098991e-02,\n",
       "       1.33176978e-01, 9.53817603e-01, 7.31354355e-02, 2.05139168e-03,\n",
       "       1.69820578e-03, 9.91327192e-02, 1.45326571e-03, 4.48477775e-01,\n",
       "       7.68670368e-04, 8.15329358e-01, 6.65916937e-02, 8.31747979e-02,\n",
       "       3.03626637e-02, 1.05965852e-01, 1.97470273e-04, 1.53728196e-01,\n",
       "       9.54598050e-01, 7.46357034e-01, 2.43537635e-03, 3.46107843e-02,\n",
       "       8.57267440e-01, 2.57590055e-02, 2.78268822e-02, 5.38649078e-02,\n",
       "       5.39944344e-01, 6.18232377e-02, 2.37704380e-01, 2.39924736e-02,\n",
       "       2.43601054e-01, 4.75894309e-04, 9.23218265e-03, 9.96127184e-01,\n",
       "       4.32107728e-02, 1.49755945e-02, 3.92449021e-02, 1.59942987e-02,\n",
       "       5.19395499e-01, 1.03450351e-01, 4.40593212e-03, 7.13630695e-01,\n",
       "       8.38433119e-01, 2.17929102e-03, 8.26963457e-01, 4.68469213e-01,\n",
       "       3.20365612e-02, 1.00801897e-01, 2.75566234e-01, 9.94201744e-01,\n",
       "       7.71676394e-01, 6.59492469e-01, 1.70019587e-01, 1.60942251e-02,\n",
       "       9.70379188e-01, 1.62817444e-03, 3.97708707e-04, 9.98597920e-01,\n",
       "       8.66173415e-01, 8.64656319e-01, 1.36177522e-01, 1.67315237e-02,\n",
       "       2.43896227e-02, 9.57332027e-03, 8.07181041e-01, 3.37849033e-03,\n",
       "       7.71236020e-01, 9.16503395e-01, 4.30677185e-02, 1.87999196e-03,\n",
       "       4.06246323e-03, 2.32096032e-02, 7.21221878e-02, 2.29038618e-01,\n",
       "       7.68767248e-03, 8.01416478e-02, 2.07805020e-01, 8.03990331e-03,\n",
       "       6.97580160e-01, 6.88000061e-03, 4.20350681e-01, 6.73209434e-03,\n",
       "       1.95264427e-02])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get all class 1 probabilities-->similarly, [:,0] will give all class 0 probabilities\n",
    "logreg.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How would you interpret the <b>.predict_proba()</b> output?</summary>\n",
    "    \n",
    "- This shows the probability of being rejected ($P(Y=0)$) and the probability of being admitted ($P(Y=1)$) for each observation in the testing dataset.\n",
    "- The first array [[array_element1, array_elemen2], corresponds to the first testing observation <i>(that is, the predicted probabilities for 1st element in X_test</i>).\n",
    "    - The `.predict()` value for this observation is 0. This is because $P(Y=0) > P(Y=1)$.\n",
    "- The second array, corresponds to the second testing observation.\n",
    "    - The `.predict()` value for this observation is 0. This is because, again $P(Y=0) > P(Y=1)$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing logistic regression probabilities for class 1 (in this case, corresponds to successful admission).\n",
    "\n",
    "plt.figure(figsize = (10, 5)) # define fig size\n",
    "\n",
    "plt.scatter(X_test, y_test, s = 10); # scatter plot for the data points\n",
    "\n",
    "# we do .sort_values to standardize the input X_test values and corresponding predicted probabilities they generate\n",
    "plt.plot(X_test.sort_values('gpa'), # best fit line plot for logistic regression\n",
    "         logreg.predict_proba(X_test.sort_values('gpa'))[:,1],\n",
    "         color = 'grey', alpha = 0.8, lw = 3)\n",
    "# setting labels and plot title\n",
    "plt.xlabel('GPA')\n",
    "plt.ylabel('Admit')\n",
    "plt.title('Predicting Admission from GPA');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8938528720188109"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Evaluate model.\n",
    "logreg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9023162134944612"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `.score()` method for classification models gives us the **accuracy** score.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Accuracy} = \\frac{\\text{number of correct predictions}}{\\text{number of total predictions}}\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Remind me: what does .score() give us by default for a regression model?</summary>\n",
    "    \n",
    "- The $R^2$ score.\n",
    "- Remember that $R^2$ is the proportion of variance in our $Y$ values that are explained by our model.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the log-odds the natural logarithm of the odds.\n",
    "\n",
    "The combination of converting the \"probability of success\" to \"odds of success,\" then taking the logarithm of that is called the **logit link function**.\n",
    "\n",
    "$$\n",
    "\\text{logit}\\big(P(y=1)\\big) = \\log\\bigg(\\frac{P(y=1)}{1-P(y=1)}\\bigg) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_px_p\n",
    "$$\n",
    "\n",
    "We've bent our line how we want... but how do we interpret our coefficients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odds\n",
    "\n",
    "**Probabilities and odds represent the same thing in different ways**. The odds for probability **p** _(both of which are associated with an event)_ is defined as:\n",
    "\n",
    "$$\n",
    "\\text{odds}(p) = \\frac{p}{1-p}\n",
    "$$\n",
    "\n",
    "The odds of a probability is a measure of _how many times_ as likely an event **is to happen than it is to not happen**.\n",
    "\n",
    "**Example**: Suppose I'm looking at the probability and odds of a specific horse, \"Secretariat,\" winning a race. _just substitute `p` in the above equation for getting respective `odds`.\n",
    "\n",
    "- When **`p = 0.5`**: **`odds = 1`**\n",
    "    - The horse Secretariat is as likely to win as it is to lose.\n",
    "- When **`p = 0.75`**: **`odds = 3`**\n",
    "    - The horse Secretariat is three times as likely to win as it is to lose.\n",
    "- When **`p = 0.40`**: **`odds = 0.666..`**\n",
    "   - The horse Secretariat is two-thirds as likely to win as it is to lose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting a one-unit change in $x_i$.\n",
    "With linear regression, it was easier to realize the effect of a one-unit change in $x_i$, moving response $\\hat y$ by $\\beta_i$\n",
    "\n",
    "Let's understand for logistic regression, starting with the link function applied equation:\n",
    "\n",
    "$$\\log\\bigg(\\frac{P(y=1)}{1-P(y=1)}\\bigg) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_px_p$$\n",
    "\n",
    "Given the above model, a one-unit change in $x_i$ implies a $\\beta_i$ unit change in the **log odds of success** _(in the LHS)_.\n",
    "\n",
    "We often convert log-odds back to \"regular odds\" when interpreting our coefficient... our mind understands odds better than the log of odds.\n",
    "\n",
    "**(BONUS)** So, let's get rid of the log on the left-hand side. Mathematically, we do this by \"exponentiating\" each side. _(as exponentiation is the **inverse** of log)_\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\log\\bigg(\\frac{P(y=1)}{1-P(y=1)}\\bigg) &=& \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_px_p \\\\\n",
    "\\Rightarrow e^{\\Bigg(\\log\\bigg(\\frac{P(y=1)}{1-P(y=1)}\\bigg)\\Bigg)} &=& e^{\\Bigg(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_px_p\\Bigg)} \\\\\n",
    "\\Rightarrow \\frac{P(y=1)}{1-P(y=1)} &=& e^{\\Bigg(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_px_p\\Bigg)} \\\\\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "**Unit change Interpretation for logistic regression**: A one-unit change in $x_i$ means that success is $e^{\\beta_i}$ times as likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.92045156]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary> I want to interpret the coefficient $\\hat{\\beta}_1$ for my logistic regression model. How would I interpret this coefficient?</summary>\n",
    "    \n",
    "- Let's start with our model equation: $\\log\\bigg(\\frac{P(admit=1)}{1-P(admit=1)}\\bigg) = \\beta_0 + \\beta_1\\text{GPA}$.\n",
    "    - As GPA increases by 1, the log-odds of someone being admitted <i>(that is, admit=1)</i> increases by 4.92.\n",
    "- Let's now apply the exponential transformation discussed above: \n",
    "    - As GPA increases by 1, someone is $e^{4.92}$ times as likely to be admitted.\n",
    "    - As GPA increases by 1, someone is about 137.06 times as likely to be admitted to grad school.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[137.06449198]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can Use np.exp() to exponentiate the coefficient to arrive at above conclusion automatically.\n",
    "np.exp(logreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[np.exp Documentationn](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The goal of logistic regression is to find the best-fitting model to describe the relationship between a binary outcome and a set of independent variables.\n",
    "\n",
    "Logistic regression generates the coefficientsof a formula to predict a logit transformation of the probability that the characteristic of interest is present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What is the difference between a classification and a regression problem?</summary>\n",
    "    \n",
    "- A classification problem has a categorical $Y$ variable. A regression problem has a numeric $Y$ variable.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What are some of the benefits of logistic regression as a classifier?</summary>\n",
    "\n",
    "(Answers may vary; this is not an exhaustive list!)\n",
    "- Logistic regression is a classification algorithm that shares similar properties to linear regression.\n",
    "- The coefficients in a logistic regression model are interpretable. (They represent the change in log-odds caused by the input variables.)\n",
    "- Logistic regression is a very fast model to fit and generate predictions from.\n",
    "- It is by far the most common classification algorithm.\n",
    "\n",
    "**Note**: The original interview question was \"If you're comparing decision trees and logistic regression, what are the pros and cons of each?\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (BONUS) The Logistic Function\n",
    "\n",
    "The ***inverse function of the logit*** is called the **logistic function**. \n",
    "\n",
    "By inverting the logit, we can have the right side of our regression equation solve explicitly for $P(y = 1)$:\n",
    "\n",
    "$$\n",
    "P(y=1) = logit^{-1}\\left(\\beta_0 + \\sum_{j}^p\\beta_jx_j\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "logit^{-1}(a) = logistic(a) = \\frac{e^{a}}{e^{a} + 1}\n",
    "$$ \n",
    "\n",
    "Giving us:\n",
    "\n",
    "$$\n",
    "P(y=1) = \\frac{e^{\\left(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_px_p\\right)}}{e^{\\left(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_px_p\\right)}+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (BONUS) Solving for the Beta Coefficients\n",
    "\n",
    "Logistic regression minimizes the \"deviance,\" which is similar to the residual sum of squares in linear regression, but is a more general form. \n",
    "\n",
    "**There's no closed-form solution to the beta coefficients like in linear regression, and the betas are found through optimization procedures.**\n",
    "- We can't just do $\\hat{\\beta} = (X^TX)^{-1}X^Ty$ like we can in linear regression!\n",
    "\n",
    "The `solver` hyperparameter in sklearn's LogisticRegression class specifies which method should be used to solve for the optimal beta coefficients (the coefficients that minimize our cost function). A former DC DSI instructor Jeff Hale has a great blog post about which solver to choose [here](https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451) and [this](https://www.linkedin.com/posts/justmarkham_sklearntips-machinelearning-python-activity-6654000730321534976-Um6C/) is a good post by Kevin Markham that compares the solvers.\n",
    "\n",
    "If you're particularly interested in the math, here are two helpful resources:\n",
    "- [A good blog post](http://www.win-vector.com/blog/2011/09/the-simpler-derivation-of-logistic-regression/) on the logistic regression beta coefficient derivation.\n",
    "- [This paper](https://www.stat.cmu.edu/~cshalizi/402/lectures/14-logistic-regression/lecture-14.pdf) is also a good reference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
