{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Regression Metrics\n",
    "\n",
    "_Authors:_ Tim Book, Matt Brems, Riley Dallas, Noelle Brown, Dan Wilhelm\n",
    "\n",
    "\n",
    "> All models are wrong, but some are useful.\n",
    ">\n",
    "> -- <cite>George EP Box</cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we'll cover metrics in more depth. **A metric allows us to state precisely how well our model is performing.**\n",
    "\n",
    "As a real-life example shared before, we can think of a metric like the score from an examination to gauge how well a student performed vs actual answers. Other business examples could be KPI metrics related to hiring or employee retention which the organization has to meet and that then translates to downstream incentives like corporate bonus. \n",
    "\n",
    "> We've already evaluated our model with some metrics -- the Sum of Squared Errors (SSE), Mean of Squared Errors (MSE) and $R^2$\n",
    "\n",
    "In a mathematical context, you can consider a *metric* to be a ***distance function*** -- it measures how *far away our predictions are from the actual* targets.\n",
    "\n",
    "\n",
    "### What is a metric?\n",
    "\n",
    "A metric is a _distance function_! It is used to give us a **numerical measure of our model's performance**.\n",
    "\n",
    "Metrics have a _mathematical_ definition. Given some set $X$ (just a collection of elements), a **distance function** $d$ maps any two elements $x, y \\in X$ (where x and y are elements of set X) to a real number. The distance function $d$ must satisfy some axioms as captured below:\n",
    "\n",
    "_(I find it easier to interpret these, by looking at them in relation to a metric we already evaluated in our modeling, like $MSE$)_\n",
    "1. $d(x, y) \\geq 0$.  --> $MSE$ is always positive because it is made of squared elements\n",
    "2. $d(x, y) = 0$ if and only if $x = y$.  --> $MSE$ = 0 when y_true = y_pred\n",
    "3. $d(x, y) = d(y, x)$. --> refer intuition deck\n",
    "4. $d(x, y) \\leq d(x, z) + d(z, y)$, for any $z \\in X$. --> refer intuition deck\n",
    "\n",
    "In this class, $x, y, z$ are usually ***vectors of real numbers***. However, we will see examples later in the class where they can be other things, such as sets of words in an email.\n",
    "\n",
    "---\n",
    "\n",
    "### What is a loss function?\n",
    "\n",
    "A **loss function** is often a *metric*. It is used along with an _optimization algorithm_ to determine a **\"good\" set of parameter values**. Like, finding the best line of fit in a linear regression model.\n",
    "\n",
    "A required characteristic for a loss function in an optimization context is for it to be _differentiable_ (this is referring to [mathematical differentiation](https://en.wikipedia.org/wiki/Derivative) which revolves around: the derivative of a function of a real variable measures the sensitivity to change of the function value (output value) with respect to a change in its argument (input value) and differentiation is the process of finding this derivative)\n",
    "\n",
    "> Be careful! In machine learning, you will see the loss function referred to variously as **cost function**, **scoring function**, **evaluation function**, **objective function**, and **entropy function**. These all refer to the same thing -- they ***measure how far our predictions are from the actual values***.\n",
    "\n",
    "\n",
    "<details><summary>QUESTION: The Sum of Squared Errors (SSE) is a metric. Assuming the predictive model is differentiable, can we use it as a loss function? --> refer to intuition deck\n",
    "    \n",
    "$$SSE = \\sum_{i=1}^N{(\\hat{y}(x_i) - y_i)^2}.$$</summary>\n",
    "\n",
    "Yes! The sum of squared terms is differentiable, so it would be a good choice for optimization. In fact, SSE is a very common loss function in regression problems -- even for neural networks.\n",
    "\n",
    "$$SSE = \\sum{(y_{true} - y_{pred})^2}$$\n",
    "\n",
    "$$SSE = \\sum{(y_{true} - (x_1 a_1 + x_0 a_0))^2}$$\n",
    "\n",
    "$$SSE = \\sum{(y_{true} - x_1 a_1 - x_0 a_0)^2}$$\n",
    "\n",
    "$$\\frac{\\partial{SSE}}{\\partial{a_1}} = \\sum{2(y_{true} - x_1 a_1 - x_0 a_0)(0 -x_1 -0)}$$\n",
    "\n",
    "$$\\frac{\\partial{SSE}}{\\partial{a_1}} = \\sum{-2x_1(y_{true} - x_1 a_1 - x_0 a_0)}$$\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example -- Distance between emails as a metric\n",
    "\n",
    "**Metrics are anything that measures distance**. So, they _do not have to be differentiable!_\n",
    "\n",
    "For example, suppose we want to know how similar two emails (email_1, email_2) are. Supposing each email is a non-empty _set_ of words (i.e. there are no duplicates), we propose the following metric:\n",
    "\n",
    "$$d(\\text{email}_1, \\text{email}_2) = 1 - \\frac{\\text{# words in common}}{\\text{# total words}}$$\n",
    "\n",
    "This is effectively a \"distance\" between two sets of words! It is called the [Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index). \n",
    "\n",
    "- A Jaccard _coefficient_ measures **similarity** between finite sample sets, and is defined as the _size of the intersection between the sets_ divided by the _size of the union_ of the sample sets.\n",
    "\n",
    "$$\\text{Jaccard coeff} = \\frac{\\text{# words in common}}{\\text{# total words}}$$\n",
    "\n",
    "- A Jaccard _distance_ measures **dissimilarity (or, what is different)** between sample sets, and is complementary to the Jaccard coefficient. It is obtained by **subtracting the Jaccard coefficient from 1** \n",
    "\n",
    "    - $\\text{email}_1 = \\{\\text{the, meeting, is, today}\\}$.\n",
    "    - $\\text{email}_2 = \\{\\text{the, prince, will, see, you}\\}$.\n",
    "        - we can straightway see, the only similarity from the above set of words is \"the\"\n",
    "    - Then: $d(\\text{email}_1, \\text{email}_2) = 1 - \\frac{\\text{# words in common}}{\\text{# total words}} = 1 - \\frac{1}{8} = \\frac{7}{8}$, i.e. they are far apart.\n",
    "\n",
    "\n",
    "<details><summary>QUESTION: The Jaccard distance is a metric. Can we also use it as a loss function?</summary>\n",
    "\n",
    "No -- at least not easily! It is a function, but it is definitely not differentiable (recap from our intuition deck, the derivative of a constant --> 0).\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33333333333333337"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can you calculate the Jaccard distance using Python?\n",
    "\n",
    "# What is the Jaccard distance between user1 and user2, based on their brand preferences?\n",
    "user1 = {'nike', 'adidas', 'starbucks'}\n",
    "user2 = {'nike', 'adidas'}\n",
    "\n",
    "# -- Hint: refer to Jaccard coefficient definition above\n",
    "# -- Hint: Use the set intersection(), union() and len() (- this is for size) functions!\n",
    "\n",
    "1 - len(user1.intersection(user2)) / len(user1.union(user2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example -- Euclidean distance is a metric\n",
    "\n",
    "Let's take a look at a classic distance function -- the **Euclidean distance**. Don't be scared by the word _Euclidean_ -- this just indicates the geometry of the world around us!  Euclid wrote the math textbook \"The Elements,\" the second-most widely printed book after the Bible.\n",
    "\n",
    "> We will use a different notation from grade-school math. First, $\\mathbb{R}^2$ refers to the set of all _pairs_ of real numbers, i.e. two-dimensional points, or tuples. So, $x \\in \\mathbb{R}^2$ refers to a tuple $(x_1, x_2)$ and $y \\in \\mathbb{R}^2$ refers to a tuple $(y_1, y_2)$. --> refer to intuition slide\n",
    "\n",
    "_Note: this is NOT the coefficient of determination $R^2$_.\n",
    "\n",
    "> We write them in this way because each tuple will have millions of components later in this class. We can then easily write e.g. $(x_1, x_2, x_3, \\cdots, x_{1000})$! We would quickly run out of letters if we use the grade-school notation $(x, y, z)$.\n",
    "\n",
    "Let's start with 2-D points. Suppose we have two-dimensional points $x, y \\in \\mathbb{R}^2$. Using the [Pythagorean Theorem](https://en.wikipedia.org/wiki/Pythagorean_theorem) _(that relates the length of the 3 sides of a hypotenuse triangle, $a^2$ + $b^2$ = $c^2$ (or) $c = \\sqrt{(a)^2 + (b)^2}$)_, we can prove that the Euclidean distance between $x$ and $y$ (or, simply x - y) by substituting $a$, $b$ and $c$ as:\n",
    "\n",
    "$$d(x, y) = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2}.$$\n",
    "\n",
    "By applying the Pythagorean Theorem once more, we discover a similar function for **three**-dimensional points $x, y \\in \\mathbb{R}^3$:\n",
    "\n",
    "$$d(x, y) = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + (x_3 - y_3)^2}.$$\n",
    "\n",
    "Seeing a pattern and applying the concept we had picked up about **summation**, we then define the Euclidean distance for **N-dimensional points** $x, y \\in \\mathbb{R}^N$:\n",
    "\n",
    "$$d(x, y) = \\sqrt{\\sum_{i=1}^N{(x_i - y_i)^2}}.$$\n",
    "\n",
    "<details><summary>QUESTION: What is the relationship between $d, x, y$ and Sum of Squared Errors?</summary>\n",
    "    \n",
    "- Let $x$ be a vector of predicted values.\n",
    "- Let $y$ be a vector of actual values.\n",
    "- Then, $d^2(x, y)$ is the SSE.\n",
    "$$d(x, y) := \\sqrt{SSE}.$$\n",
    "    \n",
    "So, the SSE is the squared distance between the two vectors!\n",
    "</details>\n",
    "<details><summary>QUESTION: What is the relationship between $d, x, y$ and the population variance?</summary>\n",
    "\n",
    "Population variance (denoted by $\\sigma^2$) tells us how data points in a specific population (like a vector of numbers) are spread out - which mathematically could be distance of each vector elements from the overall mean\n",
    "$$\\sigma^2 = \\frac{\\sum_{i=1}^N{(x_i-\\mu)^2}}{N}$$\n",
    "- Let $x$ be any vector of real numbers, where $N = len(x)$. \n",
    "- Let $y$ be the mean of $x$ (denoted by $\\mu$) repeated $N$ times.\n",
    "$$\\sigma^2 = \\frac{d^2(x, \\mu)}{N}$$\n",
    "- Then, $d^2(x, y)/N$ is the variance of $x$!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from math import sqrt # alternative to (variable)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you calculate the distance between x and y using Python?\n",
    "x = (4, 18, 9, 2, 7)\n",
    "y = (3, 3, 8, 1, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 3), (18, 3), (9, 8), (2, 1), (7, 8)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zip yields tuples until an input is exhausted\n",
    "# we know from our distance equation above that it is made up of several tuples\n",
    "list(zip(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.132745950421556"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating euclidean distance \n",
    "sqrt(sum((xi - yi)**2 for xi, yi in zip(x, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.132745950421556"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same solution using list comprehension?\n",
    "# steps:\n",
    "    #1: loop - for xi, yi in zip(x, y)\n",
    "    #2: operation - calculate difference x - y per tuple and square\n",
    "    #3: operation - sum all outputs from #2\n",
    "    #4: operation - calculate sqrt of output from #3\n",
    "sqrt(sum([(xi - yi)**2 for xi, yi in zip(x, y)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you calculate the distance between x and y using numpy?\n",
    "x = np.array([4, 18, 9, 2, 7])\n",
    "y = np.array([3, 3, 8, 1, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4 18  9  2  7] [3 3 8 1 8] [ 1 15  1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "print(x, y, x-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.132745950421556"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating euclidean distance using numpy\n",
    "np.sqrt(((x - y)**2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do metrics matter?\n",
    "---\n",
    "\n",
    "Nearly all models require use of a **metric to assess our performance**.\n",
    "- For example: linear regression, logistic regression, and neural networks.\n",
    "\n",
    "Many algorithms work by finding the \"nearest\" points. _(finding **distance** between points)_\n",
    "- For example: k-Nearest Neighbors, clustering, and recommendation systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How are loss functions used?\n",
    "---\n",
    "\n",
    "Over the next few weeks, we will use the same process for nearly every (parametric) model _(parametric model is every model that can be written with a equation)_. We will choose an equation, a loss function, and an optimization algorithm.\n",
    "\n",
    "For example, for multiple linear regression:\n",
    "\n",
    "- Model: $\\hat{y} = \\beta_0 + \\beta_1x_1 + \\cdots + \\beta_Nx_N$\n",
    "- Loss Function: $SSE = \\sum_{i=1}^N{(\\hat{y}_i - y_i)^2}$\n",
    "- Optimization Algorithm: Linear Algebra _(direct solution, used under the hood when we use sklearn, instead of fitting several lines through our data points)_\n",
    "\n",
    "For the same model, we could have made other (less common) choices:\n",
    "\n",
    "- Model: $\\hat{y} = \\beta_0 + \\beta_1x_1 + \\cdots + \\beta_Nx_N$\n",
    "- Loss Function: $E = SSE + \\alpha\\sum_{i=1}^N{\\beta^2}$ (SSE with regularization)\n",
    "- Optimization Algorithm: Gradient Descent (using derivative of the loss function to decide direction of parameter change iteratively until an optimal best fit is reached)\n",
    "\n",
    "This same framework is even used with neural networks _(right now, neural networks is the basis of everything AI - more on these topics later in the course)_:\n",
    "- Model: $\\hat{y} = \\sigma(\\sigma(\\frac{1}{1 + e^{-(x_1+\\cdots+x_N)}} + \\cdots) + \\cdots)$, for a non-linear function $\\sigma$.\n",
    "- Loss Function: $E = SSE + \\alpha\\sum_{i=1}^N{\\beta^2}$ (SSE with regularization)\n",
    "- Optimization Algorithm: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# modeling imports - linear regression\n",
    "from sklearn.linear_model import LinearRegression # needs scipy 1.6.3 \n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "---\n",
    "\n",
    "Today's dataset (`Advertising.csv`) is from the [ISLR](https://faculty.marshall.usc.edu/gareth-james/ISL/data.html) website.\n",
    "\n",
    "Drop `\"Unnamed: 0\"` once you've loaded the CSV into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     TV  radio  newspaper  sales\n",
       "0           1  230.1   37.8       69.2   22.1\n",
       "1           2   44.5   39.3       45.1   10.4\n",
       "2           3   17.2   45.9       69.3    9.3\n",
       "3           4  151.5   41.3       58.5   18.5\n",
       "4           5  180.8   10.8       58.4   12.9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bring in advertising data\n",
    "ads = pd.read_csv('../datasets/Advertising.csv')\n",
    "ads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TV', 'radio', 'newspaper', 'sales'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop Unnamed: 0 column\n",
    "ads = ads.drop(columns='Unnamed: 0')\n",
    "ads.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "---\n",
    "\n",
    "Run the following checks in the cells provided:\n",
    "\n",
    "- Are there any null values (`NaN`)?\n",
    "- Are there any corrupted columns?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TV           0\n",
       "radio        0\n",
       "newspaper    0\n",
       "sales        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for nulls\n",
    "ads.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TV           float64\n",
       "radio        float64\n",
       "newspaper    float64\n",
       "sales        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check datatypes for any corrupted columns\n",
    "ads.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our feature matrix (`X`) and target vector (`y`)\n",
    "---\n",
    "\n",
    "The following columns will be our features:\n",
    "\n",
    "- `'TV'`\n",
    "- `'radio'`\n",
    "- `'newspaper'`\n",
    "\n",
    "The `sales` column is our label (/target): the column we're trying to predict.\n",
    "\n",
    "In the cell below, create your `X` and `y` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  (200, 3)\n",
      "y:  (200,)\n"
     ]
    }
   ],
   "source": [
    "# create X and y\n",
    "FEATURES = ['TV', 'radio', 'newspaper']\n",
    "\n",
    "X = ads[FEATURES]\n",
    "y = ads['sales'].values\n",
    "\n",
    "# Verify dimensions\n",
    "print('X: ', X.shape)\n",
    "print('y: ', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  radio  newspaper\n",
       "0  230.1   37.8       69.2\n",
       "1   44.5   39.3       45.1\n",
       "2   17.2   45.9       69.3\n",
       "3  151.5   41.3       58.5\n",
       "4  180.8   10.8       58.4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22.1, 10.4,  9.3, 18.5, 12.9])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TV', 'radio', 'newspaper'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one possible alternate approach when you have too many features to manually specify\n",
    "response = ['sales']\n",
    "features = ads.columns[~ads.columns.isin(response)] # tilde reverses .isin()\n",
    "features\n",
    "# X = ads[features] yields the same output as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression model\n",
    "---\n",
    "\n",
    "In the cell below, create an instance of `LinearRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "---\n",
    "\n",
    "The `.fit()` method is how our model will learn the coefficients for each of the features (`'TV'`, `'radio'` and `'newspaper'`).\n",
    "\n",
    "Once it's fit, you can see the bias (aka intercept) and coefficients by running:\n",
    "\n",
    "```python\n",
    "model.coef_\n",
    "model.intercept_\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit\n",
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04576465,  0.18853002, -0.00103749])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coefficients\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9388893694594103"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y intercept\n",
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TV           0.045765\n",
       "radio        0.188530\n",
       "newspaper   -0.001037\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(lr.coef_, index=FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "---\n",
    "\n",
    "To get predictions from our model, all we have to do is run `.predict(X_to_predict)`. This will return a list (`np` array) of predictions, one for each row in our `X_to_predict`. Which, can be compared against the actual `y_response` values to gauge model effectiveness. Normally you'll use this method for making predictions on unseen data, but today we'll be evaluating the data that was fed into the model during `.fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([22.1, 10.4,  9.3, 18.5, 12.9]),\n",
       " array([20.52397441, 12.33785482, 12.30767078, 17.59782951, 13.18867186]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get predictions from our X data\n",
    "predictions = lr.predict(X)\n",
    "\n",
    "y[:5], predictions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Back to Metrics!\n",
    "\n",
    "Now that we've computed some predictions, let's understand the rationale behind why Mean Squared Error is so commonly used as a metric and loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum Squared Error (SSE)\n",
    "---\n",
    "\n",
    "$$SSE = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "SSE forms the basis for several loss/optimization functions that we'll cover next:\n",
    "\n",
    "- Mean squared error (MSE)\n",
    "- Root mean squared error (RMSE)\n",
    "- R2\n",
    "\n",
    "> **NOTE:** Depending on what corner of the internet you're in, the SSE goes by many different names. Some common names are the **residual sum of squares (RSS)** and **total sum of squares (TSE)**. However, some other sources abbreviate something else RSS. Be careful!\n",
    "\n",
    "<details><summary>QUESTION: Can you think of a reason why NOT to use this as a metric?</summary>\n",
    "\n",
    "Its value is dependent on the number of data points! So we cannot use it to reliably compare between two datasets, e.g. our training and test sets, where training will always have much more data points to train the model (we'll cover this soon).\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556.8252629021872"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate SSE by hand\n",
    "sse = ((y - predictions)**2).sum()\n",
    "sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error (MSE)\n",
    "---\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "Goal: Get $MSE$ as close to 0 as possible. _(we covered in previous lesson, with errors, ALWAYS lesser is better)_\n",
    "\n",
    "Pros: \n",
    "- Very common; part of other calculations.\n",
    "- Represents average distance squared from the predicted value.\n",
    "- Punishes outliers severely.\n",
    "- Coincides directly with the metric used to fit OLS model.\n",
    "\n",
    "Cons: \n",
    "- Can be heavily affected by outliers. _(mostly from outlier data in y_true)_\n",
    "- Not in the original units of $Y$. _(we squared all residuals to neutralize negative deltas between y_true vs y_pred, so we are dealing with values in a squared scale)_\n",
    "- Depends on scale of $Y$. (i.e. housing prices vs. GPA)\n",
    "- Uninterpretable to humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.784126314510936"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MSE by hand\n",
    "((y - predictions)**2).sum() / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.784126314510936"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MSE using sklearn\n",
    "metrics.mean_squared_error(y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error (RMSE)\n",
    "---\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n",
    "\n",
    "$$RMSE = \\sqrt{MSE}$$\n",
    "\n",
    "Goal: Get $RMSE$ as close to 0 as possible.\n",
    "\n",
    "Pros: \n",
    "- Pretty common.\n",
    "- Represents (approximately) average distance from the predicted value.\n",
    "    - Looks similar to standard deviation.\n",
    "- In the original units of $Y$. _(sqrt(MSE), scales down the amplification that happens from all the residual squaring in MSE)_\n",
    "\n",
    "Cons: \n",
    "- Can be heavily affected by outliers. _(mostly from outlier data in y_true)_\n",
    "- Depends on scale of $Y$. (i.e. housing prices vs. GPA)\n",
    "- Only a _little_ interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6685701407225697"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RMSE\n",
    "np.sqrt(metrics.mean_squared_error(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error (MAE)\n",
    "---\n",
    "\n",
    "$$MAE = \\frac{1}{n}\\sum_{i=1}^n(|y_i-\\hat{y}_i|)$$\n",
    "\n",
    "Goal: Get MAE as close to 0 as possible. _(still an error)_\n",
    "\n",
    "Pros: \n",
    "- Represents mean distance from the predicted value.\n",
    "- In the original units of $Y$. \n",
    "- Is not heavily affected by outliers.\n",
    "\n",
    "Cons: \n",
    "- Depends on scale of $Y$. (i.e. housing prices vs. GPA)\n",
    "- Punishes all errors with same \"severity\".\n",
    "- Cannot be easily used as a loss function, since it is non-differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2520112296870685"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcuate MAE by hand\n",
    "np.abs(y - predictions).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2520112296870685"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Via sklearn.metrics\n",
    "metrics.mean_absolute_error(y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficient of Determination, $R^2$\n",
    "---\n",
    "\n",
    "$$R^2 = \\frac{\\text{Explained Variance}}{\\text{Total Variance}} = 1 - \\frac{\\text{Model Error}}{\\text{Total Variance}} = 1 - \\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n(y_i - \\bar{y})^2}$$\n",
    "\n",
    "recap: $\\bar{y}$ = mean(y) associated with the null model\n",
    "\n",
    "Goal: Get $R^2$ as close to 1 as possible.\n",
    "\n",
    "> - score = 0: Model explains none of the variability of the response data around its mean.\n",
    "> - score = 1: Model explains all the variability of the response data around its mean.\n",
    "\n",
    "Pros:\n",
    "- Easy interpretation. \"An $R^2$ value of 0.8 means that 80% of the variability in _y_ is explained by the _x_-variables in our model.\"\n",
    "- Common metric.\n",
    "- Does not depend on the scale of $Y$.\n",
    "- Works with more than just _linear_ regression.\n",
    "\n",
    "Cons:\n",
    "- As you add more variables, $R^2$ will never decrease (with linear regression). [Adjusted $R^2$](https://www.statisticshowto.com/adjusted-r2/) can handle this assumption better.\n",
    "- **ONLY INTERPRETABLE WITH LINEAR REGRESSION!**\n",
    "- Outside linear regression values outside 0 and 1 are possible.\n",
    "\n",
    "> Are low $R^2$ scores bad?\n",
    ">\n",
    "> I'm glad you asked! Not everything in regression is about getting the best predictions. In some fields, such as human behavior, you would expect to achieve scores much lower then 50\\%! For inference, perhaps 0.3 is enough to measure an effect! Yes, there is more to machine learning than prediction. Inference can be the goal as well!\n",
    "\n",
    "[Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null prediction for null model\n",
    "null_prediction = y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null residuals\n",
    "null_residuals = y - null_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null sum of squares\n",
    "null_ss = (null_residuals ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8972106381789522"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R2 by hand\n",
    "1 - sse / null_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8972106381789522"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R2 from sklearn\n",
    "metrics.r2_score(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8972106381789522"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R2 from model\n",
    "lr.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
