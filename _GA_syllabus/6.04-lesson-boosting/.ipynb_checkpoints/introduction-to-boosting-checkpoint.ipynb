{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Boosting\n",
    "\n",
    "_Authors: Kiefer Katovich (SF), Matt Brems (DC)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Understand the differences between bagging and boosting.\n",
    "- Understand how boosting is an ensemble method.\n",
    "- Learn the pros and cons to using boosting models.\n",
    "- Learn the effect of boosting on the bias-variance trade-off.\n",
    "- Learn the math and procedure for AdaBoost, the \"classic\" boosting model.\n",
    "- Understand the differences between AdaBoost and gradient-boosting models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson Guide\n",
    "\n",
    "- [Boosting as an Ensemble Method](#intro)\n",
    "- [Pros and Cons of Boosting](#pros-cons)\n",
    "- [A Visual Description of Bagging vs. Boosting](#viz)\n",
    "- [Boosting and the Bias-Variance Trade-Off](#bias-variance)\n",
    "- [AdaBoost](#adaboost)\n",
    "    - [Training Example: Weights](#ex-weights)\n",
    "- [AdaBoost Visualization](#adaboost-viz)\n",
    "- [Gradient Boosting Models](#gradient)\n",
    "- [Gradient Boosting Visualization](#gboost-viz)\n",
    "- [The difference Between AdaBoost and Gradient Boosting](#the-difference-between-the-adaboost-and-gradient-boosting)\n",
    "- [Additional Resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Boosting as an Ensemble Method\n",
    "\n",
    "---\n",
    "\n",
    "Boosting is another ensemble method with a different approach to bagging. Boosting takes a weak base learner and tries to make it a strong learner by retraining it on the misclassified samples.\n",
    "\n",
    "1) **Base model fitting is an iterative procedure**: It cannot be run in parallel.\n",
    "- **Weights are assigned to observations to indicate their \"importance:\"** Samples with higher weights are given higher influence on the total error of the next model, prioritizing those observations.\n",
    "- **Weights change at each iteration with the goal of correcting the errors/misclassifications of the previous iteration**: The first base estimator is fit with uniform weights on the observations.\n",
    "- **Final prediction is typically constructed by a weighted vote**: Weights for each base model depend on their training errors or misclassification rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='pros-cons'></a>\n",
    "## Pros and Cons of Boosting\n",
    "\n",
    "---\n",
    "\n",
    "### Pros\n",
    "\n",
    "- Achieves higher performance than bagging when the hyperparameters are properly tuned.\n",
    "- Works equally well for classification and regression.\n",
    "- Can use \"robust\" loss functions that make the model resistant to outliers.\n",
    "\n",
    "---\n",
    "\n",
    "### Cons\n",
    "\n",
    "- Difficult and time consuming to properly tune hyperparameters.\n",
    "- Cannot be parallelized like bagging (bad scalability when there are huge amounts of data).\n",
    "- Higher risk of overfitting compared to bagging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='viz'></a>\n",
    "![boostvsbag](./images/BoostingVSBagging.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='bias-variance'></a>\n",
    "## Boosting and the Bias-Variance Trade-Off\n",
    "\n",
    "---\n",
    "\n",
    "Recall that **bagging aims to reduce variance**.\n",
    "\n",
    "**Boosting aims to reduce bias** (and can reduce variance a bit as well)!\n",
    "\n",
    "### Why?\n",
    "\n",
    "The rationale/theory behind boosting is to combine **many weak learners into a single strong learner.**\n",
    "\n",
    "Instead of using deep/full decision trees like in bagging, **boosting uses shallow/high-bias base estimators.**\n",
    "\n",
    "Thus, each weak learner has:\n",
    "\n",
    "- Low variance.\n",
    "- High bias.\n",
    "\n",
    "It uses iterative fitting to explain error/misclassification unexplained by the previous base models and reduces bias without increasing variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='adaboost'></a>\n",
    "## AdaBoost\n",
    "\n",
    "---\n",
    "\n",
    "AdaBoost is the original boosting algorithm. Predictions from AdaBoost follow this formula:\n",
    "\n",
    "\n",
    "### $$ AdaBoost(X) = sign\\left(\\sum_{t=1}^T\\alpha_t h_t(X)\\right) $$\n",
    "\n",
    "Where:\n",
    "\n",
    "$AdaBoost(X)$ is the classification predictions for $y$ using predictor matrix $X$.\n",
    "\n",
    "$T$ is the set of \"weak learners.‚Äù\n",
    "\n",
    "$\\alpha_t$ is the contribution weight for weak learner $t$.\n",
    "\n",
    "$h_t(X)$ is the prediction of weak learner $t$.\n",
    "\n",
    "$y$ is binary **with values of negative one and one.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The core principle of AdaBoost is to **fit a sequence of weak learners** (i.e., models that are only slightly better than random guessing, such as a single-split tree) **on repeatedly modified versions of the data**. After each fit, the importance weights on each observation need to be updated. \n",
    "\n",
    "The predictions are then combined through a weighted majority vote (or sum) to produce the final prediction. AdaBoost, like all boosting ensemble methods, focuses the next model's fit on the misclassifications/weaknesses of the prior models.\n",
    "\n",
    "All training examples start with equal importance weighting. When we finish training a classifier, we update the importance weighting of the classifier itself, represented by alpha $\\alpha$:\n",
    "\n",
    "### $$ \\alpha_t = \\frac{1}{2}ln \\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right) \\text{where } \\epsilon_t < 1$$\n",
    "\n",
    "Where $\\epsilon_t$ is the misclassification rate for the current classifier:\n",
    "\n",
    "### $$ \\epsilon_t = \\frac{\\text{misclassifications}_t}{\\text{observations}_t} $$\n",
    "\n",
    "As iterations continue, **examples that are difficult to predict receive ever-increasing influence**. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='adaboost-viz'></a>\n",
    "## AdaBoost Visualization\n",
    "\n",
    "![boostvsbag](./images/adaboost-viz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='ex-weights'></a>\n",
    "### Training Example Weights\n",
    "\n",
    "AdaBoost sets up a weight vector on the observations, denoted as $D_t$ where $t$ is the current model iteration. $D_t$ is a probability distribution that determines how likely it is that a given observation will be selected as part of the training set for the current estimator.\n",
    "\n",
    "The $\\alpha$ weighting of the last-fit estimator is used in the equation for the weighting distribution. The update equation is:\n",
    "\n",
    "### $$ D_{t+1}(i) = D_t(i) \\cdot e^{-\\alpha_t \\cdot y_i \\cdot h_t(x_i)} = D_t(i) \\cdot (e^{\\alpha_t})^{-y_i \\cdot h_t(x_i)}.$$\n",
    "\n",
    "- $D_t$ is the vector of observation indices, $D_t(i)$ is the ith datapoint in $D_t$,\n",
    "- $x_i$ is the observation at the index, $y_i$ is the target, and\n",
    "- $h_t$ is the previous model fit in the boosting chain.\n",
    "\n",
    "> Note that $e^{\\alpha_t}$ is always positive, and $-y_i h_t(x_i)$ is either -1 (if the sample is correctly classified) or +1 (if the sample is incorrectly classified).\n",
    "\n",
    "Lastly, we'll divide the weights by the sum of weights to normalize them, ensuring that they sum to one and form a probability distribution:\n",
    "\n",
    "### $$ D_{t+1}(i) = \\frac{D_{t+1}(i)}{\\sum_{i=1}^N D_{t+1}(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='gradient'></a>\n",
    "## Gradient Boosting Models\n",
    "\n",
    "---\n",
    "\n",
    "Gradient boosting classifiers are a generalization of boosting to arbitrary, differentiable loss functions. The intuition behind this mechanism is to:\n",
    "\n",
    "1. Fit a model $F$ to the data.\n",
    "2. Look at the difference between our observed $y$ and our model $F$. (The $y_i - F(x_i)$ can be thought of as residuals!)\n",
    "3. Fit a second model, $F_2$, to (roughly) the residuals $y_i - F(x_i)$.\n",
    "4. Aggregate your model $F$ and $F_2$. While we won't get into the details now, we can interpret residuals as negative gradients. By doing this, we can apply our gradient descent algorithm to optimize our loss and generalize this to many loss functions.\n",
    "\n",
    "GBRT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems. Gradient tree boosting models are used in a variety of areas, including web search ranking and ecology.\n",
    "\n",
    "[This presentation](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf) shows a visual example of AdaBoost at work and how AdaBoost laid the groundwork for gradient boosting classifiers.\n",
    "\n",
    "**The advantages of GBRT are:**\n",
    "\n",
    "- Natural handling of mixed data types (= heterogeneous features).\n",
    "- Predictive power.\n",
    "- Robustness to outliers in output space (via robust loss functions).\n",
    "\n",
    "**The disadvantages of GBRT are:**\n",
    "- Scalability: Due to the sequential nature of boosting, it can hardly be parallelized.\n",
    "- Difficult hyperparameters to tune.\n",
    "\n",
    "\n",
    "> _For more detailed explanations, see [here](https://www.quora.com/What-is-an-intuitive-explanation-of-Gradient-Boosting) and [here](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/)._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='gboost-viz'></a>\n",
    "## Gradient Boosting Visualization\n",
    "![boostvsbag](./images/gboost-viz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='the-difference-between-the-adaboost-and-gradient-boosting'></a>\n",
    "## The Difference Between the AdaBoost and Gradient Boosting\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- AdaBoost is about reweighting the preceding model's errors in subsequent iterations.\n",
    "- Gradient boosting is about fitting subsequent models to the residuals of the last model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n",
    "\n",
    "---\n",
    "\n",
    "Let's practice boosting in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "---\n",
    "\n",
    "There is no lab today!  You can choose your own adventure instead.  Below are some options for how you may want to spend your time.\n",
    "\n",
    "- Practice boosting on a your own dataset.\n",
    "- Take a deep-dive into how AdaBoost works by implementing it from scratch in the `adaboost-from-scratch.ipynb` notebook. (Solutions provided)\n",
    "- `XGBoost` is a popular boosting package, seperate from `sklearn`.  (Many Kaggle competitions have been won with XGBoost.)  Try installing it and using it on a sample dataset (maybe the diabetes data included with this repo.) [http://xgboost.readthedocs.io/en/latest/](http://xgboost.readthedocs.io/en/latest/)\n",
    "- Work on project 3\n",
    "- Catch up on labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='resources'></a>\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "---\n",
    "\n",
    "- [Random Forest on Wikipedia](https://en.wikipedia.org/wiki/Random_forest)\n",
    "- [Quora Question on Random Forests](https://www.quora.com/How-does-randomization-in-a-random-forest-work?redirected_qid=212859)\n",
    "- [Scikit-Learn Ensemble Methods](http://scikit-learn.org/stable/modules/ensemble.html)\n",
    "- [Scikit-Learn Random Forest Classifiers](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "- [Academic Introduction to Adaptive Boosting](http://rob.schapire.net/papers/explaining-adaboost.pdf)\n",
    "- [Stack Exchange AdaBoost vs. Gradient Boosting](http://stats.stackexchange.com/questions/164233/intuitive-explanations-of-differences-between-gradient-boosting-trees-gbm-ad)\n",
    "- [A Gentle Introduction to Gradient Boosting](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf)\n",
    "- [Quora on Intuitive Explanations of AdaBoost](https://www.quora.com/What-is-AdaBoost)\n",
    "- A Lighter [Math Introduction](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf) to AdaBoosting and Gradient Boosting\n",
    "- A [Walk Through](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/) on Tuning Gradient Boosting Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
