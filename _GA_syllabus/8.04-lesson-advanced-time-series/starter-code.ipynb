{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Time Series Modeling\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this lesson, students should be able to:\n",
    "- Create a proper train/test split on time series data.\n",
    "- Identify when seasonality exists in time series data.\n",
    "- Fit and tune a SARIMAX model.\n",
    "- Fit a multivariate time series model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review\n",
    "\n",
    "<details><summary>Check: When do we use an AR (autoregressive) model? (THREAD)</summary>\n",
    "\n",
    "- We use an AR model when we are measuring/predicting **long-term trends**.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>Check: When do we use an MA (moving average) model? (THREAD)</summary>\n",
    "\n",
    "- We fit an MA model when we anticipate our time series will have **substantial, sudden shocks** in the data.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data.\n",
    "stocks = pd.read_csv('./datasets/stocks_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We're going to work with stock data. We have:\n",
    "- `AAPL`: the monthly volume of stocks traded for AAPL (Apple).\n",
    "- `DOW`: the monthly volume of stocks traded for DOW (Dow Chemical Company).\n",
    "- `INTC`: the monthly volume of stocks traded for INTC (Intel).\n",
    "- `NKE`: the monthly volume of stocks traded for NKE (Nike).\n",
    "- `SBUX`: the monthly volume of stocks traded for SBUX (Starbucks).\n",
    "- `YHOO`: the monthly volume of stocks traded for YHOO (Yahoo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index to be date.\n",
    "stocks.index = pd.date_range(start = '1/1/1998',\n",
    "                                periods = len(stocks['date']),\n",
    "                                freq = 'M')\n",
    "\n",
    "# Drop date column.\n",
    "stocks.drop(columns = 'date', inplace = True)\n",
    "\n",
    "# Print shape of dataframe.\n",
    "print(stocks.shape)\n",
    "\n",
    "# Check head of dataframe.\n",
    "stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data.\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(stocks['DOW'])\n",
    "plt.title(label = 'DOW Monthly Volume', fontsize=18)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this for now - we'll use this later!\n",
    "stocks['INTC_lag_1'] = stocks['INTC'].shift(1)\n",
    "stocks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What did the last cell of code do?</summary>\n",
    "\n",
    "- Created a new variable called `INTC_lag_1` that shifts the volume for `INTC` down one value.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "\n",
    "Our goal, as with all train/test splits, is to:\n",
    "- avoid overfitting in our model, and\n",
    "- to get an unbiased estimate of model performance on new, \"unseen\" data.\n",
    "\n",
    "When fitting a time series model, we shouldn't do a random train-test split like we do with non-correlated data.\n",
    "\n",
    "Since our goal with time series analysis is almost always to forecast values forward in time, the idea with a time series train/test split is to train on earlier data and test/evaluate on later data.\n",
    "\n",
    "Most commonly, we'll set our:\n",
    "- training set to be the \"first\" 67% - 80% of our data timewise.\n",
    "- test set be the \"last\" 20% - 33% timewise.\n",
    "\n",
    "Let's split our dataframe by taking the first 80% of rows for training and the rest for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What index gives us our 80th percentile of rows?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training data to be first 80% of rows.\n",
    "train = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training data to be last 20% of rows.\n",
    "test ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus**: Cross-validation is also more complicated with time series data; you can check out more [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) and [here](https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an AR(1) model on training data.\n",
    "# Note this training data consists of the \n",
    "# time periods 0 through 149.\n",
    "ar1 = ARIMA()\n",
    "\n",
    "# Fit AR(1) model.\n",
    "model = \n",
    "\n",
    "# Generate predictions based on test set.\n",
    "# Start at time period 150 and end at 187.\n",
    "preds = \n",
    "\n",
    "# Evaluate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data.\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Plot training data in blue.\n",
    "plt.plot(train['DOW'], color = 'blue')\n",
    "\n",
    "# Plot testing data in orange.\n",
    "plt.plot(test['DOW'], color = 'orange')\n",
    "\n",
    "# Plot predicted values in green.\n",
    "plt.plot(preds, color = 'green')\n",
    "\n",
    "plt.title(label = 'DOW Monthly Volume', fontsize=18)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't a particularly good model. Let's see if we can improve it by fitting an ARIMA model.\n",
    "\n",
    "Remember:\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "    ARIMA(p,d,q) \\Rightarrow Y_t - Y_{t-d} &=& AR(p) + MA(q) \\\\\n",
    "    Y_t - Y_{t-d} &=& \\mu + \\sum_{k=1}^p \\beta_kY_{t-k} + \\sum_{i=1}^q w_i\\varepsilon_{t-i} + \\varepsilon_t\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "<details><summary>Check: What assumption must be met in order for us to fit an ARIMA model?</summary>\n",
    "\n",
    "- Stationarity\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>Check: How do we check whether or not this assumption is met?</summary>\n",
    "\n",
    "- Visually examining the original data plot (for a constant mean) and autocorrelation plot (for autocorrelation).\n",
    "- Execute the augmented Dickey-Fuller test.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Check: How would you interpret this $p$-value?</summary>\n",
    "\n",
    "- Because $p$ is less than our reasonable choices for $\\alpha$, we have enough evidence to reject the null hypothesis and we accept that\n",
    "our time series is stationary.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the right values of $p$, $d$, and $q$.\n",
    "\n",
    "You can get a sense for which parameters will work best based on autocorrelation and partial autocorrelation charts.\n",
    "\n",
    "**Note**: We will walk through this algorithm, but this algorithm will be a helpful reference.\n",
    "\n",
    "Below are some general guidelines (edited from [this site](https://people.duke.edu/~rnau/411arim3.htm) and [this site](https://people.duke.edu/~rnau/arimrule.htm)):\n",
    "1. **Start by finding the correct value of $d$.** This can be done by differencing your data and by checking the results of your Dickey-Fuller test.\n",
    "2. **Generate the PACF (partial autocorrelation function) on your differenced data.** If your PACF has a sharp cut-off and the first lag of your ACF (autocorrelation function) value is positive, choose an $AR(p)$ term where $p$ is the lag in the PACF after the cut-off.\n",
    "3. **Generate the ACF on your differenced data.** If the ACF has a sharp cut-off and the lag-1 ACF value is negative, choose an $MA(q)$ term where $q$ is the lag in the ACF after the cut-off.\n",
    "4. If both the ACF and PACF show a gradual decay, an ARMA model is likely appropriate as opposed to AR or MA alone. This is normally done through a \"manual GridSearch.\"\n",
    "    \n",
    "In general, you should first try to choose an AR or MA model as opposed to an ARMA model, if you can. It is possible for the AR and MA terms to work against each other in the model and create an overly complex representation.\n",
    "\n",
    "**In summary**: there's a lot of guessing and checking involved... but the Dickey-Fuller test, the ACF plot, and the PACF plot will be your friends here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate autocorrelation plot on training data.\n",
    "\n",
    "\n",
    "# Note: we would generate the ACF plot on \n",
    "# differenced data, if we needed to difference\n",
    "# in order to achieve stationarity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate partial autocorrelation plot on training data.\n",
    "\n",
    "\n",
    "# Note: we would generate the PACF plot on \n",
    "# differenced data, if we needed to difference\n",
    "# in order to achieve stationarity!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does my PACF have a sharp cut-off?**\n",
    "> It appears so. I observe a sharp cut-off between lags 0 and 1 as well as a sharp cut-off between lags 1 and 2 in this image. I'll fit two models and compare them (one assuming the cut-off is between 0 and 1, and one assuming the cut-off is between 1 and 2).\n",
    "\n",
    "**Is the first-lag value in the autocorrelation plot positive or negative?**\n",
    "> It is positive. (Note that the first-lag value should always be the same in the ACF and PACF plots.)\n",
    "\n",
    "**What model should we fit?**\n",
    "> \"If your PACF has a sharp cut-off and the first lag of your ACF (autocorrelation function) value is positive, choose an $AR(p)$ term where $p$ is the lag in the PACF after the cut-off.\" Thus, I will fit an $AR(1)$ model and an $AR(2)$ model and see how well they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an AR(1) model on training data.\n",
    "# Note this training data consists of the \n",
    "# time periods 0 through 149.\n",
    "ar1 = ARIMA(endog = train['DOW'],\n",
    "            order = (1, 0, 0)) # (p, d, q)\n",
    "\n",
    "# Fit AR(1) model.\n",
    "model = ar1.fit()\n",
    "\n",
    "# Generate predictions based on test set.\n",
    "# Start at time period 150 and end at 187.\n",
    "preds = model.predict(start=150, end=187)\n",
    "\n",
    "# Evaluate predictions.\n",
    "mean_squared_error(test['DOW'], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data.\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train['DOW'], color = 'blue')\n",
    "plt.plot(test['DOW'], color = 'orange')\n",
    "plt.plot(preds, color = 'green')\n",
    "plt.title(label = 'DOW Monthly Volume with AR(1) Predictions', fontsize=18)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an AR(2) model on training data.\n",
    "# Note this training data consists of the \n",
    "# time periods 0 through 149.\n",
    "ar2 = ARIMA(endog = train['DOW'],\n",
    "            order = ) # (p, d, q)\n",
    "\n",
    "# Fit AR(2) model.\n",
    "model = ar2.fit()\n",
    "\n",
    "# Generate predictions based on test set.\n",
    "# Start at time period 150 and end at 187.\n",
    "preds = model.predict(start=150, end=187)\n",
    "\n",
    "# Evaluate predictions.\n",
    "mean_squared_error(test['DOW'], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data.\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train['DOW'], color = 'blue')\n",
    "plt.plot(test['DOW'], color = 'orange')\n",
    "plt.plot(preds, color = 'green')\n",
    "plt.title(label = 'DOW Monthly Volume with AR(2) Predictions', fontsize=18)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes. Neither of these appear to be particularly good, either. What else can we try?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonal Model\n",
    "\n",
    "We can think of seasonality as a repeating pattern (like a cycle) that occurs over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data.\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(stocks['DOW'])\n",
    "plt.title(label = 'DOW Monthly Volume', fontsize=18)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>By looking at this data, is there is strong evidence of seasonality? If so, what evidence supports seasonality?</summary>\n",
    "\n",
    "- Yes! There are repeated spikes that seem to occur every 16 months or so.\n",
    "- If we look back on our PACF plot, we note that there is a significant value around lag 16.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate partial autocorrelation plot on training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit a **seasonal ARIMA (SARIMA) model** to our data. This model will:\n",
    "- Have the ARIMA components we've described thus far, which are good for stationary trends.\n",
    "- Include a seasonal component that will allow us to also account for seasonality that occurs over time.\n",
    "\n",
    "The SARIMA model is given by $ARIMA(p, d, q) × (P, D, Q, S)$, where:\n",
    "- $ARIMA(p, d, q)$ is the ARIMA model we've already discussed with AR order $p$, MA order $q$, and differencing order $d$.\n",
    "- $(P, D, Q, S)$ refers to the seasonal AR order $P$, the seasonal MA order $Q$, the seasonal differencing order $D$, and the length of the season $S$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a SARIMA(1, 0, 0) x (1, 0, 0, 16) model.\n",
    "\n",
    "sarima = SARIMAX( ,\n",
    "                  ,              # (p, d, q)\n",
    "                  ) # (P, D, Q, S)\n",
    "\n",
    "# Fit SARIMA model.\n",
    "model = sarima.fit()\n",
    "\n",
    "# Generate predictions based on test set.\n",
    "# Start at time period 150 and end at 187.\n",
    "preds = model.predict(start=150, end=187)\n",
    "\n",
    "# Evaluate predictions.\n",
    "print(mean_squared_error(test['DOW'], preds))\n",
    "\n",
    "# Plot data.\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train['DOW'], color = 'blue')\n",
    "plt.plot(test['DOW'], color = 'orange')\n",
    "plt.plot(preds, color = 'green')\n",
    "plt.title(label = 'DOW Monthly Volume with SARIMA(1, 0, 0) x (1, 0, 0, 16) Predictions', fontsize=16)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our predictions are more complex and visually better match the data!  Let's try tweaking parameters $P$, $D$, $Q$, and $S$ to see how we can get our green line to better match the orange line.\n",
    "\n",
    "We will assume that our value, $S$, is 16 as it appears (based on the ACF plot and PACF plot) that each season/period is approximately 16 months.\n",
    "\n",
    "Generally, this is \"most easily\" done through a manual GridSearch process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting MSE and (P, D, Q).\n",
    "\n",
    "\n",
    "for P in range(3):\n",
    "    for Q in range(3):\n",
    "        for D in range(3):\n",
    "            try:\n",
    "                # Instantiate SARIMA model.\n",
    "                sarima = SARIMAX(endog = train['DOW'],\n",
    "                                 order = (1, 0, 0),              # (p, d, q)\n",
    "                                 seasonal_order = (P, D, Q, 16)) # (P, D, Q, S)\n",
    "\n",
    "                # Fit SARIMA model.\n",
    "                model = sarima.fit()\n",
    "\n",
    "                # Generate predictions based on training set.\n",
    "                # Start at time period 0 and end at 149.\n",
    "                preds = model.predict(start=0, end=149)\n",
    "\n",
    "                # Evaluate predictions.\n",
    "                print(f'The MSE for (1, 0, 0)x({P},{D},{Q},16) is: {mean_squared_error(train[\"DOW\"], preds)}')\n",
    "                \n",
    "                # Save for final report.\n",
    "                if mse > mean_squared_error(train[\"DOW\"], preds):\n",
    "\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "print(f'Our model that minimizes MSE on the training data is the SARIMA(1, 0, 0)x({final_P},{final_D},{final_Q},16).')\n",
    "print(f'This model has an MSE of {mse}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What should our main concerns be about a manual GridSearch process like this?</summary>\n",
    "\n",
    "- The amount of time it takes to fit our models!\n",
    "- We aren't cross-validating; we're only evaluating on all of our training data, which will likely end up overfitting. (Cross-validation is also more complicated with time series data; you can check out more [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) and [here](https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/).)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and visualize a SARIMA(1, 0, 0) x (2, 0, 0, 16) model.\n",
    "\n",
    "sarima = SARIMAX()\n",
    "\n",
    "# Fit SARIMA model.\n",
    "model = sarima.fit()\n",
    "\n",
    "# Generate predictions based on test set.\n",
    "# Start at time period 150 and end at 187.\n",
    "preds = model.predict(start=150, end=187)\n",
    "\n",
    "# Evaluate predictions.\n",
    "print(mean_squared_error(test['DOW'], preds))\n",
    "\n",
    "# Plot data.\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train['DOW'], color = 'blue')\n",
    "plt.plot(test['DOW'], color = 'orange')\n",
    "plt.plot(preds, color = 'green')\n",
    "plt.title(label = 'DOW Monthly Volume with SARIMA(1, 0, 0) x (2, 0, 0, 16) Predictions', fontsize=16)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eXogenous Predictors\n",
    "\n",
    "In time series analysis, we spend most of our time focused on how we can quantify the relationship between $Y_t$ and $Y_{t-k}$ and then use that relationship to forecast values of $Y_t$.\n",
    "\n",
    "Future performance of Dow Chemical probably isn't limited to just past performance of Dow Chemical. The performance of Dow Chemical will be based on a whole host of things: how well its competitors are doing, how agriculture is doing (one of Dow's focuses), and so on.\n",
    "\n",
    "However, we can (and should!) still use $X$ variables in our data.\n",
    "- In `statsmodels` and in various disciplines like econometrics, the $X$ variables are known as `exogenous`.\n",
    "- More on exogenous/endogenous [here](http://www.statsmodels.org/stable/endog_exog.html).\n",
    "\n",
    "In fact, the SARIMAX model stands for \"Seasonal Autoregressive Integrated Moving Average with eXogenous regressors model.\"\n",
    "\n",
    "#### One important practical note: \n",
    "If you're going to fit a SARIMAX model, make sure that your $X$ variable makes sense - especially timewise.\n",
    "> For example, if we want to forecast Dow Chemical stock volume for December 2018 based on Intel's stock volume, I probably shouldn't use Intel's stock volume in December 2018 as an input. (I'm forecasting a value of $Y$ when I wouldn't have access to that value of $X$.)\n",
    "\n",
    "I should probably lag my $X$ variable by at least one time point. \n",
    "\n",
    "\n",
    "| Index | $DOW_t$ | $INTC_{t-1}$ |\n",
    "|-------|---------|--------------|\n",
    "| 1     | $DOW_1$ | NA           |\n",
    "| 2     | $DOW_2$ | $INTC_1$     |\n",
    "| 3     | $DOW_3$ | $INTC_2$     |\n",
    "| 4     | $DOW_4$ | $INTC_3$     |\n",
    "\n",
    "Remember: we did this above!\n",
    "> `stocks['INTC_lag_1'] = stocks['INTC'].shift(1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and visualize a SARIMAX(1, 0, 0) x (2, 0, 0, 16) model.\n",
    "\n",
    "sarima = SARIMAX(endog = ,\n",
    "                 order = (1, 0, 0),              # (p, d, q)\n",
    "                 seasonal_order = (2, 0, 0, 16), # (P, D, Q, S)\n",
    "                 exog = )\n",
    "\n",
    "# Fit SARIMA model.\n",
    "model = sarima.fit()\n",
    "\n",
    "# Generate predictions based on test set.\n",
    "# Start at time period 150 and end at 186.\n",
    "preds = model.predict(start=150, end=186, exog=test[['INTC_lag_1']])\n",
    "\n",
    "# Evaluate predictions.\n",
    "print(mean_squared_error(test['DOW'].iloc[1:], preds))\n",
    "\n",
    "# Plot data.\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train['DOW'], color = 'blue')\n",
    "plt.plot(test['DOW'], color = 'orange')\n",
    "plt.plot(preds, color = 'green')\n",
    "plt.title(label = 'DOW Monthly Volume with SARIMA(1, 0, 0) x (2, 0, 0, 16) Predictions', fontsize=16)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-Up\n",
    "We have covered **a lot** about time series analysis:\n",
    "- Pandas tricks for handling time series data.\n",
    "- Autocorrelation and partial autocorrelation.\n",
    "- Autoregressive models.\n",
    "- Moving average models.\n",
    "- Integrated autoregressive and moving average models.\n",
    "- Properly creating a train/test split for time series data.\n",
    "- Fitting a model that accounts for seasonality.\n",
    "- Tuning model parameters (and some of the pitfalls of doing so).\n",
    "- Fittng a time series model with additional independent (exogenous) variables.\n",
    "\n",
    "There's so much information about time series analysis out there; we've only just scratched the surface! However, hopefully you have greater context for how to properly handle time series problems and what to keep your eye out for as you build models with some [temporal](http://desktop.arcgis.com/en/arcmap/10.3/map/time/what-is-temporal-data.htm) component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interested in exploring more?\n",
    "- Fit a model forecasting Google's volume based on four other tech stocks: Facebook, Apple, Intel, and Yahoo.\n",
    "- Explore cross-validation ([here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) and [here](https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/)) in time series data for better performance.\n",
    "- Rather than using a metric like MSE or $R^2$, consider the application here. If you build an algorithm that can help you decide whether to buy/sell/hold stock, then you could quantify that by seeing how much money you might gain/lose if you had applied your models to your test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the right values of $P$, $D$, and $Q$. (BONUS)\n",
    "\n",
    "You can get a sense for which parameters will work best based on autocorrelation and partial autocorrelation charts by following a similar process to above. **Specifically, this time, we want to look at our ACF and PACF charts but focus only on multiples of $S$ (in this case, 16)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
