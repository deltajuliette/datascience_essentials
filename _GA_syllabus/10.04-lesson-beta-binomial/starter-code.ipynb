{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Conjugacy, Beta-Binomial Model, Maximum Likelihood\n",
    "\n",
    "_Authors: Kiefer Katovich (SF), Riley Dallas (Austin)_ \n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the Beta Distribution\n",
    "- Describe the concept of \"conjugacy\" and \"conjugate priors\" in Bayesian statistics\n",
    "- Set up an example of the Beta-Binomial model using election data\n",
    "- Calculate the Maximum Likelihood Estimate\n",
    "- Use the Beta-Binomial model to build our example in a Bayesian framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2020 Democratic Party presidential primaries\n",
    "---\n",
    "\n",
    "For today's lecture, we're going to focus on the 2020 presidential race. It's custom for the incumbent (Trump) to run unopposed for their party, so we'll target the pool of Democratic challengers. Here are the current front runners:\n",
    "\n",
    "- Joe Biden\n",
    "- Bernie Sanders\n",
    "- Kamala Harris\n",
    "- Beto O'Rourke\n",
    "- Elizabeth Warren\n",
    "- Cory Booker\n",
    "- Pete Buttigieg\n",
    "\n",
    "In keeping with the spirit of Bayes, we need to create a prior that represents our beliefs about the outcome of this primary. From there, we'll gather some recent polling results from [FiveThirtyEight](https://projects.fivethirtyeight.com/2020-primaries/democratic/national/) and use that data to update our beliefs as a posterior distribution.\n",
    "\n",
    "## Who should we represent?\n",
    "---\n",
    "\n",
    "Let's imagine we're a consultancy, and all the candidates are interested in our services (because we're awesome, of course). Ethics dictate that we only choose one candidate to represent, so we'll vote on who we'll work for:\n",
    "\n",
    "Cast your vote in the **anonymous** Slack poll:\n",
    "\n",
    "> /poll \"Who should we work for?\" \"Joe Biden\" \"Bernie Sanders\" \"Kamala Harris\" \"Beto O'Rourke\" \"Elizabeth Warren\" \"Cory Booker\" \"Pete Buttigieg\" anonymous limit 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our objective: understand the posterior distribution\n",
    "---\n",
    "\n",
    "$p$ is the parameter we're trying to learn more about: the proportion of votes our candidate will get. \n",
    "\n",
    "To get a posterior for $p$, we'll do the following:\n",
    "1. Establish a **prior** belief about $p$ as a distribution: $f(p)$\n",
    "2. Use recent polling data to create our Binomial likelihood: $f(n,k|p)$\n",
    "    - `n` is the total survey responses\n",
    "    - and `k` is the number of responses in favor of our candidate\n",
    "3. Use that data to update our belief about $p$ as a posterior distribution: $f(p|n,k)$\n",
    "\n",
    "Mathematically, our objective can be summarized using a variant of Bayes' rule:\n",
    "\n",
    "### $$f(p|n,k) \\propto f(n,k|p) * f(p)$$\n",
    "\n",
    "## Our prior and posterior beliefs must be distributions\n",
    "---\n",
    "\n",
    "In Bayesian statistics, we interested in both $p$ **and the uncertainty that surrounds $p$.** Therefore, the prior and posterior will both be distributions, where:\n",
    "- the X-axis is infinite number of percentages between 0 and 1\n",
    "- and the Y-axis represents our **confidence** in each of those percentages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Establish our prior\n",
    "---\n",
    "\n",
    "The first step in Bayesian inference is to create a prior distribution that represents our belief about $p$. Remember, $p$ represents the percentage of votes our candidate is likely to win in the primaries. \n",
    "\n",
    "## The Beta distribution\n",
    "---\n",
    "\n",
    "The Beta distribution works perfectly for our prior because it **represents _a distribution of probabilities_**. \n",
    "\n",
    "The beta distribution is parameterized by two values, $\\alpha$ and $\\beta$.\n",
    "\n",
    "###  $ Beta(\\alpha, \\beta) =\n",
    "\\begin{cases}\n",
    "\\alpha &= \\text{number of successes + 1} \\\\\n",
    "\\beta &= \\text{number of failures + 1}\n",
    "\\end{cases} $\n",
    "\n",
    "The mean of the Beta distribution is defined as:\n",
    "\n",
    "### $\\mu = \\dfrac{\\alpha}{\\alpha + \\beta}$\n",
    "\n",
    "\n",
    "## Uninformed prior\n",
    "---\n",
    "\n",
    "We'll take the naive approach and start with an *uninformed prior*. That is, we assume each value for $p$ is equally likely. To create a uniform Beta distribution, we'll use the following parameters:\n",
    "\n",
    "```python\n",
    "alpha_prior = 1\n",
    "beta_prior = 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our uninformed prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of the prior dist'n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Std of the prior dist'n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the beta prior\n",
    "---\n",
    "\n",
    "In the cell below, let's plot our beta distribution. Use `numpy.linspace` to create a list of probabilities for plotting the x axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of percentages for plotting x-axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prior dist'n using percentages from previous cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Establish our likelihood function\n",
    "---\n",
    "\n",
    "Now we need to update our beliefs with some data. [FiveThirtyEight](https://projects.fivethirtyeight.com/2020-primaries/democratic/national) is an excellent resource for political polling information. \n",
    "\n",
    "Let's use the results from the 5 most recent polls for our likelihood:\n",
    "\n",
    "<div style=\"float: left\">\n",
    "\n",
    "| Dates              | Pollster           | Sample Size | Biden | Sanders | Warren | Harris | Buttigieg | O'Rourke | Booker |\n",
    "|--------------------|--------------------|-------------|-------|---------|--------|--------|-----------|----------|--------|\n",
    "| JUL 31-AUG 2, 2019 | HarrisX            | 914         | 256   | 146     | 73     | 73     | 27        | 27       | 27     |\n",
    "| AUG 1, 2019        | Morning Consult    | 2,419       | 774   | 435     | 363    | 242    | 145       | 73       | 73     |\n",
    "| JUL 31-AUG 1, 2019 | Harris Interactive | 585         | 199   | 99      | 47     | 53     | 23        | 18       | 12     |\n",
    "| JUL 30-AUG 1, 2019 | HarrisX            | 896         | 269   | 161     | 81     | 72     | 36        | 27       | 27     |\n",
    "| JUL 31, 2019       | Morning Consult    | 2,410       | 819   | 458     | 337    | 241    | 145       | 48       | 72     |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the following totals:\n",
    "\n",
    "```python\n",
    "biden = 2317\n",
    "sanders = 1299\n",
    "warren = 901\n",
    "harris = 681\n",
    "buttigieg = 376\n",
    "orourke = 193\n",
    "booker = 211\n",
    "\n",
    "n_surveys = 7224\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pmf'></a>\n",
    "## Review: the Binomial distribution probability mass function\n",
    "---\n",
    "\n",
    "Recall that the number of \"success\" trials in $n$ trials is modeled with the Binomial distribution. The binomial distribution has the probability mass function:\n",
    "\n",
    "### $$ P(k, n \\;|\\; p) = \\binom{n}{k} p^k (1 - p)^{(n-k)} $$\n",
    "\n",
    "Where $k$ is the number of successes,\n",
    "\n",
    "$n$ is the number of total trials,\n",
    "\n",
    "and $p$ is the probability of success for each trial.\n",
    "\n",
    "**We can calculate the probability mass function for a given $n$ and $p$:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coin flips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siblings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polling numbers, likelihood fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='likelihood'></a>\n",
    "## The Binomial likelihood\n",
    "---\n",
    "\n",
    "The likelihood represents the probability of observing $k$ successes out of $n$ trials _given a probability of success $p$._\n",
    "\n",
    "This $p$ can be fixed, say at $p = 0.3$, in which case we would evaluate the likelihood at exactly that point. We could also represent $p$ as a distribution over the range of possible $p$ values. Evaluating the likelihood at all of our different \"hypotheses\" about what $p$ could be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot binom likelihood for our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mle'></a>\n",
    "## The \"Maximum Likelihood Estimate\" for $p$\n",
    "---\n",
    "\n",
    "If we were to _just_ focus on the likelihood part of Bayes Theorem, we could ask \"what is the value of the parameter $p$ that maximizes the value of the likelihood function?\" This is precisely what we do in Frequentist statistics to find our point estimate of a parameter. \n",
    "\n",
    "Remember that Frequentists have no interest in the prior or posterior beliefs about the probability of the parameter's value. Frequentists state that there is no probability associated with a parameter. \n",
    "\n",
    "Because we only take a sample of people, we may by chance measure a percentage of votes for our candidate that deviates from that true probability to some degree. Remember: in Frequentist statistics, it is the data that has a probability rather than the parameter!\n",
    "\n",
    "**For the Binomial distribution, we can easily calculate the value for $p$ that makes our observed data the most likely: it is going to be the fraction of survey responses in favor of our candidate, divided by the total responses.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='beta-conjugate'></a>\n",
    "## Putting it all together: the Beta as a \"conjugate prior\" to the Binomial likelihood\n",
    "---\n",
    "\n",
    "Let's recap what we've done up to this point:\n",
    "1. We're trying to understand $p$: The proportion of votes our candidate will win\n",
    "2. We created a beta distribution $f(p)$ to represent our prior beliefs about $p$\n",
    "3. We've gathered survey data, and calculated the value for $p$ that maximizes the likelihood of the surveys we've observed.\n",
    "\n",
    "Like our prior, our posterior distribution $f(p|n,k)$ will be a beta distribution as well. Because our likelihood function is binomial, creating the parameters $\\alpha_{posterior}$ and $\\beta_{posterior}$ for our beta distribution is a matter of simple addition:\n",
    "\n",
    "### $$ \\begin{aligned}\n",
    "\\alpha_{posterior} &= k + \\alpha_{prior} \\\\\n",
    "\\beta_{posterior} &= n - k + \\beta_{prior}\n",
    "\\end{aligned} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create alpha and beta posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugacy\n",
    "---\n",
    "\n",
    "Our prior and likelihood play nicely together because of one word: conjugacy. **Conjugacy** and \"conjugate priors\" are important concepts in Bayesian statistics. \n",
    "\n",
    "> The essential idea is that the *posterior* distribution is guaranteed to have the same mathematical form as the *prior* distribution when the prior distribution is a conjugate prior to the likelihood function.\n",
    "\n",
    "There are [many conjugate priors and posteriors](https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions). They are extremely useful because they make the prior-posterior update algebraically solvable. When there is no conjugate prior, sampling techniques such as Markov Chain Monte Carlo are often necessary.\n",
    "\n",
    "This lecture covers the most classic conjugate prior scenario: the Beta-Binomial model. Binomial models are appropriate for binary events. The prior distribution on the probability of a binary event is a Beta distribution. As it turns out, the Beta distribution is conjugate to the Binomial likelihood and we are guaranteed to get out a posterior distribution that is also a Beta distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum A Posteriori (MAP): Mode of Posterior Distribution\n",
    "---\n",
    "\n",
    "Once we have $\\alpha_{posterior}$ and $\\beta_{posterior}$ for the Beta distribution, we have completely characterized the probability of the conversion rate.\n",
    "\n",
    "The MAP (i.e., mode of the posterior distribution) can be calculated quite easily if the mathematical form of the distribution is known.  For Beta it is\n",
    "\n",
    "### $$MAP(\\alpha, \\beta) = \\frac{\\alpha - 1}{\\alpha + \\beta - 2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAP for our posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tying it all together\n",
    "---\n",
    "\n",
    "The posterior distribution is the combination of our prior distribution with our likelihood function. The one with more data (in this case our likelihood) will have a greater influence on the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prior vs posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
