{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "## Classification Metrics II\n",
    "\n",
    "_Authors: Matt Brems, Dave Yerrington, Noelle Brown, Jeff Hale_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Calculate various classification model evaluation metrics.\n",
    "- Describe the inverse relationship between sensitivity and specificity.\n",
    "- Understand what the ROC shows and interpret ROC AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data.\n",
    "smoking = pd.read_csv('./data/Whickham.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outcome</th>\n",
       "      <th>smoker</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alive</td>\n",
       "      <td>Yes</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alive</td>\n",
       "      <td>Yes</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dead</td>\n",
       "      <td>Yes</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alive</td>\n",
       "      <td>No</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alive</td>\n",
       "      <td>No</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  outcome smoker  age\n",
       "0   Alive    Yes   23\n",
       "1   Alive    Yes   18\n",
       "2    Dead    Yes   71\n",
       "3   Alive     No   67\n",
       "4   Alive     No   64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out first five rows.\n",
    "smoking.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `outcome`: Whether someone is alive or dead.\n",
    "- `smoker`: Whether somebody smoked or did not smoke.\n",
    "- `age`: Age in years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn outcome to 1 if dead and 0 if alive.\n",
    "smoking['outcome'] = np.where(smoking['outcome']=='Dead', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.719178\n",
       "1    0.280822\n",
       "Name: outcome, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the distribution of our outcome variable.\n",
    "smoking['outcome'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn smoker column to 1 if smoker and 0 if non-smoker.\n",
    "smoking['smoker'] = smoking['smoker'].map({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outcome</th>\n",
       "      <th>smoker</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   outcome  smoker  age\n",
       "0        0       1   23\n",
       "1        0       1   18\n",
       "2        1       1   71\n",
       "3        0       0   67\n",
       "4        0       0   64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoking.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the interesting things about this data is the relationship between age and smoking.\n",
    "- You can read more about it [here](https://www2.stat.duke.edu/courses/Spring08/sta103/whickham.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outcome</th>\n",
       "      <th>smoker</th>\n",
       "      <th>age</th>\n",
       "      <th>interaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   outcome  smoker  age  interaction\n",
       "0        0       1   23           23\n",
       "1        0       1   18           18\n",
       "2        1       1   71           71\n",
       "3        0       0   67            0\n",
       "4        0       0   64            0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How do we usually account for a relationship\n",
    "# between two independent variables?\n",
    "smoking['interaction'] = smoking['smoker']*smoking['age']\n",
    "smoking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up X and y.\n",
    "X = smoking.drop(columns = 'outcome')\n",
    "y = smoking['outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model.\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>In this situation, what term would we use to describe someone who is predicted to be dead but actually is alive? (Remember that alive is coded as 0 and dead is coded as 1.)</summary>\n",
    "\n",
    "- We **falsely** predict someone to be **positive**.\n",
    "- This would be a **false positive**.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>In this situation, what is a true negative?</summary>\n",
    "\n",
    "- We **correctly** predict someone to be **negative**.\n",
    "- Someone who is predicted to be alive (`0`) and actually is alive (`0`).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>If I want to get a good understanding of how our model will do on new data, should I generate a confusion matrix on our training or testing set? Why?</summary>\n",
    "    \n",
    "- Testing set!\n",
    "- If we generate one on our training set, we're going to overestimate the performance of our model... just like if we calculated MSE on our training set.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions.\n",
    "preds = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[210,  27],\n",
       "       [ 30,  62]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate confusion matrix.\n",
    "# Documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-e21fd7953be0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# https://github.com/justmarkham/scikit-learn-tips/blob/master/notebooks/20_plot_confusion_matrix.ipynb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m plot_confusion_matrix(logreg, X_test, y_test, cmap='Blues', \n\u001b[0m\u001b[0;32m      5\u001b[0m                       values_format='d', display_labels=['alive', 'dead']);\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# plot it!\n",
    "# https://github.com/justmarkham/scikit-learn-tips/blob/master/notebooks/20_plot_confusion_matrix.ipynb\n",
    "\n",
    "plot_confusion_matrix(logreg, X_test, y_test, cmap='Blues', \n",
    "                      values_format='d', display_labels=['alive', 'dead']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TN/FP/FN/TP values.\n",
    "\n",
    "tn, fp, fn, tp =\n",
    "\n",
    "# Note that .ravel() will arrange items in a one-dimensional array.\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.ravel.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's the accuracy?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by hand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's the Specificity?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the test specificity of our model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's the sensitivity (recall)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By hand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the precision?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By hand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score\n",
    "\n",
    "The F1 score is the harmonic mean of precision and recall.\n",
    "\n",
    "If you care about precision and recall roughly the same amount, F1 score is a great metric to use.\n",
    "\n",
    "Note that even though all the metrics you’ve seen can be followed by the word score F1 always is. 🤷‍♀️\n",
    "\n",
    "$$\n",
    "2*\\frac{(\\text{Precision}*\\text{Recall})} {(\\text{Precision} + \\text{Recall})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the F1 Score?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by hand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between Sensitivity and Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a dataframe called pred_df that contains:\n",
    "# 1. The true values of our test set.\n",
    "# 2. The predicted probabilities based on our model.\n",
    "\n",
    "pred_df = pd.DataFrame({'true_values': y_test,\n",
    "                        'pred_probs': logreg.predict_proba(X_test)[:,1]})\n",
    "\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 5))\n",
    "\n",
    "# plot distributions of predicted probabilities by actual values\n",
    "for group in pred_df.groupby('true_values'):\n",
    "    sns.distplot(group[1], kde = False, bins = 20, label = f'Actual Outcome = {group[0]}')\n",
    "\n",
    "plt.xlabel('Predicted Probability that Outcome = 1')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What do you notice about this distribution?</summary>\n",
    "\n",
    "- Way more blue than orange.\n",
    "- There's lots of overlap!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 5))\n",
    "\n",
    "# plot distributions of predicted probabilities by actual values\n",
    "for group in pred_df.groupby('true_values'):\n",
    "    sns.distplot(group[1], kde = False, bins = 20, label = f'Actual Outcome = {group[0]}')\n",
    "\n",
    "# Add cutoff line\n",
    "\n",
    "\n",
    "plt.xlabel('Predicted Probability that Outcome = 1')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Where are my true positives, true negatives, false positives, false negatives in this image?</summary>\n",
    "    \n",
    "- True Positive:\n",
    "    - Items I correctly predict to be positive.\n",
    "    - The orange bars (actual `1`) that are to the right of the black line (predicted `1`).\n",
    "- True Negative: \n",
    "    - Items I correctly predict to be negative.\n",
    "    - The blue bars (actual `0`) that are to the left of the black line (predicted `0`).\n",
    "- False Positive:\n",
    "    - Items I incorrectly predict to be positive.\n",
    "    - The blue bars (actual `0`) that are to the right of the black line (predicted `1`).\n",
    "- False Negative: \n",
    "    - Items I incorrectly predict to be negative.\n",
    "    - The orange bars (actual `1`) that are to the left of the black line (predicted `0`).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's adjust our classification threshold to be lower. Instead of using 50% as the cutoff between the positive and negative classes, let's move that threshold down to 10%.\n",
    "- Any observation with a **predicted probability above 10%** would be **predicted to be in the positive class**.\n",
    "- Any observation with a **predicted probability below 10%** would be **predicted to be in the negative class**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>When I moved my classification threshold to the left, what happened to sensitivity and specificity?</summary>\n",
    "\n",
    "- Our number of true negatives decreased and our number of total negatives remains the same.\n",
    "    - $\\text{Specificity} = \\frac{TN}{N} \\Rightarrow \\text{Specificity decreases.}$\n",
    "- Our number of true positives increased and our number of total positives remains the same.\n",
    "    - $\\text{Sensitivity} = \\frac{TP}{P} \\Rightarrow \\text{Sensitivity increases.}$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal should be to build a model such that there is **no overlap** between the blue histogram and the orange histogram!\n",
    "- If there is overlap, we need to recognize the tradeoff between sensitivity and specificity. (As one increases, the other decreases.)\n",
    "- One measure of how much overlap exists between our distributions is the **area under the ROC curve**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operating Characteristic (ROC) Curve\n",
    "\n",
    "Plot the True Positive Rate vs. False Positive Rate for the range of possible decision thresholds and you get the ROC curve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "\n",
    "# add worst case scenario line\n",
    "\n",
    "# add a legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check out [this visualization](http://www.navan.name/roc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC\n",
    "\n",
    "The more area under this blue curve is, the better separated our distributions are.\n",
    "- Check out this gif ([source](https://twitter.com/DrHughHarvey/status/1104435699095404544)):\n",
    "\n",
    "![](https://media.giphy.com/media/H1SZ5oRLIuZ1t1c4Di/giphy.gif)\n",
    "\n",
    "We use the **area under the ROC curve** (abbreviated **ROC AUC** or **AUC ROC**) to quantify the gap between our distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Note: Not to be confused with the ROC AOC.</summary>\n",
    "<img src=\"./images/AOC.jpg\" alt=\"AOC\" width=\"400\"/>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC AUC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting ROC AUC\n",
    "- If you have an ROC AUC of 0.5, your positive and negative populations perfectly overlap and your model is as bad as it can get.\n",
    "- If you have an ROC AUC of 1, your positive and negative populations are perfectly separated and your model is as good as it can get.\n",
    "- The closer your ROC AUC is to 1, the better. (1 is the maximum score.)\n",
    "- If you have an ROC AUC of below 0.5, your positive and negative distributions have flipped sides. By flipping your predicted values (i.e. flipping predicted 1s and 0s), your ROC AUC will now be above 0.5.\n",
    "    - Example: You have an ROC AUC of 0.2. If you change your predicted 1s to 0s and your predicted 0s to 1s, your ROC AUC will now be 0.8!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate one ROC curve per model. The ROC curve is generated by varying our threshold from 0 to 1. This doesn't actually change the threshold or our original predictions, but it helps us to visualize our tradeoff between _sensitivity_ and _specificity_ and understand how well-separated our populations are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing your prediction threshold\n",
    "\n",
    "If you want, you could change your prediction threshold to a custom value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced / Unbalanced Classes\n",
    "\n",
    "Suppose I want to predict the incidence of West Nile Virus (WNV) in Chicago.\n",
    "- 99.9% of my observations are \"no WNV.\"\n",
    "- 0.1% of my observations contain \"WNV.\"\n",
    "\n",
    "If we fit a model and tried to optimize for accuracy, I can predict \"no WNV\" for every location and have an accuracy score that is really, _really_ good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Why is this a bad model?</summary>\n",
    "    \n",
    "- We'll never predict that a location has West Nile Virus, which is probably going to eventually lead to outbreaks of the disease.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification problems, methods generally perform better when we have roughly equally-sized classes. (i.e. 50% in the positive class and 50% in the negative class for binary classification problems.)\n",
    "\n",
    "When we do not have each class equally represented, we say we have **unbalanced classes**.\n",
    "\n",
    "How big of a problem is this? Depends. Realistically, we are probably not going to have a perfect balance of representation from each class. A small amount of imbalance will most likely not matter too much, but a large amount can cause problems. Keep in mind, there are situations where we expect imbalance in our classes (ex. predicting fraud).\n",
    "\n",
    "#### Methods for Dealing with Unbalanced Classes\n",
    "\n",
    "- **Weighting observations.** Some models allow the weighting of classes such as linear and logistic regression, Naive Bayes, Random Forests, SVMs, etc. See an example of this [here](https://towardsdatascience.com/weighted-logistic-regression-for-imbalanced-dataset-9a5cd88e68b).\n",
    "- **Stratified cross-validation.** If we use $k$-fold cross-validation entirely randomly, we may run into issues where some of our folds have no observations from the minority class. Stratifying is almost always a good idea.\n",
    "- **Change threshold for classification.** By adjusting our classification threshold, we might find a better fit for our particular use-case.\n",
    "- **Bias correction.** Gary King wrote a [whitepaper](https://gking.harvard.edu/files/gking/files/0s.pdf) on this topic. This is a rigorous approach and while provide good results it's a bit of work.\n",
    "- **Create synthetic data of minority class.**\n",
    "- **Oversample minority class.**\n",
    "- **Undersample majority class.**\n",
    "- **Combine oversampling majority and undersampling minority classes.**\n",
    "- **Optimize for a specific metric.**\n",
    "- **Get more data!** Cop-out answer. But great if you can! 😀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What is the ROC curve?</summary>\n",
    "    \n",
    "- The ROC curve is a plot of the True Positive Rate (sensitivity) vs. the False Positive Rate (1 - specificity) for all possible decision thresholds.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Let's say you were building a search engine and wanted to build a classification model that would recommend articles based on the search input. What metric would you want to optimize for and why?</summary>\n",
    "    \n",
    "- You could make a case for wanting to minimize false positives (stories that weren't relevant), in which case you'd want to optimize for precision.\n",
    "- You could make a case for wanting to minimize false negatives (not passing along possibly useful content), in which case you'd want to optimize for recall. \n",
    "- Alumni Comment: \"The interviewer seemed more interested in seeing if I knew what the metrics were and explaining what priorities would lead me to optimize for one over the other.\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More interview practice questions on these topics [here](https://kiwidamien.github.io/interview-practice-with-precision-and-recall.html)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
