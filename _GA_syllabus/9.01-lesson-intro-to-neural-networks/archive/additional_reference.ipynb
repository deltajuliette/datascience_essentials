{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "## Introduction to Neural Networks - Additional Reference\n",
    "\n",
    "_Authors: Justin Pounders (SV), David Yerrington (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "In this lesson we will get an overview of basic feed-forward neural networks.  The emphasis will be on terminology and and the fundamental building blocks of these powerful networks.  Later today you will learn how to use Keras, a powerful and (relatively) user-friendly library for building your own networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Concepts\n",
    "\n",
    "#### Neurons\n",
    "\n",
    "A neural network (at its core) is built up of different neurons that are linked together. Each takes in either the original input features or some transformed version of them and puts out a value (or set of values). One neuron looks something akin to this:\n",
    "\n",
    "![](./images/perceptron.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each neuron is going to be the combination of the following:\n",
    "\n",
    "- A **bias** term (akin to a constant or $B_0$ term in a linear regression)\n",
    "- The input terms they've received, each multiplied by a **weight**\n",
    "\n",
    "If our model has one neuron, this looks suspiciously similar to a linear regression:\n",
    "\n",
    "1. take each term\n",
    "2. multiply it by a weight\n",
    "3. sum those new values together \n",
    "4. add an additional bias term\n",
    "\n",
    "That output should, as we train our neural network, get closer and closer to what the output is for that specific set of inputs ($x_1...x_n$). As we'll see, the way we train the network and the way we transform our outputs (plus the number of neurons) distinguishes neural networks from linear regression quite strongly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layers\n",
    "\n",
    "What makes neural networks tick is the idea of hidden layers. Hidden does not mean anything particularly devious here, just that it is not the input or the output layer.\n",
    "\n",
    "Hidden layers can have:\n",
    "- any number of neurons per layer \n",
    "- can be of any number in your model**\n",
    "\n",
    "At each layer each neuron in that layer receives the same weight. However, each neuron is going to transform the data in a different way, based on how we assign or change the weights and bias in that neuron. \n",
    "\n",
    "![](./images/neuralnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the network above, we have two hidden layers and one output layer.\n",
    "\n",
    "- Hidden Layer 1\n",
    "    - 4 Neurons\n",
    "    - Each Neuron has 6 weights and 1 bias term\n",
    "    - Inputs: the original data\n",
    "    - Outputs: one number each\n",
    "- Hidden Layer 2\n",
    "    - 3 Neurons \n",
    "    - Each Neuron has 4 weights and 1 bias term\n",
    "    - Inputs: the four outputs from each Neuron in Hidden Layer 1\n",
    "    - Outputs: one number each\n",
    "- Output Layer\n",
    "    - 1 Neuron\n",
    "    - The one nNuron has 3 weights and 1 bias term\n",
    "    - Inputs: the three outputs from each Neuron in Hidden Layer 2\n",
    "    - Outputs: the final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation\n",
    "\n",
    "So how exactly does information/data propogate through the network?\n",
    "\n",
    "We can write the equation for the activation of the $j$th neuron in the $i$th layer as:\n",
    "\n",
    "### $$ a_j^i = \\sigma \\left( \\sum_k w_{jk}^i a_k^{i-1} + b_j^i \\right)$$\n",
    "\n",
    "There is a decent amount going on in this equation. We will examine the pieces.\n",
    "\n",
    "- $a_j^i$ represents the activation of the $j$th neuron in the $i$th layer. Note that the superscript corresponds to the layer number and the subscript corresponds to the neuron number within the layer.\n",
    "\n",
    "- $a_k^{i-1}$ is the activation of the $k$th neuron in the $i-1$th layer.\n",
    "\n",
    "- $\\sigma$ represents an \"activation function\". More on this later, but it is a function that can transform the activation of neurons. The simplest activation function is the linear activation, $f(x) = x$.\n",
    "\n",
    "- $w_{jk}^i$ represents the weight of the activation in the $k$th neuron in the $i-1$ layer to the $j$th neuron in the $i$th layer. So, $j$ is the destination neuron in the $i$ layer. $k$ is the departure neuron in the previous layer.\n",
    "\n",
    "- $b_j^i$ is the \"bias\" of the $j$th neuron in the $i$th layer. The bias adds a constant to the value of the activation.\n",
    "\n",
    "The gist of the equation is that each neuron is a sum of the weighted activations of neurons that feed into it plus a \"bias\" value, all fed through a final activation function.\n",
    "\n",
    "The formula becomes cleaner in matrix notation. Here is the vectorized version of the formula above:\n",
    "\n",
    "### $$ a^i = f(W^i a^{i-1} + b^i) $$\n",
    "\n",
    "Now there is a weight matrix $W^i$ for each layer $i$. The weight matrix defines the weightings on the previous layer neuron activations to the neurons of the current layer. \n",
    "\n",
    "![](./images/activation.png)\n",
    "\n",
    "Neurons also have an activation function that transforms the output in a certain way. Some common examples are:\n",
    "\n",
    "- [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks): Also known as a Rectified Linear Unit, this turns the output to 0 if the output would be less than 0 (i.e., take the output and feed it through $f(y) = max(0, y)$). This means that the neuron is activated when its output is positive and not activated otherwise. This has the intuitive effect of turning a neuron \"on\" in certain cases and off in other cases.\n",
    "- [Softmax](https://en.wikipedia.org/wiki/Softmax_function): Used frequently at the output layer, this essentially \"squishes\" a bunch of inputs into a normalized scale of 0-1, which is great for creating something akin to a probability of falling into a given class. \n",
    "- [Sigmoid or Logistic](https://en.wikipedia.org/wiki/Logistic_function): Much like how we transformed the linear regression model to change the output to a zero or one through the use of a logistic or sigmoid function, we can do the same as an activation to squash the output to a scale between 0 and 1. \n",
    "\n",
    "There's a wealth of information on different types of activation functions within [this article](https://en.wikipedia.org/wiki/Activation_function) -- different activation functions, hidden layers, and neurons per layer can change how effective your neural network will be!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 2 (5 Minutes plus Over Break)\n",
    "\n",
    "Independently or with a partner (your choice) -- pick two of the following activation functions:\n",
    "\n",
    "- Binary Step\n",
    "    - If $x \\le 0$: $f(x) = 0$; else $f(x) = 1$ \n",
    "- ReLU\n",
    "    - If $x \\le 0$: $f(x) = 0$; else $f(x) = x$\n",
    "- Logistic / Sigmoid\n",
    "    - $f(x) = \\frac{1}{1 + e^{-x}}$\n",
    "- TanH\n",
    "    - $f(x) = \\tanh(x) = \\frac{2}{1 + e^{-2x}} - 1$\n",
    "- Softsign\n",
    "    - $f(x)={\\frac {x}{1+|x|}}$\n",
    "    \n",
    "Write a function in Python for each. Your functions should take in one value ($x$) and output the transformed version of $x$. \n",
    "\n",
    "> Note: $e^x$ can be evaluated using `np.exp(x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to use this function to plot a range of x values\n",
    "def activation_plotter(activation_function):\n",
    "    x = list(range(-10, 11))\n",
    "    y = [activation_function(val) for val in x]\n",
    "    plt.plot(x, y)\n",
    "    plt.ylabel('Activated value for X')\n",
    "    plt.xlabel('X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A: code activationn functions here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
