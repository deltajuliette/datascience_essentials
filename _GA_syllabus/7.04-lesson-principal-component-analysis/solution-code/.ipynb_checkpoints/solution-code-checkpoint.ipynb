{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Principal Component Analysis\n",
    "\n",
    "_Authors: Justin Pounders, Matt Brems, Noelle Brown_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEARNING OBJECTIVES\n",
    "By the end of the lesson, students should be able to:\n",
    "1. Differentiate between feature selection and feature extraction.\n",
    "2. Describe the PCA algorithm.\n",
    "3. Implement PCA in `scikit-learn`.\n",
    "4. Calculate and interpret proportion of explained variance.\n",
    "5. Identify use cases for PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before covering what's here, let's cover some foundations on `Dimensionality reduction` in this separate [intuition deck](https://docs.google.com/presentation/d/1acHgUvkR-qx1SxM5rA8dikHNqMlZFM3t4DNKWZXhYbA/edit#slide=id.g10cc392b88f_1_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import from sklearn.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "# Set a random seed.\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction of Problem\n",
    "\n",
    "Today, we're going to be using the [wine quality](http://www3.dsi.uminho.pt/pcortez/wine/) dataset by Cortez, Cerdeira, Almeida, Matos and Reis.\n",
    "\n",
    "Specifically, we are going to use physicochemical properties of the wine in order to **predict the *quality* of the wine.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6497, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the wine quality datasets.\n",
    "df_red = pd.read_csv('../datasets/winequality-red.csv', sep=';')\n",
    "df_white = pd.read_csv('../datasets/winequality-white.csv', sep=';')\n",
    "\n",
    "# Stack datasets together. (They have the same column names!)\n",
    "df = pd.concat([df_red, df_white])\n",
    "\n",
    "# Check out head of our dataframe.\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a multiple linear regression model in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed acidity           0\n",
       "volatile acidity        0\n",
       "citric acid             0\n",
       "residual sugar          0\n",
       "chlorides               0\n",
       "free sulfur dioxide     0\n",
       "total sulfur dioxide    0\n",
       "density                 0\n",
       "pH                      0\n",
       "sulphates               0\n",
       "alcohol                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set y to be the quality column.\n",
    "y = df['quality']\n",
    "\n",
    "# Set X as all other columns.\n",
    "X = df.drop(columns=['quality'])\n",
    "\n",
    "# How much missing data do we have? --> none for this dataset\n",
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[sklearn polynomial features recap](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n",
    "\n",
    "- if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2]\n",
    "- accordingly, degree-3 would incorporate cubed features as follows: [1,a,b,a^2,ab,b^2,a^3,a^2b,ab^2,b^3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000000e+00, 7.400000e+00, 7.000000e-01, 5.476000e+01,\n",
       "        5.180000e+00, 4.900000e-01, 4.052240e+02, 3.833200e+01,\n",
       "        3.626000e+00, 3.430000e-01],\n",
       "       [1.000000e+00, 7.800000e+00, 8.800000e-01, 6.084000e+01,\n",
       "        6.864000e+00, 7.744000e-01, 4.745520e+02, 5.353920e+01,\n",
       "        6.040320e+00, 6.814720e-01],\n",
       "       [1.000000e+00, 7.800000e+00, 7.600000e-01, 6.084000e+01,\n",
       "        5.928000e+00, 5.776000e-01, 4.745520e+02, 4.623840e+01,\n",
       "        4.505280e+00, 4.389760e-01],\n",
       "       [1.000000e+00, 1.120000e+01, 2.800000e-01, 1.254400e+02,\n",
       "        3.136000e+00, 7.840000e-02, 1.404928e+03, 3.512320e+01,\n",
       "        8.780800e-01, 2.195200e-02],\n",
       "       [1.000000e+00, 7.400000e+00, 7.000000e-01, 5.476000e+01,\n",
       "        5.180000e+00, 4.900000e-01, 4.052240e+02, 3.833200e+01,\n",
       "        3.626000e+00, 3.430000e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's examine with a small subset of X before applying on entire 'X' for model training\n",
    "sample_X = df[['fixed acidity','volatile acidity']].head()\n",
    "\n",
    "trans = PolynomialFeatures(degree=3)\n",
    "poly_sample_X = trans.fit_transform(sample_X)\n",
    "poly_sample_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6497, 364)\n",
      "(6497, 11)\n"
     ]
    }
   ],
   "source": [
    "# To show off the strength of PCA, we're going to make many, many more features.\n",
    "pf = PolynomialFeatures(degree = 3)\n",
    "\n",
    "# Fit and transform our X data using Polynomial Features.\n",
    "X_new = pf.fit_transform(X) # Fit to data, then transform it\n",
    "\n",
    "# How many features do we have now?\n",
    "print(X_new.shape)\n",
    "\n",
    "# How many features did we start out with?\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split our data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.4563.\n",
      "Testing Score: -0.8865.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and fit a linear regression model.\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# Score on training set. (We'll use the default R^2 scoring metric)\n",
    "print(f'Training Score: {round(lm.score(X_train, y_train),4)}.')\n",
    "\n",
    "# Score on testing set.\n",
    "print(f'Testing Score: {round(lm.score(X_test, y_test),4)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Check: What is the problem with this?</summary>\n",
    "    \n",
    "- Our model's performance is really bad. It is poorly fit and overfit (the model hasn't generalized)!\n",
    "- We have a lot of columns relative to our number of rows! (If you have $n$ rows and you're fitting a linear model, it's often advised to keep your number of columns below $\\sqrt{n}$.)\n",
    "    - With this guiding principle, we are clearly way-off as post polynomial features creation, we had 364 features, while $\\sqrt{6497}$ would be ~81.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Check: How can we overcome this problem?</summary>\n",
    "\n",
    "- We can drop features from our model. (However, we need to ensure we're not dropping model critical features! It can also be time-consuming and/or require subject-matter expertise.)\n",
    "- Maybe we can combine features together so that we can get the benefits of most/all of our features. <b>(This is what PCA will do.)</b>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "[Dimensionality reduction](https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/) refers to (approximately) reducing the number of features we use in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Dimensionality reduction has a number of advantages:</summary>\n",
    "\n",
    "- Increases computational efficiency when fitting models.\n",
    "- Can help with addressing a multicollinearity problem.\n",
    "- Makes visualization simpler (or feasible).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Dimensionality reduction can suffer from some drawbacks, though:</summary>\n",
    "\n",
    "- We've invested our time and money into collecting information... why do we want to get rid of it?\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there a way to get the advantages of dimensionality reduction while minimizing the drawbacks?\n",
    "\n",
    "Dimensionality reduction can generally be broken down into one of two categories:\n",
    "\n",
    "<img src=\"../images/dim_red.png\" alt=\"drawing\" width=\"550\"/>\n",
    "\n",
    "- **Feature Selection**\n",
    "    - We drop variables from our model. In other words, we *pick and choose*\n",
    "- **Feature Extraction**\n",
    "    - In feature extraction, we take our existing features and *combine them together* in a particular way. We can then drop some of these \"new\" variables, but the variables we keep are still a combination of the old variables!\n",
    "    - This allows us to still reduce the number of features in our model **but** we can keep all of the most important pieces of the original features!\n",
    "    - Let's consider this example:\n",
    "\n",
    "<img src=\"../images/feast.png\" alt=\"drawing\" width=\"550\"/>\n",
    "\n",
    "### $$\n",
    "\\begin{eqnarray*}\n",
    "X_1, \\ldots, X_p &\\Rightarrow& Z_1, \\ldots, Z_p \\\\\n",
    "\\\\\n",
    "\\text{most important: }Z_1 &=& w_{1,1}X_1 + w_{1,2}X_2 + \\cdots + w_{1,p}X_p \\\\\n",
    "\\text{slightly less important: }Z_2 &=& w_{2,1}X_1 + w_{2,2}X_2 + \\cdots + w_{2,p}X_p \\\\\n",
    "&\\vdots&\\\\\n",
    "\\text{least important: }Z_p &=& w_{p,1}X_1 + w_{p,2}X_2 + \\cdots + w_{p,p}X_p \\\\\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "- What is shown here is, we are taking our original features in `X` and combining them in a certain way to create `Z`\n",
    "- We don't usually care about the values of weights captured in `w` here. They aren't very meaningful and we don't try to interpret them.\n",
    "- You can think of $Z_1$ as a \"<b>high performance</b>\" predictor, where $Z_1$ has all of the <b>best pieces</b> of our original predictors $X_1$ through $X_p$.\n",
    "- As we move down the list toward $Z_p$, the variables will consist of the more \"redundant\" parts of our $X$ variables and become lesser and lesser meaningful. \n",
    "- You can think of $Z_p$ as a \"<b>low performance</b>\" predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Based on above. If we're going to keep three of our new predictors, which three would that be?</summary>\n",
    "    \n",
    "- The first three: $Z_1$, $Z_2$, and $Z_3$.\n",
    "- This is how we do feature extraction.\n",
    "    - We take our old features $X_1$, $X_2$, $X_3$, and $X_4$.\n",
    "    - We turn them into new features $Z_1$, $Z_2$, $Z_3$, and $Z_4$.\n",
    "    - The new features are combinations of our old features.\n",
    "    - If we drop some new features, we're doing dimensionality reduction, but we also keep parts of every old feature!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction can be used as an exploratory/unsupervised learning method or as a pre-processing step for supervised learning later.\n",
    "\n",
    "**Principal component analysis** is one algorithm for doing **feature extraction**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How would you describe the difference between feature selection and feature extraction?</summary>\n",
    "\n",
    "- Feature selection is a process of dropping original features from our model.\n",
    "- Feature extraction is a process of transforming our original features into \"new\" features, then dropping some of the \"new\" features from our model.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "### Big picture, what is PCA doing?\n",
    "1. We are going to look at how all of the $X$ variables relate to one another and summarize these relationships.\n",
    "2. Then, we will take this summary and look at which combinations of our $X$ variables are most important/which relationships are most meaningful.\n",
    "3. We can also quantify how important each combination is and rank these combinations.\n",
    "\n",
    "Once we've taken our original $X$ data and transformed it into $Z$, we can then drop the columns of $Z$ that are \"least important.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you are this [Whale shark](https://en.wikipedia.org/wiki/Whale_shark):\n",
    "\n",
    "<img src=\"../images/whaleshark.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "And you want a snack. Which way would you tilt your head to eat the most krill at once?\n",
    "- anti-clockwise to maximize outcome\n",
    "\n",
    "<img src=\"../images/krill.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Above artwork by [@allison_horst](https://twitter.com/allison_horst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/pca.gif\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "[Source](https://rpubs.com/jormerod/594859)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visually...**\n",
    "\n",
    "> Think of our data floating out in $p$-dimensional space. Each observation is a dot and you can imagine this massive cloud of dots that exists somewhere. PCA is a way to rotate this cloud of dots (formally, a [coordinate transformation](http://farside.ph.utexas.edu/teaching/336k/Newtonhtml/node153.html)). The old axes are the original $X_1$, $X_2$, $\\ldots$ features. **The new axes are the principal components from PCA**.\n",
    "\n",
    "The principal components are the most concise, informative descriptors of our data as a whole.\n",
    "- What does this mean?\n",
    "- If we wanted to take our full data set and condense it into one dimension (think like our $X$ axis), we'd only use $Z_1$.\n",
    "- If we wanted to take our full data set and condense it into two dimensions (think like our $X$ and $Y$ axes), we'd use $Z_1$ and $Z_2$. *(like pc1 and pc2 in below site)*\n",
    "\n",
    "You can head to [this site](http://setosa.io/ev/principal-component-analysis/) that allows us to visualize what PCA does to original data. Play around with the 2D data.\n",
    "\n",
    "- PCA finds a *new coordinate system* in which every point has a new (x,y) value. The axes don't actually mean anything physical; they're combinations of height and weight called \"principal components\" that are chosen to give one axes lots of variation.\n",
    "\n",
    "---\n",
    "\n",
    "### Principal Components\n",
    "\n",
    "- We are looking for new *directions*.\n",
    "\n",
    "**These new *directions* are the \"principal components.\"**\n",
    "\n",
    "> Applying PCA to your data *transforms* your original data columns (variables) onto the new principal component axes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two important notes:\n",
    "\n",
    "1. Train/test split **before** applying PCA!\n",
    "2. Standardize our data **before** applying PCA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our StandardScaler.\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Standardize X_train.\n",
    "X_train = ss.fit_transform(X_train)\n",
    "\n",
    "# Standardize X_test.\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PCA.\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (BONUS) Why decomposition?\n",
    "The way PCA works \"under the hood\" is it takes one matrix and **decomposes** that matrix into multiple matrices.\n",
    "\n",
    "Written out, we might take some matrix $\\mathbf{A}$ and break it down into multiple matrices like this:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\mathbf{A} &=& \\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "Check out [the Wikipedia article](https://en.wikipedia.org/wiki/Matrix_decomposition) for a list of ways to decompose matrices.\n",
    "- A specific method of decomposition commonly used for PCA is known as the [eigendecomposition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) or spectral decomposition of a matrix. However, eigendecompositon requires [diagonalizable](https://en.wikipedia.org/wiki/Diagonalizable_matrix) matrices. To generalize this to non-square/non-diagonalizable matrices, we more commonly use the [Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) (SVD) for PCA. [PCA in Sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) uses SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PCA.\n",
    "pca = PCA(random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(random_state=42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit PCA on the training data.\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform PCA on the scaled training data.\n",
    "Z_train = pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>354</th>\n",
       "      <th>355</th>\n",
       "      <th>356</th>\n",
       "      <th>357</th>\n",
       "      <th>358</th>\n",
       "      <th>359</th>\n",
       "      <th>360</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.948627</td>\n",
       "      <td>-3.258955</td>\n",
       "      <td>-0.832581</td>\n",
       "      <td>-2.804569</td>\n",
       "      <td>4.013368</td>\n",
       "      <td>2.543780</td>\n",
       "      <td>-7.019037</td>\n",
       "      <td>0.210613</td>\n",
       "      <td>-1.044385</td>\n",
       "      <td>4.588738</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.753030e-07</td>\n",
       "      <td>2.244862e-07</td>\n",
       "      <td>-3.355503e-07</td>\n",
       "      <td>1.695424e-07</td>\n",
       "      <td>-2.723171e-08</td>\n",
       "      <td>-4.479860e-07</td>\n",
       "      <td>5.305204e-07</td>\n",
       "      <td>2.465473e-07</td>\n",
       "      <td>-5.421411e-08</td>\n",
       "      <td>-7.468380e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.656720</td>\n",
       "      <td>-5.133174</td>\n",
       "      <td>1.321977</td>\n",
       "      <td>7.870713</td>\n",
       "      <td>7.259017</td>\n",
       "      <td>1.834894</td>\n",
       "      <td>-4.380642</td>\n",
       "      <td>1.323332</td>\n",
       "      <td>-0.326929</td>\n",
       "      <td>0.671756</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.582953e-08</td>\n",
       "      <td>-7.814499e-08</td>\n",
       "      <td>-1.373560e-07</td>\n",
       "      <td>-1.330862e-07</td>\n",
       "      <td>6.570108e-09</td>\n",
       "      <td>7.538702e-10</td>\n",
       "      <td>-6.368494e-08</td>\n",
       "      <td>-7.070962e-08</td>\n",
       "      <td>2.419514e-08</td>\n",
       "      <td>2.008061e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.990220</td>\n",
       "      <td>-3.640704</td>\n",
       "      <td>-4.845163</td>\n",
       "      <td>-1.805145</td>\n",
       "      <td>-4.065424</td>\n",
       "      <td>-7.852356</td>\n",
       "      <td>3.627241</td>\n",
       "      <td>3.246181</td>\n",
       "      <td>5.454453</td>\n",
       "      <td>1.442888</td>\n",
       "      <td>...</td>\n",
       "      <td>2.341384e-07</td>\n",
       "      <td>2.386694e-07</td>\n",
       "      <td>-1.500054e-07</td>\n",
       "      <td>1.673883e-07</td>\n",
       "      <td>-1.234804e-07</td>\n",
       "      <td>1.920235e-07</td>\n",
       "      <td>-1.052504e-07</td>\n",
       "      <td>1.918555e-07</td>\n",
       "      <td>-1.796324e-08</td>\n",
       "      <td>-1.602284e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.978165</td>\n",
       "      <td>37.883902</td>\n",
       "      <td>-1.540496</td>\n",
       "      <td>-18.391432</td>\n",
       "      <td>-18.798863</td>\n",
       "      <td>39.651140</td>\n",
       "      <td>19.141686</td>\n",
       "      <td>-18.185576</td>\n",
       "      <td>0.848820</td>\n",
       "      <td>-16.846516</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.379348e-07</td>\n",
       "      <td>-3.991949e-07</td>\n",
       "      <td>4.642472e-07</td>\n",
       "      <td>-8.555283e-08</td>\n",
       "      <td>1.090973e-08</td>\n",
       "      <td>-6.307144e-08</td>\n",
       "      <td>8.913501e-08</td>\n",
       "      <td>-5.217746e-08</td>\n",
       "      <td>3.277049e-08</td>\n",
       "      <td>2.305878e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.351178</td>\n",
       "      <td>-8.745763</td>\n",
       "      <td>-0.088128</td>\n",
       "      <td>-6.097010</td>\n",
       "      <td>2.148219</td>\n",
       "      <td>1.394471</td>\n",
       "      <td>-1.591332</td>\n",
       "      <td>2.148140</td>\n",
       "      <td>1.898963</td>\n",
       "      <td>1.393446</td>\n",
       "      <td>...</td>\n",
       "      <td>7.049872e-08</td>\n",
       "      <td>1.449276e-08</td>\n",
       "      <td>2.896338e-08</td>\n",
       "      <td>2.498999e-08</td>\n",
       "      <td>-7.445622e-09</td>\n",
       "      <td>-3.288498e-08</td>\n",
       "      <td>-8.061787e-08</td>\n",
       "      <td>1.991670e-10</td>\n",
       "      <td>-1.138442e-08</td>\n",
       "      <td>6.281998e-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 364 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1         2          3          4          5          6    \\\n",
       "0  12.948627  -3.258955 -0.832581  -2.804569   4.013368   2.543780  -7.019037   \n",
       "1   2.656720  -5.133174  1.321977   7.870713   7.259017   1.834894  -4.380642   \n",
       "2  -0.990220  -3.640704 -4.845163  -1.805145  -4.065424  -7.852356   3.627241   \n",
       "3  -4.978165  37.883902 -1.540496 -18.391432 -18.798863  39.651140  19.141686   \n",
       "4  -1.351178  -8.745763 -0.088128  -6.097010   2.148219   1.394471  -1.591332   \n",
       "\n",
       "         7         8          9    ...           354           355  \\\n",
       "0   0.210613 -1.044385   4.588738  ... -1.753030e-07  2.244862e-07   \n",
       "1   1.323332 -0.326929   0.671756  ... -6.582953e-08 -7.814499e-08   \n",
       "2   3.246181  5.454453   1.442888  ...  2.341384e-07  2.386694e-07   \n",
       "3 -18.185576  0.848820 -16.846516  ... -2.379348e-07 -3.991949e-07   \n",
       "4   2.148140  1.898963   1.393446  ...  7.049872e-08  1.449276e-08   \n",
       "\n",
       "            356           357           358           359           360  \\\n",
       "0 -3.355503e-07  1.695424e-07 -2.723171e-08 -4.479860e-07  5.305204e-07   \n",
       "1 -1.373560e-07 -1.330862e-07  6.570108e-09  7.538702e-10 -6.368494e-08   \n",
       "2 -1.500054e-07  1.673883e-07 -1.234804e-07  1.920235e-07 -1.052504e-07   \n",
       "3  4.642472e-07 -8.555283e-08  1.090973e-08 -6.307144e-08  8.913501e-08   \n",
       "4  2.896338e-08  2.498999e-08 -7.445622e-09 -3.288498e-08 -8.061787e-08   \n",
       "\n",
       "            361           362           363  \n",
       "0  2.465473e-07 -5.421411e-08 -7.468380e-15  \n",
       "1 -7.070962e-08  2.419514e-08  2.008061e-16  \n",
       "2  1.918555e-07 -1.796324e-08 -1.602284e-17  \n",
       "3 -5.217746e-08  3.277049e-08  2.305878e-17  \n",
       "4  1.991670e-10 -1.138442e-08  6.281998e-17  \n",
       "\n",
       "[5 rows x 364 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out the resulting data.\n",
    "pd.DataFrame(Z_train).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>354</th>\n",
       "      <th>355</th>\n",
       "      <th>356</th>\n",
       "      <th>357</th>\n",
       "      <th>358</th>\n",
       "      <th>359</th>\n",
       "      <th>360</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.352000e+03</td>\n",
       "      <td>4.352000e+03</td>\n",
       "      <td>4.352000e+03</td>\n",
       "      <td>4.352000e+03</td>\n",
       "      <td>4.352000e+03</td>\n",
       "      <td>4.352000e+03</td>\n",
       "      <td>4.352000e+03</td>\n",
       "      <td>4.352000e+03</td>\n",
       "      <td>4.352000e+03</td>\n",
       "      <td>4.352000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>4.352000e+03</td>\n",
       "      <td>4.352000e+03</td>\n",
       "      <td>4.352000e+03</td>\n",
       "      <td>4.352000e+03</td>\n",
       "      <td>4.352000e+03</td>\n",
       "      <td>4.352000e+03</td>\n",
       "      <td>4.352000e+03</td>\n",
       "      <td>4.352000e+03</td>\n",
       "      <td>4.352000e+03</td>\n",
       "      <td>4.352000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.146958e-16</td>\n",
       "      <td>-1.595946e-16</td>\n",
       "      <td>-6.122553e-17</td>\n",
       "      <td>4.693958e-18</td>\n",
       "      <td>8.783313e-17</td>\n",
       "      <td>-6.959302e-17</td>\n",
       "      <td>6.046022e-17</td>\n",
       "      <td>1.299002e-16</td>\n",
       "      <td>1.050528e-16</td>\n",
       "      <td>-8.316468e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.247476e-17</td>\n",
       "      <td>-5.682199e-17</td>\n",
       "      <td>-2.278533e-17</td>\n",
       "      <td>1.445710e-17</td>\n",
       "      <td>-3.035692e-17</td>\n",
       "      <td>5.544647e-18</td>\n",
       "      <td>-8.241894e-18</td>\n",
       "      <td>1.649713e-17</td>\n",
       "      <td>5.311508e-17</td>\n",
       "      <td>4.066600e-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.059077e+01</td>\n",
       "      <td>8.173139e+00</td>\n",
       "      <td>6.290206e+00</td>\n",
       "      <td>5.899809e+00</td>\n",
       "      <td>4.755777e+00</td>\n",
       "      <td>4.187379e+00</td>\n",
       "      <td>3.663482e+00</td>\n",
       "      <td>3.519570e+00</td>\n",
       "      <td>2.952624e+00</td>\n",
       "      <td>2.382871e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.324420e-07</td>\n",
       "      <td>2.824236e-07</td>\n",
       "      <td>2.473689e-07</td>\n",
       "      <td>1.937117e-07</td>\n",
       "      <td>1.745561e-07</td>\n",
       "      <td>1.617731e-07</td>\n",
       "      <td>1.377643e-07</td>\n",
       "      <td>9.470085e-08</td>\n",
       "      <td>2.940241e-08</td>\n",
       "      <td>3.084790e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.652547e+01</td>\n",
       "      <td>-1.274783e+01</td>\n",
       "      <td>-2.469224e+01</td>\n",
       "      <td>-4.787008e+01</td>\n",
       "      <td>-2.255302e+01</td>\n",
       "      <td>-2.058849e+01</td>\n",
       "      <td>-2.871649e+01</td>\n",
       "      <td>-1.818558e+01</td>\n",
       "      <td>-1.423366e+01</td>\n",
       "      <td>-2.376801e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.023989e-06</td>\n",
       "      <td>-2.779914e-06</td>\n",
       "      <td>-1.989357e-06</td>\n",
       "      <td>-1.760353e-06</td>\n",
       "      <td>-1.692596e-06</td>\n",
       "      <td>-1.716397e-06</td>\n",
       "      <td>-1.619285e-06</td>\n",
       "      <td>-8.943172e-07</td>\n",
       "      <td>-2.324601e-07</td>\n",
       "      <td>-7.468380e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.399924e+00</td>\n",
       "      <td>-5.048388e+00</td>\n",
       "      <td>-4.000986e+00</td>\n",
       "      <td>-3.001194e+00</td>\n",
       "      <td>-2.383840e+00</td>\n",
       "      <td>-2.356163e+00</td>\n",
       "      <td>-2.189302e+00</td>\n",
       "      <td>-2.283286e+00</td>\n",
       "      <td>-1.858204e+00</td>\n",
       "      <td>-1.434927e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.378026e-07</td>\n",
       "      <td>-9.565181e-08</td>\n",
       "      <td>-1.013028e-07</td>\n",
       "      <td>-7.476632e-08</td>\n",
       "      <td>-6.683678e-08</td>\n",
       "      <td>-6.463550e-08</td>\n",
       "      <td>-4.998106e-08</td>\n",
       "      <td>-3.541248e-08</td>\n",
       "      <td>-1.156609e-08</td>\n",
       "      <td>-7.859317e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.779810e+00</td>\n",
       "      <td>-1.784668e+00</td>\n",
       "      <td>-6.808596e-01</td>\n",
       "      <td>-6.803642e-02</td>\n",
       "      <td>-4.819930e-02</td>\n",
       "      <td>-7.350413e-02</td>\n",
       "      <td>-1.303354e-01</td>\n",
       "      <td>-1.860893e-01</td>\n",
       "      <td>1.402668e-01</td>\n",
       "      <td>-7.198744e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.628219e-08</td>\n",
       "      <td>1.037889e-08</td>\n",
       "      <td>-9.718861e-09</td>\n",
       "      <td>3.965882e-09</td>\n",
       "      <td>3.880132e-10</td>\n",
       "      <td>-1.324977e-09</td>\n",
       "      <td>6.129282e-10</td>\n",
       "      <td>2.191272e-09</td>\n",
       "      <td>4.248704e-10</td>\n",
       "      <td>1.950242e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.595688e+00</td>\n",
       "      <td>2.770390e+00</td>\n",
       "      <td>3.760017e+00</td>\n",
       "      <td>3.103933e+00</td>\n",
       "      <td>2.411031e+00</td>\n",
       "      <td>2.287913e+00</td>\n",
       "      <td>1.959506e+00</td>\n",
       "      <td>2.180386e+00</td>\n",
       "      <td>1.909862e+00</td>\n",
       "      <td>1.358250e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.393233e-07</td>\n",
       "      <td>1.054804e-07</td>\n",
       "      <td>1.014314e-07</td>\n",
       "      <td>7.356252e-08</td>\n",
       "      <td>6.753657e-08</td>\n",
       "      <td>5.911348e-08</td>\n",
       "      <td>4.857960e-08</td>\n",
       "      <td>3.957595e-08</td>\n",
       "      <td>1.240289e-08</td>\n",
       "      <td>1.172730e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.524253e+01</td>\n",
       "      <td>1.149057e+02</td>\n",
       "      <td>8.074751e+01</td>\n",
       "      <td>1.338557e+02</td>\n",
       "      <td>7.609410e+01</td>\n",
       "      <td>3.965114e+01</td>\n",
       "      <td>3.559624e+01</td>\n",
       "      <td>5.628123e+01</td>\n",
       "      <td>1.936733e+01</td>\n",
       "      <td>2.458697e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>4.492273e-06</td>\n",
       "      <td>4.635209e-06</td>\n",
       "      <td>3.338466e-06</td>\n",
       "      <td>2.044531e-06</td>\n",
       "      <td>2.028879e-06</td>\n",
       "      <td>1.779281e-06</td>\n",
       "      <td>1.906753e-06</td>\n",
       "      <td>1.092489e-06</td>\n",
       "      <td>4.183708e-07</td>\n",
       "      <td>1.523538e-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 364 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0             1             2             3             4    \\\n",
       "count  4.352000e+03  4.352000e+03  4.352000e+03  4.352000e+03  4.352000e+03   \n",
       "mean   1.146958e-16 -1.595946e-16 -6.122553e-17  4.693958e-18  8.783313e-17   \n",
       "std    1.059077e+01  8.173139e+00  6.290206e+00  5.899809e+00  4.755777e+00   \n",
       "min   -2.652547e+01 -1.274783e+01 -2.469224e+01 -4.787008e+01 -2.255302e+01   \n",
       "25%   -7.399924e+00 -5.048388e+00 -4.000986e+00 -3.001194e+00 -2.383840e+00   \n",
       "50%   -1.779810e+00 -1.784668e+00 -6.808596e-01 -6.803642e-02 -4.819930e-02   \n",
       "75%    6.595688e+00  2.770390e+00  3.760017e+00  3.103933e+00  2.411031e+00   \n",
       "max    9.524253e+01  1.149057e+02  8.074751e+01  1.338557e+02  7.609410e+01   \n",
       "\n",
       "                5             6             7             8             9    \\\n",
       "count  4.352000e+03  4.352000e+03  4.352000e+03  4.352000e+03  4.352000e+03   \n",
       "mean  -6.959302e-17  6.046022e-17  1.299002e-16  1.050528e-16 -8.316468e-17   \n",
       "std    4.187379e+00  3.663482e+00  3.519570e+00  2.952624e+00  2.382871e+00   \n",
       "min   -2.058849e+01 -2.871649e+01 -1.818558e+01 -1.423366e+01 -2.376801e+01   \n",
       "25%   -2.356163e+00 -2.189302e+00 -2.283286e+00 -1.858204e+00 -1.434927e+00   \n",
       "50%   -7.350413e-02 -1.303354e-01 -1.860893e-01  1.402668e-01 -7.198744e-02   \n",
       "75%    2.287913e+00  1.959506e+00  2.180386e+00  1.909862e+00  1.358250e+00   \n",
       "max    3.965114e+01  3.559624e+01  5.628123e+01  1.936733e+01  2.458697e+01   \n",
       "\n",
       "       ...           354           355           356           357  \\\n",
       "count  ...  4.352000e+03  4.352000e+03  4.352000e+03  4.352000e+03   \n",
       "mean   ... -1.247476e-17 -5.682199e-17 -2.278533e-17  1.445710e-17   \n",
       "std    ...  3.324420e-07  2.824236e-07  2.473689e-07  1.937117e-07   \n",
       "min    ... -3.023989e-06 -2.779914e-06 -1.989357e-06 -1.760353e-06   \n",
       "25%    ... -1.378026e-07 -9.565181e-08 -1.013028e-07 -7.476632e-08   \n",
       "50%    ...  1.628219e-08  1.037889e-08 -9.718861e-09  3.965882e-09   \n",
       "75%    ...  1.393233e-07  1.054804e-07  1.014314e-07  7.356252e-08   \n",
       "max    ...  4.492273e-06  4.635209e-06  3.338466e-06  2.044531e-06   \n",
       "\n",
       "                358           359           360           361           362  \\\n",
       "count  4.352000e+03  4.352000e+03  4.352000e+03  4.352000e+03  4.352000e+03   \n",
       "mean  -3.035692e-17  5.544647e-18 -8.241894e-18  1.649713e-17  5.311508e-17   \n",
       "std    1.745561e-07  1.617731e-07  1.377643e-07  9.470085e-08  2.940241e-08   \n",
       "min   -1.692596e-06 -1.716397e-06 -1.619285e-06 -8.943172e-07 -2.324601e-07   \n",
       "25%   -6.683678e-08 -6.463550e-08 -4.998106e-08 -3.541248e-08 -1.156609e-08   \n",
       "50%    3.880132e-10 -1.324977e-09  6.129282e-10  2.191272e-09  4.248704e-10   \n",
       "75%    6.753657e-08  5.911348e-08  4.857960e-08  3.957595e-08  1.240289e-08   \n",
       "max    2.028879e-06  1.779281e-06  1.906753e-06  1.092489e-06  4.183708e-07   \n",
       "\n",
       "                363  \n",
       "count  4.352000e+03  \n",
       "mean   4.066600e-26  \n",
       "std    3.084790e-16  \n",
       "min   -7.468380e-15  \n",
       "25%   -7.859317e-17  \n",
       "50%    1.950242e-17  \n",
       "75%    1.172730e-16  \n",
       "max    1.523538e-15  \n",
       "\n",
       "[8 rows x 364 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out the resulting data. - descriptive stats on above output\n",
    "pd.DataFrame(Z_train).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>354</th>\n",
       "      <th>355</th>\n",
       "      <th>356</th>\n",
       "      <th>357</th>\n",
       "      <th>358</th>\n",
       "      <th>359</th>\n",
       "      <th>360</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.103072</td>\n",
       "      <td>3.439415</td>\n",
       "      <td>-11.783759</td>\n",
       "      <td>13.016930</td>\n",
       "      <td>1.184315</td>\n",
       "      <td>-3.159621</td>\n",
       "      <td>3.414925</td>\n",
       "      <td>8.538476</td>\n",
       "      <td>-2.584179</td>\n",
       "      <td>3.018793</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.182920e-08</td>\n",
       "      <td>-1.561775e-08</td>\n",
       "      <td>2.473053e-08</td>\n",
       "      <td>-3.549909e-07</td>\n",
       "      <td>2.055061e-07</td>\n",
       "      <td>1.428390e-07</td>\n",
       "      <td>2.897451e-07</td>\n",
       "      <td>2.545958e-08</td>\n",
       "      <td>-2.549097e-08</td>\n",
       "      <td>-7.503008e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.235547</td>\n",
       "      <td>3.500906</td>\n",
       "      <td>7.817154</td>\n",
       "      <td>-7.445563</td>\n",
       "      <td>-3.417477</td>\n",
       "      <td>-8.154706</td>\n",
       "      <td>1.531397</td>\n",
       "      <td>-2.076225</td>\n",
       "      <td>1.191075</td>\n",
       "      <td>0.573568</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.511403e-07</td>\n",
       "      <td>7.566446e-08</td>\n",
       "      <td>8.390730e-08</td>\n",
       "      <td>1.703526e-07</td>\n",
       "      <td>-1.228489e-07</td>\n",
       "      <td>6.259637e-08</td>\n",
       "      <td>-4.645473e-08</td>\n",
       "      <td>8.151851e-08</td>\n",
       "      <td>-2.441678e-08</td>\n",
       "      <td>-5.379277e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.353077</td>\n",
       "      <td>-4.055033</td>\n",
       "      <td>-0.026266</td>\n",
       "      <td>1.417186</td>\n",
       "      <td>-5.566875</td>\n",
       "      <td>-2.302657</td>\n",
       "      <td>0.953539</td>\n",
       "      <td>4.012457</td>\n",
       "      <td>1.944812</td>\n",
       "      <td>-1.971464</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.313394e-07</td>\n",
       "      <td>-2.411871e-07</td>\n",
       "      <td>-4.764512e-08</td>\n",
       "      <td>-7.260178e-08</td>\n",
       "      <td>1.005425e-07</td>\n",
       "      <td>-3.236870e-08</td>\n",
       "      <td>-4.273610e-08</td>\n",
       "      <td>4.756427e-09</td>\n",
       "      <td>4.493932e-09</td>\n",
       "      <td>-3.051075e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.626931</td>\n",
       "      <td>1.017445</td>\n",
       "      <td>-1.746430</td>\n",
       "      <td>-0.796755</td>\n",
       "      <td>-1.895322</td>\n",
       "      <td>-0.293004</td>\n",
       "      <td>-1.728781</td>\n",
       "      <td>1.354999</td>\n",
       "      <td>-4.237255</td>\n",
       "      <td>-2.503114</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.367549e-07</td>\n",
       "      <td>-7.396131e-08</td>\n",
       "      <td>-4.645654e-08</td>\n",
       "      <td>-1.358216e-07</td>\n",
       "      <td>-2.208545e-08</td>\n",
       "      <td>2.778198e-08</td>\n",
       "      <td>-1.780352e-09</td>\n",
       "      <td>-1.933329e-08</td>\n",
       "      <td>-9.821300e-09</td>\n",
       "      <td>-9.857787e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.180523</td>\n",
       "      <td>2.107191</td>\n",
       "      <td>9.862115</td>\n",
       "      <td>-3.257994</td>\n",
       "      <td>-1.340473</td>\n",
       "      <td>0.833409</td>\n",
       "      <td>-2.528985</td>\n",
       "      <td>-4.532641</td>\n",
       "      <td>4.218397</td>\n",
       "      <td>0.151709</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.094121e-08</td>\n",
       "      <td>8.671944e-08</td>\n",
       "      <td>1.682491e-07</td>\n",
       "      <td>1.688573e-07</td>\n",
       "      <td>-4.166592e-08</td>\n",
       "      <td>5.811428e-08</td>\n",
       "      <td>-1.148048e-07</td>\n",
       "      <td>-9.872134e-09</td>\n",
       "      <td>2.203536e-09</td>\n",
       "      <td>1.878234e-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 364 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1          2          3         4         5         6    \\\n",
       "0   9.103072  3.439415 -11.783759  13.016930  1.184315 -3.159621  3.414925   \n",
       "1  -2.235547  3.500906   7.817154  -7.445563 -3.417477 -8.154706  1.531397   \n",
       "2   3.353077 -4.055033  -0.026266   1.417186 -5.566875 -2.302657  0.953539   \n",
       "3  18.626931  1.017445  -1.746430  -0.796755 -1.895322 -0.293004 -1.728781   \n",
       "4  23.180523  2.107191   9.862115  -3.257994 -1.340473  0.833409 -2.528985   \n",
       "\n",
       "        7         8         9    ...           354           355  \\\n",
       "0  8.538476 -2.584179  3.018793  ... -9.182920e-08 -1.561775e-08   \n",
       "1 -2.076225  1.191075  0.573568  ... -1.511403e-07  7.566446e-08   \n",
       "2  4.012457  1.944812 -1.971464  ... -1.313394e-07 -2.411871e-07   \n",
       "3  1.354999 -4.237255 -2.503114  ... -1.367549e-07 -7.396131e-08   \n",
       "4 -4.532641  4.218397  0.151709  ... -2.094121e-08  8.671944e-08   \n",
       "\n",
       "            356           357           358           359           360  \\\n",
       "0  2.473053e-08 -3.549909e-07  2.055061e-07  1.428390e-07  2.897451e-07   \n",
       "1  8.390730e-08  1.703526e-07 -1.228489e-07  6.259637e-08 -4.645473e-08   \n",
       "2 -4.764512e-08 -7.260178e-08  1.005425e-07 -3.236870e-08 -4.273610e-08   \n",
       "3 -4.645654e-08 -1.358216e-07 -2.208545e-08  2.778198e-08 -1.780352e-09   \n",
       "4  1.682491e-07  1.688573e-07 -4.166592e-08  5.811428e-08 -1.148048e-07   \n",
       "\n",
       "            361           362           363  \n",
       "0  2.545958e-08 -2.549097e-08 -7.503008e-16  \n",
       "1  8.151851e-08 -2.441678e-08 -5.379277e-17  \n",
       "2  4.756427e-09  4.493932e-09 -3.051075e-17  \n",
       "3 -1.933329e-08 -9.821300e-09 -9.857787e-17  \n",
       "4 -9.872134e-09  2.203536e-09  1.878234e-16  \n",
       "\n",
       "[5 rows x 364 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarly, Don't forget to transform the test data!\n",
    "Z_test = pca.transform(X_test)\n",
    "pd.DataFrame(Z_test).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, big picture, what is PCA doing, technically?\n",
    "Well, we're transforming our data. Specifically:\n",
    "\n",
    "The associated mathematical terminologies with PCA are captured in **bold**\n",
    "\n",
    "\n",
    "1. We are going to look at how all of the $X$ variables relate to one another, then summarize these relationships. (This is done with the **covariance matrix**.)\n",
    "2. Then, we will take this summary and look at which combinations of our $X$ variables are most important. (We will decompose our covariance matrix into its **eigenvectors**, which is a linear algebra term that allows us to understand the most important \"directions\" in our data, which are our principal components!)\n",
    "3. We can also see exactly how important each combination is, then rank these combinations. (With each eigenvector, we get an **eigenvalue**. This eigenvalue is a number that tells us how important each \"direction\" or principal component is.)\n",
    "    - Want a better understanding of eigenvectors and eigenvalues? [Check this 3Blue1Brown video out!](https://www.youtube.com/watch?v=PFDu9oVAE-g)\n",
    "\n",
    "Remember that one of our goals with PCA is to do **dimensionality reduction** (a.k.a. get rid of unnecessary features).\n",
    "\n",
    "To summarize, We can: \n",
    "- measure how important each principal component is using the eigenvalue, \n",
    "- rank the columns of `Z_train` by their eigenvalues,\n",
    "- then drop the columns with small eigenvalues (little importance) but keep the columns with big eigenvalues (very important).\n",
    "    - In `sklearn`, when transformed by PCA, the columns will <b>already be sorted by their eigenvalues</b> from biggest to smallest! The first column will be the most important, the second column will be the next most important, and so on.\n",
    "\n",
    "#### But how do we decide on how many features to discard?\n",
    "\n",
    "A useful measure is the **proportion of explained variance**, which is calculated from the **eigenvalues**. \n",
    "\n",
    "The explained variance tells us how much information (variance) is captured by each principal component.\n",
    "\n",
    "### $$ \\text{explained variance of }PC_k = \\bigg(\\frac{\\text{eigenvalue of } PC_k}{\\sum_{i=1}^p\\text{eigenvalue of } PC_i}\\bigg)$$\n",
    "\n",
    "Rather than write out \"$\\text{eigenvalue of } PC_k$\", we usually just write $\\lambda_k$.\n",
    "\n",
    "If I want to calculate the proportion of explained variance by retaining $PC_1$ and $PC_2$, I would calculate this as:\n",
    "\n",
    "### $$ \\text{explained variance of } PC_1 \\text{ and } PC_2 = \\bigg(\\frac{\\lambda_1 + \\lambda_2}{\\sum_{i=1}^p \\lambda_i} \\bigg)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance (first 20 components): [0.309 0.184 0.109 0.096 0.062 0.048 0.037 0.034 0.024 0.016 0.013 0.011\n",
      " 0.009 0.006 0.004 0.003 0.003 0.003 0.003 0.002]\n",
      "\n",
      "Cumulative explained variance (first 20 components): [0.309 0.493 0.602 0.698 0.76  0.808 0.845 0.879 0.903 0.919 0.932 0.943\n",
      " 0.952 0.958 0.962 0.966 0.969 0.972 0.975 0.977]\n"
     ]
    }
   ],
   "source": [
    "# Pull the explained variance attribute using ready-to-use method from sklearn's PCA.\n",
    "var_exp = pca.explained_variance_ratio_\n",
    "print(f'Explained variance (first 20 components): {np.round(var_exp[:20],3)}') # examine only 1st 20 variances, rounded to 3 decimals\n",
    "\n",
    "print('')\n",
    "\n",
    "# Generate the cumulative explained variance.\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(f'Cumulative explained variance (first 20 components): {np.round(cum_var_exp[:20],3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Check: If I wanted to explain at least 80% of the variability in my data with principal components, what is the smallest number of principal components that I would need to keep? </summary>\n",
    "\n",
    "- Only six! \n",
    "- I could keep $Z_1, Z_2, \\ldots, Z_6$ in my model, and this would explain 80.8% of the variability in my $X$ data. (based on the 6th value of cum_var_exp hitting ~80%)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's compare our PCA'ed performance to our original performance!\n",
    "\n",
    "#### Original performance:\n",
    "\n",
    "<img src=\"../images/lr_performance.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Component Regression performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(n_components=10, random_state=42)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: we do these additional instantiation + fitting to use PCA, outside of our ML model instantiation and fitting\n",
    "# Instantiate PCA with 10 components.\n",
    "pca = PCA(n_components = 10, random_state = 42)\n",
    "\n",
    "# Fit PCA to training data.\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.2902\n",
      "Testing Score: 0.2639\n"
     ]
    }
   ],
   "source": [
    "# Instantiate linear regression model.\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Transform Z_train and Z_test.\n",
    "Z_train = pca.transform(X_train)\n",
    "Z_test = pca.transform(X_test)\n",
    "\n",
    "# Fit on Z_train.\n",
    "lm.fit(Z_train, y_train)\n",
    "\n",
    "# Score on training and testing sets.\n",
    "print(f'Training Score: {round(lm.score(Z_train, y_train),4)}')\n",
    "print(f'Testing Score: {round(lm.score(Z_test, y_test),4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two assumptions that PCA makes:**\n",
    "1. **Linearity:** PCA detects and controls for linear relationships, so we assume that the data does not hold nonlinear relationships (or that we don't care about these nonlinear relationships).\n",
    "    - We are using our covariance matrix to determine important \"directions,\" which is a measure of the linear relationship between observations!\n",
    "    - There are other types of feature extraction like [t-SNE](https://lvdmaaten.github.io/tsne/) and [PPA](https://towardsdatascience.com/interesting-projections-where-pca-fails-fe64ddca73e6), though we won't formally cover those in a global lesson.\n",
    "    \n",
    "    \n",
    "2. **Large variances define importance:** If data is spread in a direction, that direction is important! If there is little spread in a direction, that direction is not very important.\n",
    "    - That aligns with what we saw [here](http://setosa.io/ev/principal-component-analysis/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Use Cases for PCA\n",
    "- Situations where $p \\not\\ll n$. (Situations where $p$ is not substantially smaller than $n$.)\n",
    "- Situations in which there are variables with high multicollinearity. (Can be traditional models or models with highly correlated inputs by design, like images.)\n",
    "- Situations in which there are many variables, even without explicit multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interview Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Explain PCA to me.</summary>\n",
    "\n",
    "- Principal component analysis is a method of dimensionality reduction that **identifies important relationships** in our data, **transforms the existing data** based on these relationships, and then **quantifies the importance** of these relationships so we can keep the most important relationships and drop the others!\n",
    "\n",
    "<details><summary>How can I remember the above?</summary>\n",
    "\n",
    "Matt's \"Three Signposts:\"\n",
    "- Covariance Matrix\n",
    "- Eigenvectors\n",
    "- Eigenvalues\n",
    "</details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>In what cases would I not use PCA?</summary>\n",
    "\n",
    "- Since PCA distorts the interpretability of our features, we should not use PCA if our goal is to interpret the output of our model.\n",
    "- If we have relatively few features as inputs, PCA is unlikely to have a large positive impact on our model.\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
