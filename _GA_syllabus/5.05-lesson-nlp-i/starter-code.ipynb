{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# NLP I: Language Data Pre-Processing and Sentiment Analysis\n",
    "\n",
    "_Authors: Matt Brems, Noelle Brown_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "1. Define and implement tokenizing, lemmatizing, and stemming.\n",
    "2. Preprocess text data.\n",
    "3. Define and apply sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before we begin, try running this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you get an error with the above code, run this & follow below directions:\n",
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran into issues with the above:\n",
    "\n",
    "1. Run `nltk.download()`. A new screen will pop up outside your Jupyter notebook. (It may be hidden behind other windows.)\n",
    "2. Once this box opens up, click `all`, then `download`. Once this is done, restart your Jupyter notebook and try running the first three cells again.\n",
    "3. Run:\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"cats\")```\n",
    "\n",
    "    - If this returns `cat`, then fantastic! You’re done. \n",
    "    - If not, head to http://www.nltk.org/install.html and follow instructions for your computer, then try running the first three cells again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which of these was machine generated?\n",
    "\n",
    "- A: \"Kilimanjaro is a mountain of 19,710 feet covered with snow and is said to be the highest mountain in Africa. The summit of the west is called “Ngaje Ngai” in Masai, the house of God. Near the top of the west there is a dry and frozen dead body of leopard. No one has ever explained what leopard wanted at that altitude.\"\n",
    "\n",
    "- B: \"Kilimanjaro is a snow-covered mountain 19,710 feet high, and is said to be the highest mountain in Africa. Its western summit is called the Masai “Ngaje Ngai,” the House of God. Close to the western summit there is the dried and frozen carcass of a leopard. No one has explained what the leopard was seeking at that altitude.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Answer:</summary>\n",
    "\n",
    "- Item B was written by [Ernest Hemingway](https://en.wikipedia.org/wiki/Ernest_Hemingway) in \"The Snows of Kilimanjaro.\"\n",
    "\n",
    "- Item A was produced by a Japanese author translating \"The Snows of Kilimanjaro\" into Japanese, then this Japanese version was passed through Google Translate so that it could be \"translated back\" into English.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Natural language processing** (NLP) describes the field of getting computers to understand language how we as humans do. Natural language processing has many, many applications including:\n",
    "- voice-to-text services for people who are hard of hearing.\n",
    "- text-to-voice services for people who have difficulty reading.\n",
    "- automated chatbots for organizations.\n",
    "- translation services.\n",
    "\n",
    "Generally when we get text data, strings aren't broken out into individual words or even sentences. We might have a full tweet, full chapter of a book, or full .pdf file all in one long string.\n",
    "\n",
    "Today, we're diving into the practical side of NLP: taking text data and breaking it out into words that we can then leverage in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd       \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello,\n",
      "I saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n"
     ]
    }
   ],
   "source": [
    "# Define spam text.\n",
    "spam = 'Hello,\\nI saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.'\n",
    "\n",
    "print(spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing \n",
    "\n",
    "When dealing with text data, there are common pre-processing steps. We won't necessarily use all of them every time we deal with text data.\n",
    "\n",
    "- Remove special characters\n",
    "- Tokenizing\n",
    "- Lemmatizing/Stemming\n",
    "- Stop word removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing special characters & Tokenizing\n",
    "\n",
    "We need to remove unnecessary characters when cleaning text data (punctuation, symbols, etc.). This can be done with RegEx (more on that in a bit).\n",
    "\n",
    "When we \"**tokenize**\" data, we take it and split it up into distinct chunks based on some pattern.\n",
    "\n",
    "If we use a RegEx tokenizer, we often can do these steps together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello,\\ni saw your contact information on linkedin.',\n",
       " 'i have carefully read through your profile and you seem to have an outstanding personality.',\n",
       " 'this is one major reason why i am in contact with you.',\n",
       " 'my name is mr. valery grayfer chairman of the board of directors of pjsc \"lukoil\".',\n",
       " 'i am 86 years old and i was diagnosed with cancer 2 years ago.',\n",
       " 'i will be going in for an operation later this week.',\n",
       " 'i decided to will/donate the sum of 8,750,000.00 euros(eight million seven hundred and fifty thousand euros only etc.',\n",
       " 'etc.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenizer\n",
    "sent_tokenize(spam.lower()) #Get list of sentences (Separated by full-stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " ',',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'your',\n",
       " 'contact',\n",
       " 'information',\n",
       " 'on',\n",
       " 'linkedin',\n",
       " '.',\n",
       " 'i',\n",
       " 'have',\n",
       " 'carefully',\n",
       " 'read',\n",
       " 'through',\n",
       " 'your',\n",
       " 'profile',\n",
       " 'and',\n",
       " 'you',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'have',\n",
       " 'an',\n",
       " 'outstanding',\n",
       " 'personality',\n",
       " '.',\n",
       " 'this',\n",
       " 'is',\n",
       " 'one',\n",
       " 'major',\n",
       " 'reason',\n",
       " 'why',\n",
       " 'i',\n",
       " 'am',\n",
       " 'in',\n",
       " 'contact',\n",
       " 'with',\n",
       " 'you',\n",
       " '.',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'mr.',\n",
       " 'valery',\n",
       " 'grayfer',\n",
       " 'chairman',\n",
       " 'of',\n",
       " 'the',\n",
       " 'board',\n",
       " 'of',\n",
       " 'directors',\n",
       " 'of',\n",
       " 'pjsc',\n",
       " '``',\n",
       " 'lukoil',\n",
       " \"''\",\n",
       " '.',\n",
       " 'i',\n",
       " 'am',\n",
       " '86',\n",
       " 'years',\n",
       " 'old',\n",
       " 'and',\n",
       " 'i',\n",
       " 'was',\n",
       " 'diagnosed',\n",
       " 'with',\n",
       " 'cancer',\n",
       " '2',\n",
       " 'years',\n",
       " 'ago',\n",
       " '.',\n",
       " 'i',\n",
       " 'will',\n",
       " 'be',\n",
       " 'going',\n",
       " 'in',\n",
       " 'for',\n",
       " 'an',\n",
       " 'operation',\n",
       " 'later',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'i',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'will/donate',\n",
       " 'the',\n",
       " 'sum',\n",
       " 'of',\n",
       " '8,750,000.00',\n",
       " 'euros',\n",
       " '(',\n",
       " 'eight',\n",
       " 'million',\n",
       " 'seven',\n",
       " 'hundred',\n",
       " 'and',\n",
       " 'fifty',\n",
       " 'thousand',\n",
       " 'euros',\n",
       " 'only',\n",
       " 'etc',\n",
       " '.',\n",
       " 'etc',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenizer\n",
    "word_tokenize(spam.lower()) # Breaking down into individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate RegExp Tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+') ## We'll talk about this in a moment.\n",
    "\n",
    "# Match either one of more words (based on regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Run\" Tokenizer\n",
    "spam_tokens = tokenizer.tokenize(spam.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'i', 'saw', 'your', 'contact', 'information', 'on', 'linkedin', 'i', 'have', 'carefully', 'read', 'through', 'your', 'profile', 'and', 'you', 'seem', 'to', 'have', 'an', 'outstanding', 'personality', 'this', 'is', 'one', 'major', 'reason', 'why', 'i', 'am', 'in', 'contact', 'with', 'you', 'my', 'name', 'is', 'mr', 'valery', 'grayfer', 'chairman', 'of', 'the', 'board', 'of', 'directors', 'of', 'pjsc', 'lukoil', 'i', 'am', '86', 'years', 'old', 'and', 'i', 'was', 'diagnosed', 'with', 'cancer', '2', 'years', 'ago', 'i', 'will', 'be', 'going', 'in', 'for', 'an', 'operation', 'later', 'this', 'week', 'i', 'decided', 'to', 'will', 'donate', 'the', 'sum', 'of', '8', '750', '000', '00', 'euros', 'eight', 'million', 'seven', 'hundred', 'and', 'fifty', 'thousand', 'euros', 'only', 'etc', 'etc']\n"
     ]
    }
   ],
   "source": [
    "# Show Results\n",
    "print(spam_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>In comparing the original text to our tokenized version of the text, we converted one long string into a list of strings. What other changes occurred?</summary>\n",
    "\n",
    "- All strings were converted to lower case.\n",
    "- All punctuation was removed. (This was done using **regular expressions**.)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Briefly: Regular Expressions\n",
    "\n",
    "Regular Expressions, or RegEx, is a helpful tool for detecting patterns in text. \n",
    "- This is a tool of which you should be aware!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([], 'hello'),\n",
       " ([], 'i'),\n",
       " ([], 'saw'),\n",
       " ([], 'your'),\n",
       " ([], 'contact'),\n",
       " ([], 'information'),\n",
       " ([], 'on'),\n",
       " ([], 'linkedin'),\n",
       " ([], 'i'),\n",
       " ([], 'have'),\n",
       " ([], 'carefully'),\n",
       " ([], 'read'),\n",
       " ([], 'through'),\n",
       " ([], 'your'),\n",
       " ([], 'profile'),\n",
       " ([], 'and'),\n",
       " ([], 'you'),\n",
       " ([], 'seem'),\n",
       " ([], 'to'),\n",
       " ([], 'have'),\n",
       " ([], 'an'),\n",
       " ([], 'outstanding'),\n",
       " ([], 'personality'),\n",
       " ([], 'this'),\n",
       " ([], 'is'),\n",
       " ([], 'one'),\n",
       " ([], 'major'),\n",
       " ([], 'reason'),\n",
       " ([], 'why'),\n",
       " ([], 'i'),\n",
       " ([], 'am'),\n",
       " ([], 'in'),\n",
       " ([], 'contact'),\n",
       " ([], 'with'),\n",
       " ([], 'you'),\n",
       " ([], 'my'),\n",
       " ([], 'name'),\n",
       " ([], 'is'),\n",
       " ([], 'mr'),\n",
       " ([], 'valery'),\n",
       " ([], 'grayfer'),\n",
       " ([], 'chairman'),\n",
       " ([], 'of'),\n",
       " ([], 'the'),\n",
       " ([], 'board'),\n",
       " ([], 'of'),\n",
       " ([], 'directors'),\n",
       " ([], 'of'),\n",
       " ([], 'pjsc'),\n",
       " ([], 'lukoil'),\n",
       " ([], 'i'),\n",
       " ([], 'am'),\n",
       " (['86'], '86'),\n",
       " ([], 'years'),\n",
       " ([], 'old'),\n",
       " ([], 'and'),\n",
       " ([], 'i'),\n",
       " ([], 'was'),\n",
       " ([], 'diagnosed'),\n",
       " ([], 'with'),\n",
       " ([], 'cancer'),\n",
       " (['2'], '2'),\n",
       " ([], 'years'),\n",
       " ([], 'ago'),\n",
       " ([], 'i'),\n",
       " ([], 'will'),\n",
       " ([], 'be'),\n",
       " ([], 'going'),\n",
       " ([], 'in'),\n",
       " ([], 'for'),\n",
       " ([], 'an'),\n",
       " ([], 'operation'),\n",
       " ([], 'later'),\n",
       " ([], 'this'),\n",
       " ([], 'week'),\n",
       " ([], 'i'),\n",
       " ([], 'decided'),\n",
       " ([], 'to'),\n",
       " ([], 'will'),\n",
       " ([], 'donate'),\n",
       " ([], 'the'),\n",
       " ([], 'sum'),\n",
       " ([], 'of'),\n",
       " (['8'], '8'),\n",
       " (['750'], '750'),\n",
       " (['000'], '000'),\n",
       " (['00'], '00'),\n",
       " ([], 'euros'),\n",
       " ([], 'eight'),\n",
       " ([], 'million'),\n",
       " ([], 'seven'),\n",
       " ([], 'hundred'),\n",
       " ([], 'and'),\n",
       " ([], 'fifty'),\n",
       " ([], 'thousand'),\n",
       " ([], 'euros'),\n",
       " ([], 'only'),\n",
       " ([], 'etc'),\n",
       " ([], 'etc')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(re.findall('\\d+', i), i) for i in spam_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RegEx in Python 3 understands `\\d+` to identify numeric digits. Therefore, the above code searched through `spam_tokens` to see if any numeric digits were in there. \n",
    "\n",
    "A `RegexpTokenizer` splits a string into substrings using regular expressions.\n",
    "\n",
    "The following example is pulled from [this site](http://www.nltk.org/_modules/nltk/tokenize/regexp.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good muffins cost $3.88\n",
      "in New York.  Please buy me\n",
      "two of them.\n",
      "\n",
      "Thanks.\n"
     ]
    }
   ],
   "source": [
    "# Define and print string.\n",
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate tokenizer.\n",
    "tokenizer_1 = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run tokenizer.\n",
    "tokenizer_1.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer_1` splits tokens up by spaces or by periods that are not attached to a digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them.',\n",
       " 'Thanks.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate tokenizer.\n",
    "tokenizer_2 = RegexpTokenizer('\\s+', gaps=True)\n",
    "\n",
    "# Run tokenizer.\n",
    "tokenizer_2.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer_2` will identify the spaces. By setting `gaps = True`, we're grabbing everything else: thus, we're splitting our tokens up by spaces.\n",
    "- If you changed to `gaps = False`, you'll return only the whitespaces!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good', 'New', 'York', 'Please', 'Thanks']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate tokenizer.\n",
    "tokenizer_3 = RegexpTokenizer('[A-Z]\\w+')\n",
    "\n",
    "# Run tokenizer.\n",
    "tokenizer_3.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenizer_3` returns only words that begin with a capital letter.\n",
    "\n",
    "As you can imagine, using RegEx _can_ be incredibly helpful if you want to find text matching a specific pattern.\n",
    "- People used to use two spaces after a period to split sentences up; you could use RegEx to detect that pattern and tokenize on entire sentences.\n",
    "- Chapters in a book could be titled \"Chapter\" followed by a number; you could use RegEx to detect that pattern and tokenize a book by its chapters.\n",
    "- When Python libraries are upgraded, syntax changes! Perhaps you want to detect a certain pattern of syntax so you can update your code efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/regex.png)\n",
    "\n",
    "[_from xkcd_](https://xkcd.com/1171/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing & Stemming\n",
    "\n",
    "- \"He is *running* really fast!\"\n",
    "- \"He *ran* the race.\"\n",
    "- \"He *runs* a five-minute mile.\"\n",
    "\n",
    "If we wanted a computer to interpret these sentences, I might count up how many times I see each word. The computer will treat words like \"running,\" \"ran,\" and \"runs\" differently... but they mean very similar things (in this context)!\n",
    "\n",
    "**Lemmatizing** and **stemming** are two forms of shortening words so we can combine similar forms of the same word.\n",
    "\n",
    "When we \"**lemmatize**\" data, we take words and attempt to return their *lemma*, or the base/dictionary form of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate lemmatizer. \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize tokens.\n",
    "tokens_lem = [lemmatizer.lemmatize(i) for i in spam_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 'hello'),\n",
       " ('i', 'i'),\n",
       " ('saw', 'saw'),\n",
       " ('your', 'your'),\n",
       " ('contact', 'contact'),\n",
       " ('information', 'information'),\n",
       " ('on', 'on'),\n",
       " ('linkedin', 'linkedin'),\n",
       " ('i', 'i'),\n",
       " ('have', 'have'),\n",
       " ('carefully', 'carefully'),\n",
       " ('read', 'read'),\n",
       " ('through', 'through'),\n",
       " ('your', 'your'),\n",
       " ('profile', 'profile'),\n",
       " ('and', 'and'),\n",
       " ('you', 'you'),\n",
       " ('seem', 'seem'),\n",
       " ('to', 'to'),\n",
       " ('have', 'have'),\n",
       " ('an', 'an'),\n",
       " ('outstanding', 'outstanding'),\n",
       " ('personality', 'personality'),\n",
       " ('this', 'this'),\n",
       " ('is', 'is'),\n",
       " ('one', 'one'),\n",
       " ('major', 'major'),\n",
       " ('reason', 'reason'),\n",
       " ('why', 'why'),\n",
       " ('i', 'i'),\n",
       " ('am', 'am'),\n",
       " ('in', 'in'),\n",
       " ('contact', 'contact'),\n",
       " ('with', 'with'),\n",
       " ('you', 'you'),\n",
       " ('my', 'my'),\n",
       " ('name', 'name'),\n",
       " ('is', 'is'),\n",
       " ('mr', 'mr'),\n",
       " ('valery', 'valery'),\n",
       " ('grayfer', 'grayfer'),\n",
       " ('chairman', 'chairman'),\n",
       " ('of', 'of'),\n",
       " ('the', 'the'),\n",
       " ('board', 'board'),\n",
       " ('of', 'of'),\n",
       " ('directors', 'director'),\n",
       " ('of', 'of'),\n",
       " ('pjsc', 'pjsc'),\n",
       " ('lukoil', 'lukoil'),\n",
       " ('i', 'i'),\n",
       " ('am', 'am'),\n",
       " ('86', '86'),\n",
       " ('years', 'year'),\n",
       " ('old', 'old'),\n",
       " ('and', 'and'),\n",
       " ('i', 'i'),\n",
       " ('was', 'wa'),\n",
       " ('diagnosed', 'diagnosed'),\n",
       " ('with', 'with'),\n",
       " ('cancer', 'cancer'),\n",
       " ('2', '2'),\n",
       " ('years', 'year'),\n",
       " ('ago', 'ago'),\n",
       " ('i', 'i'),\n",
       " ('will', 'will'),\n",
       " ('be', 'be'),\n",
       " ('going', 'going'),\n",
       " ('in', 'in'),\n",
       " ('for', 'for'),\n",
       " ('an', 'an'),\n",
       " ('operation', 'operation'),\n",
       " ('later', 'later'),\n",
       " ('this', 'this'),\n",
       " ('week', 'week'),\n",
       " ('i', 'i'),\n",
       " ('decided', 'decided'),\n",
       " ('to', 'to'),\n",
       " ('will', 'will'),\n",
       " ('donate', 'donate'),\n",
       " ('the', 'the'),\n",
       " ('sum', 'sum'),\n",
       " ('of', 'of'),\n",
       " ('8', '8'),\n",
       " ('750', '750'),\n",
       " ('000', '000'),\n",
       " ('00', '00'),\n",
       " ('euros', 'euro'),\n",
       " ('eight', 'eight'),\n",
       " ('million', 'million'),\n",
       " ('seven', 'seven'),\n",
       " ('hundred', 'hundred'),\n",
       " ('and', 'and'),\n",
       " ('fifty', 'fifty'),\n",
       " ('thousand', 'thousand'),\n",
       " ('euros', 'euro'),\n",
       " ('only', 'only'),\n",
       " ('etc', 'etc'),\n",
       " ('etc', 'etc')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare tokens to lemmatized version.\n",
    "list(zip(spam_tokens, tokens_lem)) # zip combines elements from different lists\n",
    "\n",
    "## Can't print zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('directors', 'director')\n",
      "('years', 'year')\n",
      "('was', 'wa')\n",
      "('years', 'year')\n",
      "('euros', 'euro')\n",
      "('euros', 'euro')\n"
     ]
    }
   ],
   "source": [
    "# Print only those lemmatized tokens that are different.\n",
    "for i in range(len(spam_tokens)):\n",
    "    if spam_tokens[i] != tokens_lem[i]:\n",
    "        print((spam_tokens[i], tokens_lem[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('directors', 'director'),\n",
       " ('years', 'year'),\n",
       " ('was', 'wa'),\n",
       " ('years', 'year'),\n",
       " ('euros', 'euro'),\n",
       " ('euros', 'euro')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using list comprehemsion\n",
    "[(spam_tokens[i], tokens_lem[i]) for i in range(len(spam_tokens)) if spam_tokens[i] != tokens_lem[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing is usually the more correct and precise way of handling things from a grammatical point of view, but also might not have much of an effect.\n",
    "\n",
    "We can also do this on individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computer'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize the word \"computer\"\n",
    "lemmatizer.lemmatize('computer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computer'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize the word \"computers\"\n",
    "lemmatizer.lemmatize('computers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computation'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize the word \"computation\"\n",
    "lemmatizer.lemmatize('computation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computationally'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize the word \"computationally\"\n",
    "lemmatizer.lemmatize('computationally')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we \"**stem**\" data, we take words and attempt to return a base form of the word. It tends to be cruder than using lemmatization. There's a [method developed by Porter in 1980](https://www.cs.toronto.edu/~frank/csc2501/Readings/R2_Porter/Porter-1980.pdf) that explains the algorithm used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PorterStemmer.\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem tokens.\n",
    "stem_spam = [p_stemmer.stem(i) for i in spam_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 'hello'),\n",
       " ('i', 'i'),\n",
       " ('saw', 'saw'),\n",
       " ('your', 'your'),\n",
       " ('contact', 'contact'),\n",
       " ('information', 'inform'),\n",
       " ('on', 'on'),\n",
       " ('linkedin', 'linkedin'),\n",
       " ('i', 'i'),\n",
       " ('have', 'have'),\n",
       " ('carefully', 'care'),\n",
       " ('read', 'read'),\n",
       " ('through', 'through'),\n",
       " ('your', 'your'),\n",
       " ('profile', 'profil'),\n",
       " ('and', 'and'),\n",
       " ('you', 'you'),\n",
       " ('seem', 'seem'),\n",
       " ('to', 'to'),\n",
       " ('have', 'have'),\n",
       " ('an', 'an'),\n",
       " ('outstanding', 'outstand'),\n",
       " ('personality', 'person'),\n",
       " ('this', 'thi'),\n",
       " ('is', 'is'),\n",
       " ('one', 'one'),\n",
       " ('major', 'major'),\n",
       " ('reason', 'reason'),\n",
       " ('why', 'whi'),\n",
       " ('i', 'i'),\n",
       " ('am', 'am'),\n",
       " ('in', 'in'),\n",
       " ('contact', 'contact'),\n",
       " ('with', 'with'),\n",
       " ('you', 'you'),\n",
       " ('my', 'my'),\n",
       " ('name', 'name'),\n",
       " ('is', 'is'),\n",
       " ('mr', 'mr'),\n",
       " ('valery', 'valeri'),\n",
       " ('grayfer', 'grayfer'),\n",
       " ('chairman', 'chairman'),\n",
       " ('of', 'of'),\n",
       " ('the', 'the'),\n",
       " ('board', 'board'),\n",
       " ('of', 'of'),\n",
       " ('directors', 'director'),\n",
       " ('of', 'of'),\n",
       " ('pjsc', 'pjsc'),\n",
       " ('lukoil', 'lukoil'),\n",
       " ('i', 'i'),\n",
       " ('am', 'am'),\n",
       " ('86', '86'),\n",
       " ('years', 'year'),\n",
       " ('old', 'old'),\n",
       " ('and', 'and'),\n",
       " ('i', 'i'),\n",
       " ('was', 'wa'),\n",
       " ('diagnosed', 'diagnos'),\n",
       " ('with', 'with'),\n",
       " ('cancer', 'cancer'),\n",
       " ('2', '2'),\n",
       " ('years', 'year'),\n",
       " ('ago', 'ago'),\n",
       " ('i', 'i'),\n",
       " ('will', 'will'),\n",
       " ('be', 'be'),\n",
       " ('going', 'go'),\n",
       " ('in', 'in'),\n",
       " ('for', 'for'),\n",
       " ('an', 'an'),\n",
       " ('operation', 'oper'),\n",
       " ('later', 'later'),\n",
       " ('this', 'thi'),\n",
       " ('week', 'week'),\n",
       " ('i', 'i'),\n",
       " ('decided', 'decid'),\n",
       " ('to', 'to'),\n",
       " ('will', 'will'),\n",
       " ('donate', 'donat'),\n",
       " ('the', 'the'),\n",
       " ('sum', 'sum'),\n",
       " ('of', 'of'),\n",
       " ('8', '8'),\n",
       " ('750', '750'),\n",
       " ('000', '000'),\n",
       " ('00', '00'),\n",
       " ('euros', 'euro'),\n",
       " ('eight', 'eight'),\n",
       " ('million', 'million'),\n",
       " ('seven', 'seven'),\n",
       " ('hundred', 'hundr'),\n",
       " ('and', 'and'),\n",
       " ('fifty', 'fifti'),\n",
       " ('thousand', 'thousand'),\n",
       " ('euros', 'euro'),\n",
       " ('only', 'onli'),\n",
       " ('etc', 'etc'),\n",
       " ('etc', 'etc')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare tokens to stemmed version.\n",
    "list(zip(spam_tokens, stem_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('information', 'inform'),\n",
       " ('carefully', 'care'),\n",
       " ('profile', 'profil'),\n",
       " ('outstanding', 'outstand'),\n",
       " ('personality', 'person'),\n",
       " ('this', 'thi'),\n",
       " ('why', 'whi'),\n",
       " ('valery', 'valeri'),\n",
       " ('directors', 'director'),\n",
       " ('years', 'year'),\n",
       " ('was', 'wa'),\n",
       " ('diagnosed', 'diagnos'),\n",
       " ('years', 'year'),\n",
       " ('going', 'go'),\n",
       " ('operation', 'oper'),\n",
       " ('this', 'thi'),\n",
       " ('decided', 'decid'),\n",
       " ('donate', 'donat'),\n",
       " ('euros', 'euro'),\n",
       " ('hundred', 'hundr'),\n",
       " ('fifty', 'fifti'),\n",
       " ('euros', 'euro'),\n",
       " ('only', 'onli')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print only those stemmed tokens that are different.\n",
    "[(spam_tokens[i], stem_spam[i]) for i in range(len(spam_tokens)) if spam_tokens[i] != stem_spam[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do this on individual words as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comput'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem the word \"computer\"\n",
    "p_stemmer.stem('computer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comput'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem the word \"computers\"\n",
    "p_stemmer.stem('computers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comput'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem the word \"computation\"\n",
    "p_stemmer.stem('computation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comput'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem the word \"computationally\"\n",
    "p_stemmer.stem('computationally')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Word Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sentence has had stop words (and punctuation) removed:\n",
    "\n",
    "\"Answer great question life universe everything said deep thought said deep thought paused forty two said deep thought infinite majesty calm.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What book is the above sentence from?</summary>\n",
    "\n",
    "The Hitchhiker's Guide to the Galaxy!\n",
    "    \n",
    "![](./images/hgg.jpg)\n",
    "    \n",
    "The original quote reads:  \n",
    "...\"The Answer to the Great Question...\"  \n",
    "\"Yes..!\"  \n",
    "\"Of Life, the Universe and Everything...\" said Deep Thought.  \n",
    "\"Yes...!\"  \n",
    "\"Is...\" said Deep Thought, and paused.  \n",
    "\"Yes...!\"  \n",
    "\"Is...\"  \n",
    "\"Yes...!!!...?\"  \n",
    "\"Forty-two,\" said Deep Thought, with infinite majesty and calm.”\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>If you were familiar with the book, how did you know what book the sentence was from?</summary>\n",
    "\n",
    "Removing stop words did not remove key identifying words such as \"life\", \"universe\", \"everything\", and \"forty-two\".\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Based on this, how would you define stop words?</summary>\n",
    "\n",
    "Stop words are words that have little to no significance or meaning. They are common words that only add to the grammatical structure and flow of the sentence, so it is still relatively easy to identify the contents of sentences without stop words.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Print English stopwords.\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords from \"spam_tokens.\"\n",
    "no_stop_words = [token for token in spam_tokens if token not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'saw',\n",
       " 'contact',\n",
       " 'information',\n",
       " 'linkedin',\n",
       " 'carefully',\n",
       " 'read',\n",
       " 'profile',\n",
       " 'seem',\n",
       " 'outstanding',\n",
       " 'personality',\n",
       " 'one',\n",
       " 'major',\n",
       " 'reason',\n",
       " 'contact',\n",
       " 'name',\n",
       " 'mr',\n",
       " 'valery',\n",
       " 'grayfer',\n",
       " 'chairman',\n",
       " 'board',\n",
       " 'directors',\n",
       " 'pjsc',\n",
       " 'lukoil',\n",
       " '86',\n",
       " 'years',\n",
       " 'old',\n",
       " 'diagnosed',\n",
       " 'cancer',\n",
       " '2',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'going',\n",
       " 'operation',\n",
       " 'later',\n",
       " 'week',\n",
       " 'decided',\n",
       " 'donate',\n",
       " 'sum',\n",
       " '8',\n",
       " '750',\n",
       " '000',\n",
       " '00',\n",
       " 'euros',\n",
       " 'eight',\n",
       " 'million',\n",
       " 'seven',\n",
       " 'hundred',\n",
       " 'fifty',\n",
       " 'thousand',\n",
       " 'euros',\n",
       " 'etc',\n",
       " 'etc']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check it\n",
    "no_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Sentiment Analysis\n",
    "\n",
    "![](./images/sent.jpeg)\n",
    "\n",
    "[Sentiment analysis](https://www.kdnuggets.com/2018/08/emotion-sentiment-analysis-practitioners-guide-nlp-5.html) is an area of natural language processing in which we seek to classify text as having positive or negative emotion.\n",
    "\n",
    "Let's build a simple function that can classify text as either having positive or negative sentiment.\n",
    "\n",
    "What words tell us whether certain text is positive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's come up with a list of positive and negative words we might observe.\n",
    "\n",
    "positive_words = ['delight', 'good', 'great', 'awesome', 'tremendous', 'fabulous', 'amazing', 'stellar']\n",
    "negative_words = ['garbage', 'sad', 'trash', 'ugly', 'bad', 'disgusting', 'terrible', 'gross']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T10:03:50.010475Z",
     "start_time": "2017-10-25T10:03:50.006364Z"
    }
   },
   "outputs": [],
   "source": [
    "def simple_sentiment(text):\n",
    "    # Instantiate tokenizer.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    # Tokenize text.\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    # Instantiate stemmer.\n",
    "    p_stemmer = PorterStemmer()\n",
    "    \n",
    "    # Stem words.\n",
    "    stemmed_words = [p_stemmer.stem(i) for i in tokens]\n",
    "    \n",
    "    # Stem our positive/negative words.\n",
    "    positive_stems = [p_stemmer.stem(i) for i in positive_words]\n",
    "    negative_stems = [p_stemmer.stem(i) for i in negative_words]\n",
    "\n",
    "    # Count \"positive\" words.\n",
    "    positive_count = sum([1 for i in stemmed_words if i in positive_stems])\n",
    "    \n",
    "    # Count \"negative\" words\n",
    "    negative_count = sum([1 for i in stemmed_words if i in negative_stems])\n",
    "    \n",
    "    # Calculate Sentiment Percentage \n",
    "    # (Positive Count - Negative Count) / (Total Count)\n",
    "\n",
    "    return round((positive_count - negative_count)/len(tokens), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello,\n",
      "I saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n"
     ]
    }
   ],
   "source": [
    "# Recall what is spam\n",
    "print(spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run our sentiment analyzer on our spam email.\n",
    "simple_sentiment(spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three not-so-random Chipotle reviews.\n",
    "\n",
    "yelp_1 = \"No Chipotle should ever have a 2 out of 5 star rating on Yelp. Especially not this one. As a regular (usually two or three visits a week), I have never been dissatisfied with a single meal here. It's Chipotle, so you know you'll pay $8 (after tax) for a chicken bowl and be full and satisfied afterwards. \\n The employees are friendly and give generous portions. Seating is limited, but there is a place you can stand and eat near the window, which is where I always eat. I'm sitting down eight hours a day at the office anyway - standing and eating here is probably extending my lifespan. \\nThe line gets line long during lunch, but it moves fast. Dinner time is amazing - rarely a line and the portions are extra generous during this time.\\n This fairly new Chipotle is at a great location, near McPherson Square. It's right next to my office and gym so it's perfect for me. \\nBottom line: if you're craving Chipotle and are worried about the other reviews and low ratings for this location, don't be. It's my favorite Chipotle location in the DC area, and that's not an exaggeration.\"\n",
    "\n",
    "yelp_2 = \"DISGUSTING LONG HAIR THREADED THROUGH CHICKEN IN BURRITO BOWL.\\n There was a long blonde hair threaded through my chicken as I was eating a burrito bowl.  I did not notice until it was too late and the HAIR ENTERED MY MOUTH as I was eating and I grossly pulled the hair out.\\n I calmly walked up to the register to inform them that there was hair in my food. The register person was busy, I understand that, but I was promptly ignored like my issue was not a big deal.  He proceeded to get his manager, 'Leslie' I believe.  She was not apologetic at all and offered no condolences. She did however offer a refund, but I didn't care about the money, I just wanted to eat food without eating someone's hair as a side dish.\\n The second time I went back up, a different person, the general manager Peris, was more apologetic and handled the situation better. He ended up getting Leslie to file a report, but who knows if they submitted it or not.\\n Suffice to say, if you dont want food in your hair, dont eat here.\"\n",
    "\n",
    "yelp_3 = \"First time going to this Chipotle.  The line was very quick and the food was fresh.  But as I started eating a notice that the food was very salty.  I started separating my bowl after two bites.  I ordered a bowl with white rice, black beans, chicken, sour cream, cheese and lettuce.  I tasted everything separately.  Once I tasted the Chicken by it self it was unbearable.  It taste like someone pouring the entire bottle of salt on tge chicken.  I tried to take most the chicken out the bowl but still I could not bear the taste of the salt.  So I ended up throwing the damn bowl away.  $8.00 down the drain.  SMH.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No Chipotle should ever have a 2 out of 5 star rating on Yelp. Especially not this one. As a regular (usually two or three visits a week), I have never been dissatisfied with a single meal here. It's Chipotle, so you know you'll pay $8 (after tax) for a chicken bowl and be full and satisfied afterwards. \\n The employees are friendly and give generous portions. Seating is limited, but there is a place you can stand and eat near the window, which is where I always eat. I'm sitting down eight hours a day at the office anyway - standing and eating here is probably extending my lifespan. \\nThe line gets line long during lunch, but it moves fast. Dinner time is amazing - rarely a line and the portions are extra generous during this time.\\n This fairly new Chipotle is at a great location, near McPherson Square. It's right next to my office and gym so it's perfect for me. \\nBottom line: if you're craving Chipotle and are worried about the other reviews and low ratings for this location, don't be. It's my favorite Chipotle location in the DC area, and that's not an exaggeration.\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"DISGUSTING LONG HAIR THREADED THROUGH CHICKEN IN BURRITO BOWL.\\n There was a long blonde hair threaded through my chicken as I was eating a burrito bowl.  I did not notice until it was too late and the HAIR ENTERED MY MOUTH as I was eating and I grossly pulled the hair out.\\n I calmly walked up to the register to inform them that there was hair in my food. The register person was busy, I understand that, but I was promptly ignored like my issue was not a big deal.  He proceeded to get his manager, 'Leslie' I believe.  She was not apologetic at all and offered no condolences. She did however offer a refund, but I didn't care about the money, I just wanted to eat food without eating someone's hair as a side dish.\\n The second time I went back up, a different person, the general manager Peris, was more apologetic and handled the situation better. He ended up getting Leslie to file a report, but who knows if they submitted it or not.\\n Suffice to say, if you dont want food in your hair, dont eat here.\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First time going to this Chipotle.  The line was very quick and the food was fresh.  But as I started eating a notice that the food was very salty.  I started separating my bowl after two bites.  I ordered a bowl with white rice, black beans, chicken, sour cream, cheese and lettuce.  I tasted everything separately.  Once I tasted the Chicken by it self it was unbearable.  It taste like someone pouring the entire bottle of salt on tge chicken.  I tried to take most the chicken out the bowl but still I could not bear the taste of the salt.  So I ended up throwing the damn bowl away.  $8.00 down the drain.  SMH.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate sentiment of yelp_1.\n",
    "simple_sentiment(yelp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.01"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate sentiment of yelp_2.\n",
    "simple_sentiment(yelp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate sentiment of yelp_3.\n",
    "simple_sentiment(yelp_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary> What are some shortcomings of this method? </summary>\n",
    "\n",
    "- Primarily, we're limited to the positive/negative words we came up with.\n",
    "- If someone wrote \"not good\" or \"not bad,\" our sentiment function would probably treat \"not good\" as positive or neutral... but it's probably supposed to mean negative!\n",
    "- The ordering of the words doesn't matter here, which is not how language generally works.\n",
    "- We haven't corrected for misspellings.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of ways to proceed with sentiment analysis:\n",
    "\n",
    "1. If you have already-labeled data, you can build a supervised learning model.\n",
    "2. If you don't have labeled data, you can use a Lexicon that has already been built/trained for sentiment analysis.\n",
    "    - There are a bunch of these and which to use depends on your purpose/data. Here are just a few that are available:\n",
    "        - AFINN lexicon\n",
    "        - MPQA subjectivity lexicon\n",
    "        - SentiWordNet\n",
    "        - VADER lexicon\n",
    "\n",
    "We will use the VADER (Valence Aware Dictionary and sEntiment Reasoner) lexicon to analyze the sentiments of our reviews!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Sentiment Intensity Analyzer\n",
    "sent = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.046, 'neu': 0.799, 'pos': 0.155, 'compound': 0.9795}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate sentiment of yelp_1.\n",
    "sent.polarity_scores(yelp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.078, 'neu': 0.875, 'pos': 0.047, 'compound': -0.5847}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate sentiment of yelp_2.\n",
    "sent.polarity_scores(yelp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.067, 'neu': 0.89, 'pos': 0.043, 'compound': -0.5718}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate sentiment of yelp_3.\n",
    "sent.polarity_scores(yelp_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compound score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate.\n",
    "\n",
    "It is also useful for researchers who would like to set standardized thresholds for classifying sentences as either positive, neutral, or negative. Typical threshold values (used in the literature cited on this page) are:\n",
    "\n",
    "*positive sentiment: compound score >= 0.05*\n",
    "\n",
    "*neutral sentiment: (compound score > -0.05) and (compound score < 0.05)*\n",
    "\n",
    "*negative sentiment: compound score <= -0.05*\n",
    "\n",
    "NOTE: The compound score is the one most commonly used for sentiment analysis by most researchers, including the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try analyzing the sentiment of IMDb movie reviews. The data is from [Kaggle](https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:40:07.099103Z",
     "start_time": "2017-10-24T15:40:05.659674Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in training data.\n",
    "reviews = pd.read_csv(\"./data/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-24T15:40:07.680295Z",
     "start_time": "2017-10-24T15:40:07.659485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first five rows.\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park . A secret project mutating a primal animal using fossilized DNA, like ¨Jurassik Park¨, and some scientists resurrect one of nature\\'s most fearsome predators, the Sabretooth tiger or Smilodon . Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger . In addition , a security agent (Stacy Haiduk) and her mate (Brian Wimmer) fight hardly against the carnivorous Smilodons. The Sabretooths, themselves , of course, are the real star stars and they are astounding terrifyingly though not convincing. The giant animals savagely are stalking its prey and the group run afoul and fight against one nature\\'s most fearsome predators. Furthermore a third Sabretooth more dangerous and slow stalks its victims.<br /><br />The movie delivers the goods with lots of blood and gore as beheading, hair-raising chills,full of scares when the Sabretooths appear with mediocre special effects.The story provides exciting and stirring entertainment but it results to be quite boring .The giant animals are majority made by computer generator and seem totally lousy .Middling performances though the players reacting appropriately to becoming food.Actors give vigorously physical performances dodging the beasts ,running,bound and leaps or dangling over walls . And it packs a ridiculous final deadly scene. No for small kids by realistic,gory and violent attack scenes . Other films about Sabretooths or Smilodon are the following : ¨Sabretooth(2002)¨by James R Hickox with Vanessa Angel, David Keith and John Rhys Davies and the much better ¨10.000 BC(2006)¨ by Roland Emmerich with with Steven Strait, Cliff Curtis and Camilla Belle. This motion picture filled with bloody moments is badly directed by George Miller and with no originality because takes too many elements from previous films. Miller is an Australian director usually working for television (Tidal wave, Journey to the center of the earth, and many others) and occasionally for cinema ( The man from Snowy river, Zeus and Roxanne,Robinson Crusoe ). Rating : Below average, bottom of barrel.\"'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine a review.\n",
    "reviews['review'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12500\n",
       "1    12500\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment of the review.\n",
    "reviews['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.142, 'neu': 0.8, 'pos': 0.058, 'compound': -0.9883}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does this match the sentiment given in the training data?\n",
    "sent.polarity_scores(reviews['review'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does this match the sentiment given in the training data?\n",
    "reviews['sentiment'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the accuracy of Vader\n",
    "def vader_sentiment_pred(review):\n",
    "    vader_output = sent.polarity_scores(review)\n",
    "    \n",
    "    if vader_output['compound'] < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review  \\\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...   \n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...   \n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...   \n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...   \n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ...   \n",
       "\n",
       "   sentiment_pred  \n",
       "0               0  \n",
       "1               1  \n",
       "2               0  \n",
       "3               0  \n",
       "4               1  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['sentiment_pred'] = reviews['review'].apply(vader_sentiment_pred)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6924"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader_accuracy = accuracy_score(reviews['sentiment'], reviews['sentiment_pred'])\n",
    "vader_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>When processing text, what can you do about frequently occurring words, like \"a,\" \"and,\" \"the,\" etc.?</summary>\n",
    "\n",
    "- These words, called \"stopwords,\" can either be kept or removed.\n",
    "    - If we think these words do help explain our $Y$ variable, we might keep them. (For example, if we're classifying the era of a poem, the frequency of the word \"the\" may be helpful information!)\n",
    "    - If we think these words don't help explain our $Y$ variable, we might remove them. (For example, in sentiment analysis, we might not think that people who use \"the\" more or less frequently are happier or angrier.)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple things to note:\n",
    "1. NLP broadly describes: \n",
    "    - how we can get unstructured text data into a more structured form that can be interpreted by computers, and \n",
    "    - algorithms for interpreting text data.\n",
    "2. That does not mean these tools we used today work to the exclusion of other methods. You can and should include other variables in your model!\n",
    "    - For example, maybe the length of a review tells us something about how much people liked/disliked the movie, or maybe additional information about the reviewer (i.e. geography, age, how many reviews they had submitted) has predictive value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 226,
   "position": {
    "height": "40px",
    "left": "1070px",
    "right": "20px",
    "top": "120px",
    "width": "333px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
