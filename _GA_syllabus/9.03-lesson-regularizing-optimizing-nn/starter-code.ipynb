{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "## Regularization for Neural Networks\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of the lesson, students should be able to:\n",
    "1. Explain how the three deep learning regularization techniques work\n",
    "    * L1/L2 regularization\n",
    "    * Dropout\n",
    "    * Early stopping\n",
    "2. Implement these techniques in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = pd.read_csv('data/cell_phone_churn.csv')\n",
    "churn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode state, area_code, intl_plan, and vmail_plan columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X and y\n",
    "\n",
    "# Train/test split\n",
    "\n",
    "# Scale data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a simple neural net to model churn\n",
    "\n",
    "Let's build this:\n",
    "\n",
    "- a dense network,\n",
    "- one input layer,\n",
    "- one hidden layer \n",
    "  - slightly smaller than input layer\n",
    "  - ReLU activation\n",
    "- single node output (for binary classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What activation function will we use for our output layer here?</summary>\n",
    "\n",
    "- Sigmoid activation, since we are doing binary classification.\n",
    "\n",
    "Fun fact: If we dropped the hidden layer, this model would just be logistic regression!  Can you prove that to yourself?\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert model here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look, Ma, the machine is learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss\n",
    "train_loss = \n",
    "test_loss = \n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_loss, label='Training loss', color='navy')\n",
    "plt.plot(test_loss, label='Testing loss', color='skyblue')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/overkill.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='l1'></a>\n",
    "## Regularization Method 1: $\\mathcal{l}_1$ and $\\mathcal{l}_2$\n",
    "---\n",
    "Just as we did with linear models, we can use $\\mathcal{l}_1$ (LASSO) and $\\mathcal{l}_2$ (Ridge) regularization on our neural networks.\n",
    "\n",
    "Recall from our notation:\n",
    "\n",
    "* $J(\\theta) = L(\\mathbf{y}, \\hat{\\mathbf{y}})$ is the value of the loss function with respect to...\n",
    "* $\\theta$, the list of _all_ model coefficients (all weights and biases).\n",
    "\n",
    "We regularize our neural networks by adding a penalty term to our loss function:\n",
    "\n",
    "$$ \\text{minimize } J(\\theta) + \\lambda \\|\\theta\\|_2^2 $$\n",
    "\n",
    "This has the effect of penalizing our parameters $\\theta$ by shrinking them, hence bartering in the bias-variance tradeoff by (hopefully) reducing variance by adding bias. Although it is rarely done, **Keras actually gives us the opportunity to penalize the weights at different layers by different amounts.**\n",
    "\n",
    "## Which to pick: $\\mathcal{l}_1$ or $\\mathcal{l}_2$?\n",
    "Recall the key difference: $\\mathcal{l}_1$ penalties perform _selection_. That is, they often zero out parameters when they're small enough. While this sounds appealing, $\\mathcal{l}_2$ is used almost exclusively. It's done so often, that deep learning practitions give it a special name: **weight decay**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "# Compile it\n",
    "\n",
    "# Fit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loss\n",
    "train_loss_l2 = \n",
    "test_loss_l2 = \n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_loss, label='Training loss', color='navy')\n",
    "plt.plot(test_loss, label='Testing loss', color='skyblue')\n",
    "plt.plot(train_loss_l2, label='L2 Training loss', color='darkred')\n",
    "plt.plot(test_loss_l2, label='L2 Testing loss', color='pink')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Resources\n",
    "- [Keras Regularization Documentation](https://keras.io/regularizers/)\n",
    "- [Kernel vs. Activity Regularizers](https://github.com/keras-team/keras/issues/3236)\n",
    "\n",
    "## Tensorflow Resources\n",
    "- [Implementation in Tensorflow](https://www.tensorflow.org/versions/r0.12/api_docs/python/contrib.layers/regularizers)\n",
    "- [Example in Tensorflow](http://www.ritchieng.com/machine-learning/deep-learning/tensorflow/regularization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dropout'></a>\n",
    "## Regularization Method 2: Dropout\n",
    "---\n",
    "There's another method of regularizing our terms that is specifically designed for neural networks, called **dropout regularization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we've constructed a neural network. We've decided on the number of layers we want and the number of nodes in each layer. (We might say that we've decided on the **topology** or **structure** of our network.)\n",
    "\n",
    "![](./assets/original_nn.jpeg)\n",
    "\n",
    "However, a densely connected network like this will almost certainly overfit. Our network is learning a parameter for every single connection.\n",
    "\n",
    "> In the above example, we have 55 parameters being learned - and this is a very simple network, all things considered.\n",
    "\n",
    "> We can overcome this by using **dropout regularization**. \n",
    "\n",
    "In dropout regularization, we randomly **drop** units (nodes) in our neural network ***during our training phase only***. We assign a probability of each node disappearing. Then, we essentially perform a coinflip for every node to turn that node \"on\" or \"off.\"\n",
    "\n",
    "Let's go through an example to illustrate this: For simplicity, we'll say we've assigned a 0.5 probability of keeping to every node in the network above. Then, for every node, we flip a coin, and if the coin lands on heads, the node remains, if it lands on tails, the node disappears. After we've done this for every node, we're left with a new network that looks something like this:\n",
    "\n",
    "![](./assets/after_dropout.jpeg)\n",
    "\n",
    "<!--\n",
    "Image sources: https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/\n",
    "Also, it seems, this site: http://cs231n.github.io/neural-networks-2/\n",
    "-->\n",
    "\n",
    "Let's explicitly lay out the general workflow you would follow:\n",
    "\n",
    "1. Specify the topology of your neural network.\n",
    "2. Initialize your weights and biases.\n",
    "3. Specify the \"keeping probabilities\" for every node. (Generally, we'll assign the same probability to all nodes in each layer and usually the same probability to all hidden layers.)\n",
    "4. Perform a \"coin flip\" for each node and drop out the chosen nodes.\n",
    "5. Run through one epoch of training.\n",
    "6. Repeat steps 4 and 5 for each epoch of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/04/1IrdJ5PghD9YoOyVAQ73MJw.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>If I drop out a node during one of my epochs, does it disappear from my final network?</summary>\n",
    "\n",
    "- No!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So, what does this do?\n",
    "<!-- <br/> -->\n",
    "The intuition behind dropout is that, since each node has a probability of disappearing at any time, the neural network is disincentivized from allocating too much power to any one weight. It has a similar effect as imposing an L2 penalty: the magnitude of our weights shrinks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What might be some potential problems with doing this?</summary>\n",
    "\n",
    "- expected values of nodes changes\n",
    "- induces bias\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've now run through every epoch of our training phase and we're ready to apply our neural network to our validation or testing data.\n",
    "\n",
    "<details><summary>Are we going to apply dropout to this data as well?</summary>\n",
    "\n",
    "- No!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best practices:\n",
    "\n",
    "- Don't set any keeping probabilities for layers you where you don't want to drop any nodes.\n",
    "\n",
    "<details><summary>What might be examples of these layers?</summary>\n",
    "\n",
    "- Input layers\n",
    "- Output layers\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You'll generally want to specify a single keeping probability and all the layers on which you want to apply dropout, instead of specifying different keeping probabilities for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model using Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loss\n",
    "train_loss_dropout = \n",
    "test_loss_dropout = \n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_loss, label='Training loss', color='navy')\n",
    "plt.plot(test_loss, label='Testing loss', color='skyblue')\n",
    "plt.plot(train_loss_dropout, label='Dropout Training loss', color='darkgreen')\n",
    "plt.plot(test_loss_dropout, label='Dropout Testing loss', color='lightgreen')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Resources\n",
    "- [Keras Dropout Documentation](https://keras.io/layers/core/#dropout)\n",
    "\n",
    "## Tensorflow Resources\n",
    "- [Tensorflow documentation](https://www.tensorflow.org/api_docs/python/tf/nn/dropout)\n",
    "- [List of examples in Tensorflow](https://programtalk.com/python-examples/tensorflow.nn.dropout/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stopping'></a>\n",
    "## Regularization Method 3: Early Stopping\n",
    "---\n",
    "The third method of regularization that we'll discuss today is called early stopping.\n",
    "</br>\n",
    "If we run though all our epochs of training and plot both our training and validation error, we'll typically see something like this:\n",
    "\n",
    "![](./assets/train-val-error-reduced.png)\n",
    "*source*: [Prechelt, 1997](http://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What is happening in this plot?</summary>\n",
    "\n",
    "- The Validation error begins to increase after a certain number of epochs!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping does exactly what its name implies: it stop the training process early. Instead of continuing training through every epoch, once the validation error begins to increase, our algorithm stops because it has (in theory) found the minimum for the validation loss.\n",
    "\n",
    "This might seem like a simple and robust solution to overfitting, but it can run into problems.\n",
    "\n",
    "![](./assets/validation-error-real.png)\n",
    "\n",
    "There is debate over how often this problem occurs. You can generally plot both the training and validation loss, see if you're getting multiple optima. If you are, there are multiple suggested techniques to combat this problem in the [paper reference above](http://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build model using early stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loss\n",
    "train_loss_es = \n",
    "test_loss_es = \n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_loss, label='Training loss', color='navy')\n",
    "plt.plot(test_loss, label='Testing loss', color='skyblue')\n",
    "plt.plot(train_loss_es, label='ES Training loss', color='violet')\n",
    "plt.plot(test_loss_es, label='ES Testing loss', color='lavender')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Resources\n",
    "- [Keras EarlyStopping Documentation](https://keras.io/callbacks/#earlystopping)\n",
    "- [Keras EarlyStopping Example](http://parneetk.github.io/blog/neural-networks-in-keras/)\n",
    "\n",
    "## Tensorflow Resources\n",
    "- [Tensorflow.Keras.callbacks.EarlyStopping Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "# Conclusion\n",
    "\n",
    "Today, we learned about three different methods of regularizing our neural networks: `L2` regularization, dropout, and early stopping.\n",
    "\n",
    "## Machine Learning Workflow\n",
    "\n",
    "As promised, managing bias and variance takes a lot of our attention. If our bias or variance are high, it's likely that our model isn't performing as well as it could.\n",
    "\n",
    "A workflow for how you should address this (in the context of neural networks and beyond) is as follows:\n",
    "\n",
    "- Do we have high bias? (i.e. are we performing poorly on our training set?)\n",
    "    - If so:\n",
    "        - let's build a more complex model / bigger network!\n",
    "        - let's consider a new architecture for our neural network!\n",
    "        - let's train longer!\n",
    "- Do we have high variance? (i.e. are we performing poorly on our test/holdout set?)\n",
    "    - If so:\n",
    "        - let's gather more data!\n",
    "            - Usually very difficult, but we should use \"data augmentation\" if we can!\n",
    "        - let's build a simpler model / smaller network!\n",
    "        - let's consider a new architecture for our neural network!\n",
    "        - let's regularize!\n",
    "    - Once we're satisfied, return to the bias question and repeat.\n",
    "    \n",
    "**Note:** Before deep learning, most tools for handling high bias or high variance adversely affected the other. However, depending on the amount of data we have and how complex our network is, it's often the case that we can drastically reduce variance with out affecting bias.\n",
    "\n",
    "<a id='references'></a>\n",
    "## References and Resources:\n",
    "\n",
    "- [DeepLearning.ai](https://www.deeplearning.ai/), Andrew Ng's Coursera course on Deep Learning\n",
    "  - The videos from this course are on a [YouTube Channel](https://www.youtube.com/channel/UCcIXc5mJsHVYTZR1maL5l9w/featured)   \n",
    "<br>\n",
    "- [Deep Learning Book](http://www.deeplearningbook.org/), textbook written by Ian Goodfellow, creator of Generative Adversarial Networks (GANs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
