# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Support Vector Machines

---

## Materials We Provide


| Topic | Description | Link |
| --- | --- | --- |
| Lesson | Support Vector Machines Code-Along | [Link](./starter-code.ipynb)|

> Dataset Description: MNIST Handwritten Digits Dataset
---

## Learning Objectives

*After this lesson, students will be able to:*
1. **Describe** linear separability.
2. **Differentiate between** maximal margin classifiers, support vector classifiers, and support vector machines.
3. **Implement** SVMs in `scikit-learn`.
4. **Describe** the effects of `C` and kernels on SVMs.

---

## Student Requirements

*Before this lesson(s), students should already be able to:*

1. Describe the difference between regression and classification.
2. Evaluate the performance of regression and classification models.
3. Understand the bias-variance tradeoff.

---

## Lesson Outline

> **Total Time: 90 minutes**

## Agenda
I. **Support Vector Machines** (60 minutes total)
- Intuition
- Maximal Margin Classifiers
- Support Vector Classifiers
- Support Vector Machines
- Kernel Trick

II. **Coding** (30 minutes total)
---


## OPTIONAL: Resources for Practice and Learning

*For supplemental reading material on this topic, check out the following resources:*
- For an excellent resource with visuals, check out [these slides](http://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf) from the University of Oxford.
- For a more academic resource that walks through identifying the separating hyperplane and corresponding margin, check out [these Stanford notes](https://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html).
- SVM documentation on [SKLearn](http://scikit-learn.org/stable/modules/svm.html)
- Visualization of different kernels with iris data [SKLearn](https://scikit-learn.org/stable/auto_examples/exercises/plot_iris_exercise.html#sphx-glr-auto-examples-exercises-plot-iris-exercise-py)
- Hyperplane walkthrough on [SKLearn](http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html#example-svm-plot-separating-hyperplane-py)
- A comprehensive [user guide](http://pyml.sourceforge.net/doc/howto.pdf) to support vector machines.
- A [blog post tutorial](http://www.svm-tutorial.com/2014/11/svm-understanding-math-part-2/) of understanding the linear algebra behind SVM hyperplanes. Check [part 3](http://www.svm-tutorial.com/2015/06/svm-understanding-math-part-3/) of this blog on finding the optimal hyperplane
- This [Quora discussion](https://www.quora.com/How-do-you-teach-Support-Vector-Machine-to-a-beginner-in-Machine-Learning) includes a high-level overview, plus a [20 minute video](https://www.youtube.com/watch?v=aDbsJ_S3tIA) walking through the core "need-to-knows."
- A [slideshow introduction](http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf) to the optimization considerations of SVM
- A second [slideshow overview from UCF](http://www.cs.ucf.edu/courses/cap6412/fall2009/papers/Berwick2003.pdf) on the high notes of SVM.
- Andrew Ng's [notes](http://cs229.stanford.edu/notes/cs229-notes3.pdf) on support vector machines from Stanford's CS 229 course.
- A [full lecture](https://www.youtube.com/watch?v=eHsErlPJWUU) (1+ hour) on SVMs from Dr. Yasser. He does a follow-up on [kernel tricks](https://www.youtube.com/watch?v=XUj5JbQihlU) too.
- A [full lecture](https://www.youtube.com/watch?v=_PwhiWxHK8o) (50 minutes) on SVMs from MIT Open Courseware.
- An infamous [paper](https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf) (cited 7000+ times!) on why SVM is a great text classifier.
- An [advanced discussion](http://www.icml-2011.org/papers/386_icmlpaper.pdf) of SVMs as probabilistic models.
- A [quick overview](https://www.youtube.com/watch?v=YsiWisFFruY) of SVMs (h/t Mukul Ram, DSI-DC-5).
- [YouTube Playlist](https://www.youtube.com/playlist?list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v) on Machine Learning from Sentdex. The support vector machine videos are marked 20 - 33.
---
