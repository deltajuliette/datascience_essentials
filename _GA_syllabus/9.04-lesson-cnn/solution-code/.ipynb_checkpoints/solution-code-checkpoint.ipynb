{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Convolutional Neural Networks\n",
    "\n",
    "\n",
    "\n",
    "**By the end of the lesson, you should be able to:**\n",
    "- Identify use cases for convolutional neural networks and when they are superior to other neural networks.\n",
    "- Describe convolutional and pooling layers.\n",
    "- Define padding and filters.\n",
    "- Fit CNNs in Keras.\n",
    "- Use transfer learning for real world image classification tasks!\n",
    "- Intuiton Deck is here: [Link](https://docs.google.com/presentation/d/1FiwVBKFULaDaEa194XvqoQuHDBHRAeNSADGzqSRy6b8/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Handwritten Digits dataset\n",
    "- Convolutional neural networks are generally used when we are dealing with image data. They can absolutely be used for NLP as well!\n",
    "- Their main advantage over densely connected neural networks is the ability to capture spatial information.\n",
    "- There are many versions of mnist\n",
    "    - MNIST Original Digits: [Link](https://paperswithcode.com/dataset/mnist)\n",
    "    - Fashion MNIST: [Link](https://www.kaggle.com/zalando-research/fashionmnist)\n",
    "    - Chinese MNIST: [Link](https://www.kaggle.com/gpreda/chinese-mnist)\n",
    "    \n",
    "- Today we'll be working with the original MNIST handwritten digits dataset\n",
    "![](../images/MnistExamples.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n",
      "11501568/11490434 [==============================] - 1s 0us/step\n",
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load pre-shuffled MNIST data into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check one value of X_train.\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digit(X, y):\n",
    "    plt.imshow(X, cmap=\"gray\")\n",
    "    plt.title(f'Label: {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_digit(X_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/picture.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "[Image by 3Blue1Brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The current range of X_train and X_test is 0 to 255.\n",
    "# This scales each value to be between 0 and 1.\n",
    "X_train = X_train/255.\n",
    "X_test = X_test/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at y_train\n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What change do I need to make to y_train? Why?</summary>\n",
    "\n",
    "- Right now, the values of `y_train` will be interpreted as a number. That means, if instead of `5`, we predict `4`, it will be treated as **better** than if we predict `1`. But this is not what we want. We want all inaccurate predictions to be treated equally. In other words, our y is a categorical variable!\n",
    "- We need to one hot encode it.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change y_train.\n",
    "y_train = utils.to_categorical(y_train, 10)\n",
    "y_test = utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check y_train again\n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Let's fit a feedforward neural network to this data.\n",
    "- Our image is black and white so only 1 channel\n",
    "- But it's still in 2D!\n",
    "- Let's flatten the image to make it a 1D vector so that our feed forward neural network can process it\n",
    "\n",
    "<img src=\"https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/73_blog_image_1.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "<img src=\"../images/picture.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "<img src=\"../images/network.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "[Images by 3Blue1Brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 104,938\n",
      "Trainable params: 104,938\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# In order to \"align\" our neurons in a vertical array, we\n",
    "# add a \"Flatten\" layer. This will be required before adding\n",
    "# subsequent Dense layers. We need to specify the input shape.\n",
    "model.add(Flatten(input_shape=(28, 28, 1)))\n",
    "\n",
    "# Let's create a fully-connected, feed-forward neural net with:\n",
    "# - A 128-node hidden layer.\n",
    "# - A 32-node hidden layer.\n",
    "# - The appropriate output layer (multiclass classification)\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How many parameters are being fit in this model?</summary>\n",
    "\n",
    "- The input to the first hidden layer is $784$ nodes $+1$ bias. And the layer has $128$ nodes. Thus number of parameters for first hidden layer is $785*128=100480$ parameters.\n",
    "- The input to the second hidden layer is $128$ nodes $+1$ bias. And the layer has $32$ nodes. Thus number of parameters for second hidden layer is $129*32=4128$ parameters.\n",
    "- The input to the output layer is $32$ nodes $+1$ bias. And the layer has $10$ nodes. Thus number of parameters for output layer is $33*10=330$ parameters.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What are some consequences of fitting a model with too many parameters?</summary>\n",
    "\n",
    "- Easy to overfit our model.\n",
    "- Learning is quite slow.\n",
    "- We need more data in order to meaningfully learn and fit a model!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model.\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "235/235 [==============================] - 2s 4ms/step - loss: 0.4600 - accuracy: 0.8698 - val_loss: 0.2138 - val_accuracy: 0.9375\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1833 - accuracy: 0.9470 - val_loss: 0.1474 - val_accuracy: 0.9558\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1342 - accuracy: 0.9610 - val_loss: 0.1199 - val_accuracy: 0.9622\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1064 - accuracy: 0.9687 - val_loss: 0.1091 - val_accuracy: 0.9656\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0870 - accuracy: 0.9735 - val_loss: 0.0992 - val_accuracy: 0.9695\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0740 - accuracy: 0.9780 - val_loss: 0.0884 - val_accuracy: 0.9721\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0611 - accuracy: 0.9824 - val_loss: 0.0905 - val_accuracy: 0.9712\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0541 - accuracy: 0.9838 - val_loss: 0.0924 - val_accuracy: 0.9725\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0453 - accuracy: 0.9866 - val_loss: 0.0809 - val_accuracy: 0.9755\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0393 - accuracy: 0.9889 - val_loss: 0.0852 - val_accuracy: 0.9736\n"
     ]
    }
   ],
   "source": [
    "# Fit model on training data.\n",
    "history = model.fit(X_train,\n",
    "                    y_train, \n",
    "                    batch_size=256,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=10,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfEAAAFzCAYAAAAuSjCuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA6LUlEQVR4nO3deXydZZ3//9eVc7KvJ1uXpD3pvoeWpC20DFBABNkqi4BlRwVHQXF0wPHryPyc7+jM8FUHRREdVpFVNqWKFFkLQpvSvaV0SdukbZKmzdbsJ9fvj/tka9M2bc6dc5Lzfj4eeZwld879aYC+ua77uq+PsdYiIiIiQ09MuAsQERGRk6MQFxERGaIU4iIiIkOUQlxERGSIUoiLiIgMUQpxERGRIcob7gJOVHZ2ti0oKAh3GSIiIoOmpKRkv7U25/D3h1yIFxQUsHLlynCXISIiMmiMMTv7el/T6SIiIkOUQlxERGSIUoiLiIgMUUPumriIiIROW1sbZWVlNDc3h7sUARISEsjPzyc2NrZfxyvERUSiWFlZGampqRQUFGCMCXc5Uc1aS3V1NWVlZYwbN65fP6PpdBGRKNbc3ExWVpYCPAIYY8jKyjqhWRGFuIhIlFOAR44T/WehEBcRkbCprq5m9uzZzJ49m5EjR5KXl9f1urW19Zg/u3LlSu68887jnmPBggUhqfWtt97i4osvDslnhYquiYuISNhkZWWxevVqAO69915SUlL49re/3fX99vZ2vN6+o6q4uJji4uLjnuP9998PSa2RSCNxERGJKDfddBPf+ta3WLRoEXfffTcfffQRCxYsYM6cOSxYsIBPPvkE6D0yvvfee7nllls4++yzGT9+PPfff3/X56WkpHQdf/bZZ3PllVcydepUlixZgrUWgKVLlzJ16lTOOOMM7rzzzhMacT/11FPMmjWLmTNncvfddwMQCAS46aabmDlzJrNmzeKnP/0pAPfffz/Tp0+nsLCQa665ZsC/K43ERUQEgH/74wY27qkL6WdOH53GDy6ZccI/t2XLFpYtW4bH46Guro533nkHr9fLsmXL+Jd/+Rf+8Ic/HPEzmzdv5s0336S+vp4pU6bw1a9+9YhbtT7++GM2bNjA6NGjWbhwIcuXL6e4uJjbbruNd955h3HjxnHttdf2u849e/Zw9913U1JSgs/n4/zzz+ell15izJgxlJeXs379egBqamoA+PGPf8yOHTuIj4/vem8gonokXtvUxmsb9nGopT3cpYiISA9XXXUVHo8HgNraWq666ipmzpzJXXfdxYYNG/r8mYsuuoj4+Hiys7PJzc2loqLiiGPmzZtHfn4+MTExzJ49m9LSUjZv3sz48eO7bus6kRBfsWIFZ599Njk5OXi9XpYsWcI777zD+PHj2b59O3fccQd/+ctfSEtLA6CwsJAlS5bwu9/97qiXCU5EVI/EV++u4bYnSvjdrfM5Y1J2uMsREQmrkxkxuyU5Obnr+fe//30WLVrEiy++SGlpKWeffXafPxMfH9/13OPx0N5+5ACtr2M6p9RPxtF+1ufzsWbNGl577TUeeOABnn32WR5++GFeffVV3nnnHV555RV++MMfsmHDhgGFeVSPxE8dm0GMgZU7D4S7FBEROYra2lry8vIAePTRR0P++VOnTmX79u2UlpYC8Mwzz/T7Z+fPn8/bb7/N/v37CQQCPPXUU5x11lns37+fjo4OrrjiCn74wx+yatUqOjo62L17N4sWLeK//uu/qKmpoaGhYUC1R/VIPDUhlikj01hZejDcpYiIyFH88z//MzfeeCM/+clPOOecc0L++YmJifzyl7/kggsuIDs7m3nz5h312DfeeIP8/Pyu18899xw/+tGPWLRoEdZaPve5z3HZZZexZs0abr75Zjo6OgD40Y9+RCAQ4LrrrqO2thZrLXfddRcZGRkDqt0MZBohHIqLi20o+4l//6X1vLCqjDU/OB+vJ6onJkQkCm3atIlp06aFu4ywa2hoICUlBWstX/va15g0aRJ33XVXWGrp65+JMabEWnvE/XRRn1rFBT4OtQbYvK8+3KWIiEiY/OY3v2H27NnMmDGD2tpabrvttnCX1C9RPZ0OUOT3AVCy8yAz89LDXI2IiITDXXfdFbaR90BE/Ug8LyORUekJrNyp6+IiIjK0RH2IG2Mo8vtYWaoV6iIiMrREfYgDFPt97K1tprymKdyliIiI9JtCHCguyATQaFxERIYUhTgwdWQqSXEeSnRdXERkUA2kFSk4TU16dil78MEHefzxx0NS29lnn00ob2l2Q9SvTgfwemI4daxPm76IiAyy47UiPZ633nqLlJSUrp7ht99+uxtlRiyNxIOK/D4276ujvrkt3KWIiES1kpISzjrrLIqKivjsZz/L3r17gSPbeJaWlvLggw/y05/+lNmzZ/Puu+9y7733ct999wHOSPruu+9m3rx5TJ48mXfffReAxsZGvvCFL1BYWMjVV1/N/Pnz+z3iPnDgAIsXL6awsJDTTjuNtWvXAvD22293zSDMmTOH+vp69u7dy5lnnsns2bOZOXNm1/lDSSPxoOICHx0WPt5Vw5mTc8JdjojI4PvzPbBvXWg/c+QsuPDH/T7cWssdd9zByy+/TE5ODs888wzf+973ePjhh49o45mRkcHtt9/ea/T+xhtv9Pq89vZ2PvroI5YuXcq//du/sWzZMn75y1/i8/lYu3Yt69evZ/bs2f2u7wc/+AFz5szhpZde4m9/+xs33HADq1ev5r777uOBBx5g4cKFNDQ0kJCQwEMPPcRnP/tZvve97xEIBGhsbOz3efpLIR40Z6wv2AzloEJcRCRMWlpaWL9+PZ/5zGcACAQCjBo1Cuhu47l48WIWL17cr8+7/PLLASgqKupqcPLee+/xjW98A4CZM2dSWFjY7/ree++9rl7m55xzDtXV1dTW1rJw4UK+9a1vsWTJEi6//HLy8/OZO3cut9xyC21tbSxevPiE/mehvxTiQSnxXqaOTKNEHc1EJFqdwIjZLdZaZsyYwQcffHDE9/pq43k8na1He7YmDXXrUWMM99xzDxdddBFLly7ltNNOY9myZZx55pm88847vPrqq1x//fV85zvf4YYbbjjpc/dF18R7mFvg4+NdNbQHOsJdiohIVIqPj6eqqqorxNva2tiwYcNR23impqZSX39ivS/OOOMMnn32WQA2btzIunX9v4Rw5pln8uSTTwLOorrs7GzS0tLYtm0bs2bN4u6776a4uJjNmzezc+dOcnNz+fKXv8ytt97KqlWrTqjO/tBIvIeigkwe+2Anm/bWMytf+6iLiAy2mJgYnn/+ee68805qa2tpb2/nm9/8JpMnT+6zjecll1zClVdeycsvv8zPf/7zfp3jH//xH7nxxhspLCxkzpw5FBYWkp7e99/5F110EbGxsQCcfvrp/PrXv+bmm2+msLCQpKQkHnvsMQB+9rOf8eabb+LxeJg+fToXXnghTz/9NP/93/9NbGwsKSkpIbv1raeob0Xa056aJhb8+G/84JLp3LxwnCvnEBGJJNHYijQQCNDW1kZCQgLbtm3j3HPPZcuWLcTFxYW7NODEWpFqJN7D6IxERgeboSjERUSGp8bGRhYtWkRbWxvWWn71q19FTICfKIX4YYoKMlmx4wDWWowx4S5HRERCLDU1NeJ3YusvLWw7zNwCH/vq1AxFREQin0L8MEV+H4C2YBWRqDHU1kYNZyf6z0IhfpipI9NIifeyUveLi0gUSEhIoLq6WkEeAay1VFdXk5CQ0O+f0TXxw3hiDHPGZmgkLiJRIT8/n7KyMqqqqsJdiuD8T1V+fn6/j1eI96HI7+N/3viUuuY20hJiw12OiIhrYmNjGTdOd+MMVZpO78PcgkxssBmKiIhIpFKI92H2mAw8MYaVpbouLiIikUsh3ofkeC/TRqXquriIiEQ0hfhRFPszWb27hjY1QxERkQilED+KIr+PprYAm/bWhbsUERGRPinEj6K4QJu+iIhIZFOIH8Wo9ETyMhK16YuIiEQshfgxFBf4WFl6UDsZiYhIRFKIH0Ox30dlfQtlB9UMRUREIo9C/BiK/JkAmlIXEZGIpBA/hikjU0mN92pxm4iIRCSF+DF4Ygxz/D6FuIiIRCSF+HEU+31sqayntqkt3KWIiIj04mqIG2MuMMZ8YozZaoy55xjHzTXGBIwxV7pZz8ko9vuwFlbt0mhcREQii2shbozxAA8AFwLTgWuNMdOPctx/Aq+5VctAzB7rNEMp0ZS6iIhEGDdH4vOArdba7dbaVuBp4LI+jrsD+ANQ6WItJy0pzsuM0WlaoS4iIhHHzRDPA3b3eF0WfK+LMSYP+DzwoIt1DFiR36dmKCIiEnHcDHHTx3uHb332M+Bua23gmB9kzFeMMSuNMSurqqpCVV+/FfszaW7rYMMeNUMREZHI4WaIlwFjerzOB/Ycdkwx8LQxphS4EvilMWbx4R9krX3IWltsrS3Oyclxqdyj626Goil1ERGJHG6G+ApgkjFmnDEmDrgGeKXnAdbacdbaAmttAfA88I/W2pdcrOmkjEhLIN+XSMlOLW4TEZHI4XXrg6217caYr+OsOvcAD1trNxhjbg9+P6Kvgx9ubkEm723dj7UWY/q6UiAiIjK4XAtxAGvtUmDpYe/1Gd7W2pvcrGWgivw+Xvy4nN0HmhiblRTuckRERLRjW391XhdfoeviIiISIRTi/TQ5N5XUBC8rdV1cREQihEK8n2JiDKeO9VGiTV9ERCRCKMRPwNwCH1sqGqhtVDMUEREJP4X4CSjyZwJqhiIiIpFBIX4CZo/JwBtjtLhNREQigkL8BCTGeYLNUDQSFxGR8FOIn6AifyZrdtfQ2q5mKCIiEl4K8RM0t8BHS3sHG/bUhrsUERGJcgrxE1QU3PRF+6iLiEi4KcRPUG5qAmMzk7S4TUREwk4hfhKK/T5Kdh7E2sPbo4uIiAwehfhJKCrwsb+hlZ3VjeEuRUREophC/CTMLXA2fdGtZiIiEk4K8ZMwMSeFtASv9lEXEZGwUoifhJgYQ5Hfx4pSjcRFRCR8FOInqbggk62VDdQ0toa7FBERiVIK8ZNU5Nf94iIiEl4K8ZN0Sn4GsR6jxW0iIhI2CvGT5DRDSadE18VFRCRMFOIDUOz3sbqshpb2QLhLERGRKKQQH4DiAh+t7R2sL68LdykiIhKFFOIDUOR3Nn3R/eIiIhIOCvEByEmNpyAriZW6Li4iImGgEB+gIn+mmqGIiEhYKMQHqLjAR/WhVnbsPxTuUkREJMooxAeoOLjpi+4XFxGRwaYQH6AJOSmkJ8bqfnERERl0CvEBiokxFPt9rNQKdRERGWQK8RAoKvCxreoQBw6pGYqIiAwehXgIFHfdL64pdRERGTwK8RAozE8PNkPRlLqIiAwehXgIJMR6mJmnZigiIjK4FOIhMrcgk7XltWqGIiIig0YhHiJF/s5mKLXhLkVERKKEQjxEioKbvqzQlLqIiAwShXiIZKfEMy47Wc1QRERk0CjEQ6jI72PVLjVDERGRwaEQD6G5BT4OHGplu5qhiIjIIFCIh1BR56YvmlIXEZFBoBAPoQk5yfiSYllRqk1fRETEfQrxEDLGUOT3aftVEREZFArxECvyZ7J9/yGqG1rCXYqIiAxzCvEQm1vg3C+u0biIiLhNIR5iM/PSifPEKMRFRMR1CvEQS4j1MCs/XYvbRETEdQpxFxT7fawvr6O5Tc1QRETEPQpxFxT5fbQGOlinZigiIuIihbgLOpuhaB91ERFxk0LcBVkp8YzPSaZkp66Li4iIexTiLin2+1i58yAdHWqGIiIi7lCIu6TYn0lNYxvb9zeEuxQRERmmFOIuKSrQdXEREXGXQtwl47OTyUyOY6U2fREREZcoxF2iZigiIuI2hbiLiv0+duw/RFW9mqGIiEjoKcRdVKxmKCIi4iKFuItm5qUT543R/eIiIuIKhbiL4r0eTslP1+I2ERFxhULcZUX+TNaX16oZioiIhJxC3GXFfh9tAcua3TXhLkVERIYZhbjLupqhaEpdRERCzNUQN8ZcYIz5xBiz1RhzTx/fv8wYs9YYs9oYs9IYc4ab9YSDLzmOCTnJWqEuIiIh51qIG2M8wAPAhcB04FpjzPTDDnsDOMVaOxu4BfitW/WE09yCTErUDEVERELMzZH4PGCrtXa7tbYVeBq4rOcB1toGa21nsiUDwzLlivw+apva2FalZigiIhI6boZ4HrC7x+uy4Hu9GGM+b4zZDLyKMxo/gjHmK8Hp9pVVVVWuFOum4oJMAFaoGYqIiISQmyFu+njviJG2tfZFa+1UYDHww74+yFr7kLW22FpbnJOTE9oqB0FBVhJZyXGs1KYvIiISQm6GeBkwpsfrfGDP0Q621r4DTDDGZLtYU1ioGYqIiLjBzRBfAUwyxowzxsQB1wCv9DzAGDPRGGOCz08F4oBqF2sKm7kFmeysblQzFBERCRnXQtxa2w58HXgN2AQ8a63dYIy53Rhze/CwK4D1xpjVOCvZr+6x0G1YKepqhqIpdRERCQ2vmx9urV0KLD3svQd7PP9P4D/drCFSzBydTrw3hhWlB7lg5qhwlyMiIsOAdmwbJHHeGE7Jz9DObSIiEjIK8UFUVOBjQ3ktTa1qhiIiIgOnEB9Ecwt8tHdY1pTVhLsUEREZBhTig+jUsZ2L2zSlLiIiA6cQH0QZSXFMyk1hRalWqIuIyMApxAdZcYGPVWqGIiIiIaAQH2RF/kzqmtv5tFLNUEREZGAU4oNsbnDTF+2jLiIiA6UQH2RjM5PITomnRB3NRERkgBTig8wYQ7HfxwqNxEVEZIAU4mFQXOBj94EmKuuaw12KiIgMYQrxMCjyd14X15S6iIicPIV4GMwYnU5CbAwrdV1cREQGQCEeBp3NUNSWVEREBkIhHibFBT7W76mjsbU93KWIiMgQpRAPk2J/JoEOy+rdNeEuRUREhiiFeJh0NUPRdXERETlJCvEwSU+KZcqIVK1QFxGRk6YQD6OiAh+rdh0koGYoIiJyEhTiYVTs91Hf3M6WivpwlyIiIkOQQjyMiv2ZgDZ9ERGRk9OvEDfGJBtjYoLPJxtjLjXGxLpb2vA3JjORnNR4Skp1v7iIiJy4/o7E3wESjDF5wBvAzcCjbhUVLYwxzC3waSQuIiInpb8hbqy1jcDlwM+ttZ8HprtXVvQo8mdSdrCJfbVqhiIiIiem3yFujDkdWAK8GnzP605J0aW4qxmKptRFROTE9DfEvwl8F3jRWrvBGDMeeNO1qqLI9NFpJMZ61AxFREROWL9G09bat4G3AYIL3PZba+90s7BoEeuJ4ZQx6ZTouriIiJyg/q5O/70xJs0YkwxsBD4xxnzH3dKix9yCTDbureNQi5qhiIhI//V3On26tbYOWAwsBcYC17tVVLQp8vsIdFjWqBmKiIicgP6GeGzwvvDFwMvW2jZAe4WGyKl+H8bACl0XFxGRE9DfEP81UAokA+8YY/xAnVtFRZu0hM5mKFqhLiIi/devELfW3m+tzbPWfs46dgKLXK4tqhT5fXy8q0bNUEREpN/6u7At3RjzE2PMyuDX/8MZlUuIzC3IpKGlnU/2qRmKiIj0T3+n0x8G6oEvBL/qgEfcKioaFQU3fSnRlLqIiPRTf0N8grX2B9ba7cGvfwPGu1lYtMn3JTIiLV6L20REpN/6G+JNxpgzOl8YYxYCTe6UFJ2MMRT7M7Xpi4iI9Ft/9z+/HXjcGJMefH0QuNGdkqJXkd/Hq+v2sre2iVHpieEuR0REIlx/V6evsdaeAhQChdbaOcA5rlYWheYWZAJoH3UREemX/k6nA2CtrQvu3AbwLRfqiWrTRqWSFOfRlLqIiPTLCYX4YUzIqhAAvJ4YZo/JYEWpVqiLiMjxDSTEtSuJC4r9PjbtraNBzVBEROQ4jhnixph6Y0xdH1/1wOhBqjGqFBVk0mFh9a6acJciIiIR7pghbq1Ntdam9fGVaq3t78p2OQGnjs0gxqB91EVE5LgGMp0uLkhNiGXKyDQtbhMRkeNSiEegYr+PVTsP0h7oCHcpIiISwRTiEai4wMeh1gCb1QxFRESOQSEegbqboWhKXUREjk4hHoHyMhIZlZ7ASoW4iIgcg0I8AhljKPL7KNGmLyIicgwK8QhV7Pexp7aZ8ho1ixMRkb4pxCNUcVczFI3GRUSkbwrxCDV1pJqhiIjIsSnEI5TXE8OpY31qSyoiIkelEO+I3A1Vivw+Nu+ro765LdyliIhIBIruEN+3Dn51OlRuDnclfSou8NFh4WM1QxERkT5Ed4jHJkHTQXj8MqjeFu5qjjBnrC/YDEVT6iIicqToDvGsCXDDK9DRBo9dCjW7wl1RLynxXqaOTKNEHc1ERKQP0R3iALlT4fqXoLUeHrsE6vaEu6Je5hb4+HhXjZqhiIjIERTiAKMK4boX4VC1MyJvqAx3RV2KCjJpVDMUERHpg0K8U34RLHkW6srh8cXQGBlT2MXBZigrtOmLiIgcRiHek38BXPsUVG+FJz4PTTXhrojRGYmMVjMUERHpg6shboy5wBjziTFmqzHmnj6+v8QYszb49b4x5hQ36+mX8WfD1b+Dig3w5FXQ0hDuiigqyKSk9CDW2nCXIiIiEcS1EDfGeIAHgAuB6cC1xpjphx22AzjLWlsI/BB4yK16Tsjk8+HKh6G8BJ66Blobw1rO3AIf++rUDEVERHpzcyQ+D9hqrd1urW0FngYu63mAtfZ9a23nPPHfgXwX6zkx0y+Fz/8aSt+DZ66D9pawlVIUvC6ufdRFRKQnN0M8D9jd43VZ8L2juRX4s4v1nLjCq+DSn8O2N+C5myEQnu1Pp45MIyXeq33URUSkFzdD3PTxXp8XdY0xi3BC/O6jfP8rxpiVxpiVVVVVISyxH069Hj53H3zyKrzwZQi0D+75AU+MYc7YDK1QFxGRXtwM8TJgTI/X+cARO6kYYwqB3wKXWWur+/oga+1D1tpia21xTk6OK8Ue07wvw/n/DhtehFe+HpamKUV+H59U1FOnZigiIhLkZoivACYZY8YZY+KAa4BXeh5gjBkLvABcb63d4mItA7fgDlj0PVjzFLz6LRjkleJzCzKxaoYiIiI9eN36YGttuzHm68BrgAd42Fq7wRhze/D7DwL/CmQBvzTGALRba4vdqmnAzvwOtDXBez8BbwJc8CMwfV01CL3ZYzLwxBhKSg9w1uQwzEaIiEjEcS3EAay1S4Glh733YI/nXwK+5GYNIWUMnPuvTpB/+CuITXReD0KQJ8d7mTYqVZu+iIhIF1dDfFgyxhmBtzc7I/LYRDjrnwfl1MX+TJ5ZsZu2QAexHm22JyIS7ZQEJ8MYuOgncMq18Ob/heX3D8ppi/w+mtoCbNpbNyjnExGRyKaR+MmKiYFLf+GMyF//vjMin/dlV09ZXOBs+rKy9CCF+RmunktERCKfRuID4fHC5b+BKRfB0m/DqidcPd2o9ETyMhK1c5uIiAAK8YHzxMJVj8CEc+GVO2Dtc66errjAx8qdB9QMRUREFOIh4Y13Op8VnAEv3gYbXzn+z5ykYr+PiroWyg6qGYqISLRTiIdKXBJc+zTkFcHzt8CWv7pymtMnZAFw1zOr2Vfb7Mo5RERkaFCIh1J8Clz3PIyY4XQ+2/ZmyE8xMTeV+6+dw8a9dVz883dZvnV/yM8hIiJDg0I81BLS4foXIWsiPP1F2Pl+yE9x6SmjeeXrC8lIiuP6//2QX/ztUzo6dI1cRCTaKMTdkJQJN7wM6fnw5BegrCTkp5iYm8rLX1vIJaeM5r6/buHWx1ZQ09ga8vOIiEjkUoi7JSXHCfLkLPjd52Hv2pCfIjney8+uns0PF89k+dZqLrr/Pdbsrgn5eUREJDIpxN2UNhpu/CPEpcITi6FyU8hPYYzh+tP8PHf76QBc9eAHPPH3nboFTUQkCijE3ZYxFm58BWJi4fHLoHqbK6c5ZUwGf7rjDBZMzOL7L63nrmdWc6il3ZVziYhIZFCID4asCU6QdwTgsUvg4E5XTuNLjuPhG+fy7fMn88qaPSx+YDlbK+tdOZeIiISfQnyw5EyBG16C1kNOkNeWu3KamBjD18+ZxBO3zufAoVYu/cVyXlmzx5VziYhIeCnEB9PIWXD9C9B0EB6/FBoqXTvVwonZvHrnPzB9VBp3PvUxP3h5PS3tAdfOJyIig08hPtjyimDJc1C3x7lGfqjatVONTE/gqa+cxpfOGMdjH+zkC7/+O+U12q5VRGS4UIiHw9jTnC1aD2x3bj9rqnHtVLGeGP7PxdN58LpT2VbZwEX3v8tbn7g3AyAiIoNHIR4u48+Cq5+Eio3w5JXQ4u4CtAtmjuKPd5zByLQEbn50BT95fQsB7fImIjKkKcTDadJ5cNWjUL4Kfn81tDa6erpx2cm8+I8LueLUfO5/41NueuQjqhtaXD2niIi4RyEebtMuhssfgl0fOHutt7nbmSwxzsN9V53Cf14xiw93HOCi+9+jZOcBV88pIiLuUIhHgllXwqW/gO1vwnM3Qbv7e6BfPXcsL3x1AXHeGK7+9d/53/d2aJc3EZEhRiEeKeYsgYv+H2z5M7zwZQi4v9vazLx0/njHGSyamssP/7SRr/1+FfXNba6fV0REQkMhHknmfgk++x+w8SV4+WvQ0eH6KdMTY3no+iK+e+FUXttQwWW/WM7mfXWun1dERAZOIR5pTv8anPN9WPs0/OmbMAhT3MYYbjtrAr//0nzqW9pZ/MBy/lBS5vp5RURkYBTikejMb8M/fBtWPQZ/uWdQghxg/vgsXr3zDGaPyeCfnlvDd19YR3ObdnkTEYlUCvFIdc7/gdO/Dh8+CMvuHbQgz01N4He3zuerZ0/gqY92ccWv3mdXtbu3vomIyMlRiEcqY+D8f4fiW2H5z+Dt/xq0U3s9Mdx9wVR+e0Mxuw80cvHP32XZxopBO7+IiPSPQjySGQOfuw9mXwdv/Qcs/59BPf1500fw6p3/wNisJL70+Ep+/OfNtAfcX2wnIiL9oxCPdDExcOn9MPMKeP1f4cOHBvX0YzKTeP72BXxx/lgefHsbS377IZX17m5IIyIi/aMQHwpiPPD5X8PUi+HP34GSxwb19AmxHv7j87P4yRdOYU1ZDRfd/x5/3+5e9zUREekfhfhQ4YmFKx+GiZ+BP34DXvgKlJcMagmXn5rPy187g9R4L0t++yEPvr1Nu7yJiISRQnwo8cbD1U/AaV+FzUvhN+fAb8+Ddc8PylatAFNGpvLy1xdywYyR/PjPm/ny4yXUNmmXNxGRcDBDbSRVXFxsV65cGe4ywq+5DtY8BR/+Gg5sg5SRMPdWKLoJUnJdP721lkeWl/IfSzcxKiOBXy0pYmZeuuvnFRGJRsaYEmtt8RHvK8SHuI4O2LrMuZ982xvgiXMWwc2/DUbPcf30JTsP8vXfr6L6UCv/36UzuHruGIwxrp9XRCSaKMSjQdUW+OghWP17aDsEY+Y7YT7tUueaukuqG1r45jOreffT/Vxxaj7/vngmiXEe184nIhJtFOLRpLkWPn4SPvo1HCyF1NEw9xYouhmSs105ZaDDcv8bn3L/3z5lyohUfrnkVMbnpLhyLhGRaKMQj0YdAfj0dWeqffub4ImHWVfB/K/AqFNcOeVbn1Ry1zOraQtY/uvKQj43a5Qr5xERiSYK8WhXudmZal/zFLQ1wtgFzlT71IvB4w3pqcprmvjak6tYvbuGW88Yxz0XTiXWoxshREROlkJcHE0Hg1PtD0HNTkjLc/qYn3ojJGeF7DSt7R38x9JNPPp+KUV+Hw988VRGpieE7PNFRKKJQlx66wjAltecqfYdb4M3ITjVfhuMnBWy0/xxzR7u+cNaEmI9fOv8yVw+J1+L3kRETpBCXI6uYmNwqv1paG8C/xlOmE/5XEim2rdWNvBPz65mTVkt6YmxXDNvDDecXkBeRmIIihcRGf4U4nJ8jQfg4yfgo99A7W5IHxOcar8BkjIH9NHWWlbuPMjD7+3gtQ37MMZwwYyR3LywgCK/T/eWi4gcg0Jc+i/QDlv+7OwGV/oueBOh8AvO6HzEjAF/fNnBRp74YCdPfbSLuuZ2ZuWlc/PCAi4uHE2cVwvgREQOpxCXk7NvvXO/+dpnob0ZCv4B5t8OUy50uqsNQGNrOy+sKueR5TvYVnWInNR4rpvv54vzx5KTGh+iP4CIyNCnEJeBaTwAqx6Dj34LdWWQMRbmfQXmXAeJvgF9dEeH5d2t+3lk+Q7e+qSKOE8Ml5wympsXFmg/dhERFOISKoF2+ORVZ6p953KITYJTroF5t0Hu1AF//LaqBh57v5TnS8pobA0wb1wmtyws4DPTR+KJ0XVzEYlOCnEJvb1rg1Ptz0GgBcaf7Uy1Tzp/wFPttU1tPLtiN499UErZwSbyMhK5cYGfq+eOJT3RvX3gRUQikUJc3HOoGlY96ky11+8BX4Ez1T57CSRmDOijAx2W1zdW8MjyHXy44wBJcR6uODWfmxYWMEF7s4tIlFCIi/sCbbD5T85U+64PIDYZZl/rTLXnTB7wx2/YU8sjy0t5ZfUeWgMdnDU5h5sXFnDmpBxiNNUuIsOYQlwG157VzgYy656DQCuMXwQTFkHONOfaefoYOMl7w/c3tPD7D3fxxN93UlXfwoScZG5aOI4rTs0jKS60+8CLiEQChbiER0OVM9Ve8jjU7up+Py4FcqZ0h3rnY1pev8O9tb2DV9ft4ZHlpawtqyUtwcs188Zyw+l+8n1J7vx5RETCQCEu4dd4AKo2Q+Wm3o+HqrqPiU8LhvtUyJ3W/Zg66qjhbq1l1a6DPLy8lL+s34e1lvOnj+SWM8Yxt0C7wYnI0KcQl8h1qBqqNvUI983O68bq7mMS0p1APzzcU0b0Cvc9NU088fed/P7DXdQ2tTFjdBo3LxzHJaeMIt6rxisiMjQpxGXoaagKhvvm3o9NB7uPScjoHerBx6bYTF5cvYdHlu/g08oGslPi+OJ8P9edNpbcVLVEFZGhRSEuw4O1zvT74VPylZuguab7uMRMyJ2GzZnKdsbwh90pPL0zhXpPOpcUjubmheOYla/d4ERkaFCIy/BmLTRU9BHum6GltuuwQ94MNrSNZlMgjxbfZGbNOY25cxfgTc0OY/EiIsemEJfoZC3U7+0V7u0Vm+io2ERc4FDXYY1xWcSOnE7syOm9V8sPcF94EZFQOFqI66ZaGd6MgbTRztfEc4Hgv/TWEqgpY/WqD9i05iPiDmxhys5yppatJL6jqfvnk3MhezJkT+rxOMm5z32AW8uKiAyURuIiwKa9dTy6vJQXV5eR017J5/PrWZxXxwSzB1P9Kezf0ntBnTcBsiZ2h3tWMNyzJkK8toMVkdDSdLpIP1Q3tPDUR85ucBV1LWSnxHHO1FzOmzaCM/IMSbXbnUDfvwX2fwrVn8LBUrAd3R+Sln/kyD178jHvdRcRORaFuMgJaAt08NcNFfxlwz7e+qSS+uZ24r0xLJyYzXnTRnDutFxGpAVvVWtvgQOHhXvnV2t994fGpRw5cs+eDJnjIVa3vYnI0YUlxI0xFwD/A3iA31prf3zY96cCjwCnAt+z1t53vM9UiMtgawt0sGLHAV7fVMGyTRXsPuBcMy/MT+8K9Omj0o7cGc5aqN935Mh9/6dQu7v7OBMDGf4jR+7ZkyEpS6N3ERn8EDfGeIAtwGeAMmAFcK21dmOPY3IBP7AYOKgQl0hnreXTygZe3+gE+urdNVgLo9MTOG/6CM6bNoL54zOPvztc6yGo3hocsfcYvVd/Cu3N3ccl+o4cuWdPdtq9erQuVSRahCPETwfutdZ+Nvj6uwDW2h/1cey9QINCXIaaqvoW3txcyeubKnjv0/00tQVIjvNw1pQczps2gkVTcvElx/X/Azs6nFF616h9S3fQN1R0HxcT60zDHz5yz5o44B7uIhJ5wnGLWR7QY86QMmD+yXyQMeYrwFcAxo4dO/DKREIkJzWeL8wdwxfmjqG5LcD72/bz+sZK3thUwdJ1+4gxUOzP5NxpuZw3fQQTco6zcj0mBnx+52vSeb2/11QTHL1v6T2C3/IX6GjvPi5lZO973XOnO01lErRDnchw4+ZI/Crgs9baLwVfXw/Ms9be0cex96KRuAwjHR2W9XtqWbapkmUbK9i4tw6A8dnJTqBPG0GR34fXEzPwkwXa4OBOZ+Re9Unwa5Pz2NbYfVxaXvce87nTnJDPmaJb4kSGgHCMxMuAMT1e5wN7XDyfSMSIiTEU5mdQmJ/Btz4zmfKaJv62qYLXN1Xy6Pul/ObdHWQkxbJoihPoZ07OJjUh9uRO5omF7InO15QLu9/v6ICanUfuMb9iee/r7uljgyP34Kg9dypkT4E49WQXiXRujsS9OAvbzgXKcRa2fdFau6GPY+9FI3GJEg0t7by7pYrXN1Xw5uZKDja2EesxnDY+q2u1e77PxQDtCDj3tldu6tEdbrMzPR9oDR5knCn9nGk9Ru5Tnevuuh1OZNCF6xazzwE/w7nF7GFr7f81xtwOYK190BgzElgJpAEdQAMw3Vpbd7TPVIjLcNIe6GDVrhre2FTB65sq2F7l7Oc+dWQqnwmudp+Vl05MzCDcZhZod+5379X+dZNzHb7zmruJAd+43sGeO81ZUOeNd79GOb5AOwRanP0L2lt6P49Ldu5s0JbBQ442exEZArZXNfDGJme1+8rSA3RYZ/HcecHr6AsnZpMQO8h/Abe3woFtvafkKzc5gW8DzjHGA1kTek/J50xz3vOc5GWCocRaZ21Ce7Mzm9HeHAzQzuetR37v8IA91veOOPYY3+v8Z3I03kTnn8+IGTBipvOYOwOSswbndyUnRSEuMsQcPNTKW1sqWbaxkre3VNHQ0k5CbAxnTMzhM9NzWTQ1l9zUME5tt7c4q+N7BnvVJjiwAwj+vRIT64zSu1bJB0fuvnHu3OfeEXCCsK05GIjNh71ucupuCz62Nx3j2GO9bukR1C291xgMhIlx9uX3xoMn3nk8/PnxXh/re821ULERKtY7X43V3edOGRkM9h7hnj0ZvCdwi6S4RiEuMoS1tnfw4Y5qlm2sYNmmSsprnF3jZo/J4DPTnevoU0akHrlrXDi0NTnX1ys3Q+XG7pCv2dl9jCfeub+9M9QTM04yTA973tE2sNq9ic41f2+Pr6O+jncePXHB18HHPkP0WN/r8XwwN/CxFhoqoXIDVHR+rXfuauhcGxHjdYL88HBXH4BBpxAXGSastWzeVx8M9ArWlNUCkO9L5LxpIzh9QhaF+emMTEuIjFDv1HooePtbMNw7F9T13IK2kzfRCbbYxH6EaR+vY4M/3+tzjvPaE6dgAueyQPW24Gi9R8DXlXUfk5DRHeid4Z471bnmLq5QiIsMU5V1zbyx2bkf/b2t+2lpdzqqZafEU5ifzqy8dOcxPz280+9H01IPrY0K00jXdNCZUekcsVdscKbm2w4FDzCQOa73iH3EDMgocDYxigbWQkudc9kifUxI/z1WiItEgea2ABv31rGurJa1ZbWsK69ha2UDHcH/zEemJTArP53CPCfUC/MzyDyRbWFFeurci6DndHzlRmck37kuIjbZuWTSK9ynO30BIlFHwAniphporjnKY23f32uu7W5L/N0yiE8NWVkKcZEodailnY1765xQL6thbXlt161sAHkZiV0j9cK8DGblpZOeFAUrysU9rYecSyUVh11vbzrYfUxa3mHT8dOddRKhuJsh0H5Y0B7sXwg31ToBzjFyMSbWWcORkBF8TO/xvMfjrCudmaUQUYiLSJf65jbWl9exrrwmOGKvZWd19xat/qyk7mn4vAxm5qWd/I5yItDdmrfndHzlRmedROeCRE+cs1tgV7jPcMK+58i4ufY4o+QaaG04di3eBCdoE9KPDN/jBXRsUlgu9yjEReSYahvbWFdey9rymq7p+M5V8ADjc5KD0/AZFOanM2N0GklxaocqA9Te6uz73xXuG53n9cfZpTs26Rjhm3HsgB6Cuw4qxEXkhFU3tLCuvNYJ9fJa1pfXsrfWuSc6xsDE3BRm5WV0TcdPH5U2+JvRyPDUeMAJ84aKI0fECelRd/+6QlxEQqKyvpn15cGFc2W1rCmrZX9DCwCeGMPkEak9Fs6lM2VkKvFeBbvIQCjERcQV1loq6lpYW1bjTMeX1bK2rIaDjc51zliPYerItF6r4iePSCU2FG1YRaKEQlxEBo21lvKapq5p+HXBYK9rdhqpxHljmD4qres+9ln56UzMSQlNf3WRYUghLiJhZa1l14HGrtXwa8tqWF9eR0OLE+wJsTHMGN1jc5q8dMbnpOAZjA5uIhFOIS4iEaejw7J9/6Hua+zlNWzYU0djq9OJKynOw8zR3dfXZ+alMy4reXBas4pEEIW4iAwJgQ7L9qqGXiP2jXvraG5zdsJKifcyMy+NwvwMZuY519n9WUmRtU+8SIgdLcR1k6eIRBRPjGHSiFQmjUjliqJ8ANoDHWztDPbgdfZH3y+lNbhPfFqCl1nBkXph8Ja3fF+igl2GPY3ERWRIagt0sKWivtc97Jv21tEWcP5Oy0iKdRbNdTWAyWB0eoR1dhPpJ43ERWRYifU4C+FmjE7nmuB7Le0Btuxr6Np1bl15LQ+9s532YAeYrOQ4Z7Te1d0tgxFp8Qp2GbIU4iIybMR7PcwK7h7HfOe95rYAm/fVO81fgsH+y7f2EwgGe05q/GEj9ght2SrSB4W4iAxrCbEeZo/JYPaYjK73mlo7W7bWsC7YCOatTyq7WraOSIvvtZ3srLx0slPiw/MHEDkGhbiIRJ3EOA9Ffh9F/u6e1j1btq4Prop/Y3MFncuGRqcndPVgn5Xn7DqXkxqv+9glrBTiIiJAcryXuQWZzC3I7HqvvrmNDXvquq6vryuv5bUNFV3f98QYclPjGZmewKj0BEamJTqPwdejMhLJTY3XFrPiGoW4iMhRpCbEctr4LE4bn9X1Xm1TGxvKa9lRfYh9tc3srW1mX20zn+yr561Pqro2qulkDOSkxPcI98Qeoe+8zk2LV/c3OSkKcRGRE5CeGMuCidksmJh9xPestdS3tPcI96aukN9b20zp/kY+2FbdtYd8T1nJcd3h3hn2ad2vR6YnqH+7HEH/RoiIhIgxhrSEWNISYpk8IvWoxx1qae8R7k3OY53zurymmZKdB7u6wPWUnhjba7r+8On7kekJpCbEuvlHlAijEBcRGWTJ8V4m5qYwMTflqMc0twW6R/R1vUf0+2qbWV9e19XHvaeUeO9h0/UJjEx3wn50RiJjM5NIjNPU/XChEBcRiUAJsR4KspMpyE4+6jGt7R1U1DWzr67v6ftPK/ZTWd/cdetcp9zUeAqykhmblYQ/M8l5zErGn5lERlKsNr8ZQhTiIiJDVJw3hjGZSYzJTDrqMe2BDirrW9hb20x5TRO7qg9RWt3IrupG3v20iufreo/mUxO8vQLen5XE2Mxk/FlJjExLUAe5CKMQFxEZxryeGEZnJDI6I7HXffGdmloD7D7YSOn+Q+w60MjO6kZ2HmhkQ3ktr63f17VlLTj/0zA2s8foPdMZwY/NSmKML4k4r26lG2wKcRGRKJYY52HyiNQ+F+K1BzrYU9PMzgOH2FndGAx55/n726ppauu+nS7GwKj0RPxZvUfv/uBUfUq84sYN+q2KiEifvJ4YxmY5o+5/mNT7e9Zaqhpa2FXdPXrvnKp/bUMFBw619jo+KzmuxzX4ZAp6hH12Spyuw58khbiIiJwwYwy5qQnkpiZQ3GOXu071zW09Ru/dI/gVpQd5ec0eenbBTo7zMCaze9Tuz0rCHxzJj0pPwKsd745KIS4iIiGXmhDLzLx0ZualH/G9lvYAZQeb2FXdSGl191T91soG3txcRWugo+vYGAOZyfHkpDpfucHHnJTu9zq/UuO9UTeiV4iLiMigivd6mJCTwoScI++T7+iw7KtrprT6ELuqGymvaaKqvsX5amjh04p6qupbei246/7cmD7CPuGIsM9OiSPeOzzulVeIi4hIxIiJMV2r6RdM6PuYjg5LbVMbVQ0t3QFf30JlfXNX2O/Yf4iPdhzoc+c7cHa/6xzR56b1PbLPSYnHlxQX0bfVKcRFRGRIiYkx+JLj8CXHHXN7W3A2xKk+dHjYt/Qa3X+8q4bK+maa2zqO+HlPjCE7JS44uk84atjnpMaTHIYV+ApxEREZtuK8MYxKT2RUeuIxj7PWcqg10PfIPhj2FXXNrC+vZX9DyxG74AEkxXm6Qv1/b5xLepL7+9grxEVEJOoZY0iJ95IS72XcMba6BQh0WA42th51ZF9V30xS/OBcc1eIi4iInABnij2e7JR4po0Kby26+U5ERGSIUoiLiIgMUQpxERGRIUohLiIiMkQpxEVERIYohbiIiMgQpRAXEREZohTiIiIiQ5RCXEREZIhSiIuIiAxRCnEREZEhSiEuIiIyRCnERUREhihjbR9NUSOYMaYK2BnCj8wG9ofw8+To9LseHPo9Dw79ngeHfs8Ov7U25/A3h1yIh5oxZqW1tjjcdUQD/a4Hh37Pg0O/58Gh3/OxaTpdRERkiFKIi4iIDFEKcXgo3AVEEf2uB4d+z4NDv+fBod/zMUT9NXEREZGhSiNxERGRISqqQ9wYc4Ex5hNjzFZjzD3hrmc4MsaMMca8aYzZZIzZYIz5RrhrGs6MMR5jzMfGmD+Fu5bhzBiTYYx53hizOfjv9unhrmk4MsbcFfx7Y70x5iljTEK4a4o0URvixhgP8ABwITAduNYYMz28VQ1L7cA/WWunAacBX9Pv2VXfADaFu4go8D/AX6y1U4FT0O885IwxecCdQLG1dibgAa4Jb1WRJ2pDHJgHbLXWbrfWtgJPA5eFuaZhx1q711q7Kvi8Hucvu7zwVjU8GWPygYuA34a7luHMGJMGnAn8L4C1ttVaWxPWooYvL5BojPECScCeMNcTcaI5xPOA3T1el6FwcZUxpgCYA3wY5lKGq58B/wx0hLmO4W48UAU8Erx08VtjTHK4ixpurLXlwH3ALmAvUGut/Wt4q4o80Rzipo/3tFTfJcaYFOAPwDettXXhrme4McZcDFRaa0vCXUsU8AKnAr+y1s4BDgFaUxNixhgfzuzoOGA0kGyMuS68VUWeaA7xMmBMj9f5aKrGFcaYWJwAf9Ja+0K46xmmFgKXGmNKcS4NnWOM+V14Sxq2yoAya23njNLzOKEuoXUesMNaW2WtbQNeABaEuaaIE80hvgKYZIwZZ4yJw1kw8UqYaxp2jDEG59rhJmvtT8Jdz3Blrf2utTbfWluA8+/y36y1GrW4wFq7D9htjJkSfOtcYGMYSxqudgGnGWOSgn+PnIsWEB7BG+4CwsVa226M+TrwGs6qx4ettRvCXNZwtBC4HlhnjFkdfO9frLVLw1eSyIDdATwZHABsB24Ocz3DjrX2Q2PM88AqnLtcPka7tx1BO7aJiIgMUdE8nS4iIjKkKcRFRESGKIW4iIjIEKUQFxERGaIU4iIiIkOUQlwkyhhjAsaY1T2+QrbbmDGmwBizPlSfJyLHFrX3iYtEsSZr7exwFyEiA6eRuIgAYIwpNcb8pzHmo+DXxOD7fmPMG8aYtcHHscH3RxhjXjTGrAl+dW6J6THG/CbYB/qvxpjEsP2hRIY5hbhI9Ek8bDr96h7fq7PWzgN+gdMVjeDzx621hcCTwP3B9+8H3rbWnoKzd3jnjoeTgAestTOAGuAKV/80IlFMO7aJRBljTIO1NqWP90uBc6y124NNa/ZZa7OMMfuBUdbatuD7e6212caYKiDfWtvS4zMKgNettZOCr+8GYq21/z4IfzSRqKORuIj0ZI/y/GjH9KWlx/MAWnsj4hqFuIj0dHWPxw+Cz9/H6YwGsAR4L/j8DeCrAMYYjzEmbbCKFBGH/g9ZJPok9ugoB/AXa23nbWbxxpgPcf4H/9rge3cCDxtjvgNU0d2x6xvAQ8aYW3FG3F8F9rpdvIh00zVxEQG6rokXW2v3h7sWEekfTaeLiIgMURqJi4iIDFEaiYuIiAxRCnEREZEhSiEuIiIyRCnERUREhiiFuIiIyBClEBcRERmi/n9A7ZqHKqWbOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check out our train loss and test loss over epochs.\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Set figure size.\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Generate line plot of training, testing loss over epochs.\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(test_loss, label='Testing Loss')\n",
    "\n",
    "# Set title\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>If my model were to be underfit, what might I do?</summary>\n",
    "\n",
    "- Increase the learning rate.\n",
    "- Increase the number of layers.\n",
    "- Increase the number of nodes in each layer.\n",
    "- Decrease the batch size.\n",
    "- Increase the number of epochs.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0852 - accuracy: 0.9736\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test data.\n",
    "score = model.evaluate(X_test,\n",
    "                       y_test,\n",
    "                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0852\n",
      "accuracy: 0.9736\n"
     ]
    }
   ],
   "source": [
    "# Show model performance.\n",
    "labels = model.metrics_names\n",
    "print(f'{labels[0]}: {score[0]:.4f}')\n",
    "print(f'{labels[1]}: {score[1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "Convolutional neural networks are a great way to get around this issue of too many parameters. CNNs do some complicated math up front to \"compress our images,\" allowing us to learn far fewer parameters in later layers.\n",
    "\n",
    "A CNN will generally consist of three types of layers:\n",
    "- Convolutional Layer\n",
    "- Pooling Layer\n",
    "- Densely Connected Layer\n",
    "\n",
    "<img src=\"../images/cnn.jpeg\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "[Image source.](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)\n",
    "\n",
    "This isn't overly helpful if we're trying to learn what CNNs are/do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/pic1.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "<img src=\"../images/pic2.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "<img src=\"../images/pic3.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "<img src=\"../images/pic4.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "[Image by 3Blue1Brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's fit a convolutional neural network to this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a CNN model.\n",
    "cnn_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a convolutional layer.\n",
    "cnn_model.add(Conv2D(filters=20,          # number of filters\n",
    "                     kernel_size=(3, 3),  # height/width of filter\n",
    "                     padding='same',      # Ensure Output is same shape as Input\n",
    "                     activation='relu',   # activation function \n",
    "                     input_shape=(28, 28, 1))) # shape of input (image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to specify the input shape in our first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.add(MaxPooling2D(pool_size=(2, 2))) # dimensions of region of pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add another Conv2D and pooling layers\n",
    "cnn_model.add(Conv2D(filters=10,\n",
    "                     kernel_size = (3, 3),\n",
    "                     padding='same',\n",
    "                     activation='relu'))\n",
    "\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2, 2))) # dimensions of region of pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten to a single vector\n",
    "cnn_model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 20)        200       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 20)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 10)        1810      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 10)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 490)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               62848     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,148\n",
      "Trainable params: 66,148\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Add some fully connected layers\n",
    "cnn_model.add(Dense(128, activation='relu'))\n",
    "cnn_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Check out the model summary.\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model.\n",
    "opt = Adam(learning_rate=0.001)\n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "235/235 [==============================] - 22s 89ms/step - loss: 0.4885 - accuracy: 0.8625 - val_loss: 0.1450 - val_accuracy: 0.9583\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 21s 88ms/step - loss: 0.1209 - accuracy: 0.9636 - val_loss: 0.0854 - val_accuracy: 0.9745\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 21s 89ms/step - loss: 0.0831 - accuracy: 0.9740 - val_loss: 0.0611 - val_accuracy: 0.9810\n",
      "Epoch 4/10\n",
      " 72/235 [========>.....................] - ETA: 13s - loss: 0.0677 - accuracy: 0.9783"
     ]
    }
   ],
   "source": [
    "# Fit model on training data\n",
    "history = cnn_model.fit(X_train,\n",
    "                        y_train,\n",
    "                        batch_size=256,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out our train loss and test loss over epochs.\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "epoch_labels = history.epoch\n",
    "\n",
    "# Set figure size.\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Generate line plot of training, testing loss over epochs.\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(test_loss, label='Testing Loss')\n",
    "\n",
    "# Set title\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data.\n",
    "cnn_score = cnn_model.evaluate(X_test,\n",
    "                               y_test,\n",
    "                               verbose=1)\n",
    "\n",
    "cnn_labels = cnn_model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare CNN and FFNN models.\n",
    "print(f'CNN {cnn_labels[0]}  : {cnn_score[0]}')\n",
    "print(f'CNN {cnn_labels[1]}   : {cnn_score[1]}')\n",
    "print()\n",
    "print(f'FFNN {labels[0]} : {score[0]}')\n",
    "print(f'FFNN {labels[1]}  : {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layer\n",
    "\n",
    "The convolution layer is where we pass a filter over an image and do some calculation at each step. Specifically, we take pixels that are close to one another, then summarize them with one number. The goal of the convolution layer is to identify important features in our images, like edges.\n",
    "\n",
    "<img src=\"../images/convolution.gif\" alt=\"drawing\" width=\"550\"/>\n",
    "\n",
    "[Image source.](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)\n",
    "\n",
    "Our hyperparameters here are:\n",
    "- the number of filters to use. This is given by `filters = 20`.\n",
    "- the dimensions of the filter. This is given by `kernel_size = (3, 3)`. a 3 x 3 filter\n",
    "\n",
    "**How many filters should we use?** Well, this is a hyperparameter. There's not one great answer, but the idea is that each filter can detect one type of feature in an image (like vertical edges). This may depend on the complexity of your images (simpler images require fewer filters) and usually requires trial and error to identify an adequate value of `filters`. [Source](https://stats.stackexchange.com/questions/196646/what-is-the-significance-of-the-number-of-convolution-filters-in-a-convolutional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding\n",
    "\n",
    "When we pass a filter over an image, each of the \"inside\" pixels is counted pretty frequently and thus gets \"represented\" more in the final model output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How many times does each corner get included in the \"output?\"</summary>\n",
    "\n",
    "- Right now, each corner gets included only once.\n",
    "</details>\n",
    "\n",
    "We can use **padding** to add a border of white cells around the edge of the image. This will allow pixels on the edge/in the corner to be included more frequently. (This might be good when doing computer vision for self-driving vehicles!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>In this MNIST digits case, do you think padding is a good idea or a bad idea?</summary>\n",
    "\n",
    "- Padding is probably a bad idea here. We're increasing the number of parameters we need to learn, but it's unlikely that we're getting important data from the corners/edges of the image. \n",
    "</details>\n",
    "\n",
    "[Let's visualize what the convolution operation looks like](https://ezyang.github.io/convolution-visualizer/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layer\n",
    "\n",
    "Remember that CNNs learn far fewer parameters than a regular feed-forward neural network. Most of the \"parameter reduction\" comes from the pooling layer.\n",
    "\n",
    "<img src=\"../images/maxpool.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "[Image source.](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)\n",
    "\n",
    "In Max Pooling, we pass a filter over an image. At each step, we take the maximum value and record it as part of the output.\n",
    "- Average Pooling exists, but is far less frequently used. [Andrew Ng](https://www.deeplearning.ai/deep-learning-specialization/) recommends using Max Pooling.\n",
    "- When pooling, we generally partition the result from the previous layer. That is, the filter does not usually overlap like it does in the convolutional layer.\n",
    "\n",
    "Our hyperparameters here are the **dimensions of the filter we use when pooling**. This is given by `pool_size = (2, 2)` in our example.\n",
    "\n",
    "##### Why use max pooling?\n",
    "1. Reduces the data dimensionality.\n",
    "2. Protects against overfitting by creating a more abstract representation.\n",
    "3. Provides some invariance by ignoring insignificant local changes in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Densely-Connected Layer\n",
    "The densely-connected layer is the exact same as in a normal feed-forward neural network, so we won't spend any time talking about that, **except: remember to pass a `Flatten()` layer before a `Dense()` layer!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "- Neural Network models have the capability of \"transfering\" knowledge from one model to another\n",
    "- For example, we can train a `parent` model on a very large dataset like Imagenet to identify 1000 real world objects.\n",
    "- This \"knowledge\" that this `parent` model has learned, can then be \"transfered\" to another `child` model to identify between dogs and cats, even though dogs and cats may not be part of the original 1000 real world objects the `parent` model was trained on!\n",
    "- The reason why this works is because real world object have many similar features. For example, eyes, ears, face, hands, legs, etc are common in many animals. Once the `parent` model learns how to identify an eye or a leg, it can do so even on animals that it has not seen before!\n",
    "\n",
    "\n",
    "- Tensorflow Keras provides a lot of models pretrained on the Imagenet dataset: [Link](https://www.tensorflow.org/api_docs/python/tf/keras/applications)\n",
    "- Some of the popular ones are `efficientnet` family, `mobilenet_v2` and `resnet50`\n",
    "- Let's see how we can use one of these \"pre-trained\" models in an image classification model\n",
    "- Dataset is from here: [Link](https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip). Download and unzip the file into the data directory\n",
    "- For more information on how to choose a specific pre-trained model take a look at this Medium post: [Link](https://medium.com/towards-data-science/how-to-choose-the-best-keras-pre-trained-model-for-image-classification-b850ca4428d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Train and Validation data\n",
    "train_data = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory('../data/cats_and_dogs_filtered/train/', \n",
    "                                                                                             target_size=(224,224), \n",
    "                                                                                             class_mode='binary'\n",
    "                                                                                            )\n",
    "\n",
    "val_data = ImageDataGenerator(preprocessing_function=preprocess_input).flow_from_directory('../data/cats_and_dogs_filtered/validation/', \n",
    "                                                                                           target_size=(224,224), \n",
    "                                                                                           class_mode='binary'\n",
    "                                                                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the output mappings\n",
    "class_mappings = train_data.class_indices\n",
    "print(class_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the desired pre-trained model\n",
    "# List of pre-trained models: https://www.tensorflow.org/api_docs/python/tf/keras/applications\n",
    "pre_trained_model = MobileNetV2(include_top=False, pooling='avg')\n",
    "\n",
    "# Freeze the model so we don't accidentally change the pre-trained model\n",
    "pre_trained_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our model architecture\n",
    "trf_model = Sequential()\n",
    "\n",
    "# Then add the pre-trained model to use Transfer Learning\n",
    "trf_model.add(pre_trained_model)\n",
    "\n",
    "# Finally add our custom modifications\n",
    "trf_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "trf_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "opt = Adam(learning_rate=0.001)\n",
    "trf_model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on training data\n",
    "history = trf_model.fit(train_data, \n",
    "                        batch_size=64,\n",
    "                        validation_data=val_data,\n",
    "                        epochs=5,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trf_model.save('cats_vs_dogs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions on the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "trf_model = load_model('cats_vs_dogs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image into Python\n",
    "test_image = image.load_img('../images/test.jpeg')\n",
    "\n",
    "# Convert the image to a matrix of numbers\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "test_image = preprocess_input(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "result = trf_model.predict(test_image)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the probability to actual predictions\n",
    "['Dog' if pred[0]>0.5 else 'Cat' for pred in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Convolutional neural networks are uniquely suited to tackle image data.\n",
    "- Dealing with images usually presents high-dimensional challenges. (A 28x28 image is a pretty low-resolution image.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Why are convolutional neural networks better equipped to handle image data than non-CNNs?\n",
    "</summary>\n",
    "\n",
    "- CNNs are naturally set up to consider interactions among \"close pixels\" only and can drastically cuts down the number of parameters needed to learn. (Or get better performance for a given number of parameters!)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Can you think of other situations (i.e. not images) in which we might apply a convolutional neural network?</summary>\n",
    "\n",
    "- **Videos**. A video is really just a sequence of pictures, so we might use a 3D convolutional neural network. (Length of the picture, width of the picture, and depth of the picture is time.)\n",
    "- **Time series data**. Rather than passing a filter over neighboring pixels in pictures, what if we passed a filter over neighboring time periods in time series data?\n",
    "- **Natural language data**. Rather than passing a filter over neighboring pixels in pictures, what if we passed a filter over neighboring words or tokens in natural language data?\n",
    "- Convolutional neural networks exploit the inherent structure in data we pass in.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transfer learning is a really fantastic way to use knowledge from models trained by other companies and researchers for our own projects\n",
    "- Many of you are using Huggingface for your capstone projects. That is using Transfer learning for NLP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
