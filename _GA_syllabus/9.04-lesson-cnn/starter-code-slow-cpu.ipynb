{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Convolutional Neural Networks\n",
    "\n",
    "\n",
    "\n",
    "**By the end of the lesson, you should be able to:**\n",
    "- Identify use cases for convolutional neural networks and when they are superior to other neural networks.\n",
    "- Describe convolutional and pooling layers.\n",
    "- Define padding and filters.\n",
    "- Fit CNNs in Keras.\n",
    "- Understand how edge detection works in CNNs. _(optional)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks are generally used when we are dealing with image data.\n",
    "\n",
    "Their main advantage over densely connected neural networks is **efficiency**.\n",
    "\n",
    "In order to illustrate this, let's build out a feedforward neural network and tackle the MNIST Handwritten Digits Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.datasets import mnist\n",
    " \n",
    "# Load pre-shuffled MNIST data into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check one value of X_train.\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape of an image.\n",
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We want to scale our data to be betwen 0 and 1.\n",
    "\n",
    "When working with image data, we commonly scale it to be between 0 and 1. This is a common choice if we are pulling images from various sources that are on a different scale, and can improve speed by keeping values close to 0. Depending on the type of computation you want to do, having pixel values represented with `255` might not be ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure each value is a float. (Otherwise, we get an error.)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# The current range of X_train and X_test is 0 to 255.\n",
    "# The code below is equivalent to X_train = X_train / 255.\n",
    "# This scales each value to be between 0 and 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/picture.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "[Image by 3Blue1Brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape of full training data.\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We want to reshape each image to be 28 x 28 x 1.\n",
    "\n",
    "This allows our neural network to know that there's only one value (how dark the pixel is) instead of multiple values per pixel. For example, a color image stored in RGB (red/green/blue) might have shape 28 x 28 x 3, where one value is corresponds to how red the pixel is, one value corresponds to how blue the pixel is, and one value corresponds to how green the pixel is.\n",
    "- Reshaping your images can be a hard part of pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape each image to be 28 x 28 x 1.\n",
    "# The 1 corresponds to the one black/white value.\n",
    "# If we had a color image, we would likely use 3 for RGB.\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check out `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What change do I need to make to y_train? Why?</summary>\n",
    "\n",
    "- Right now, the values of `y_train` will be interpreted as a number. Our neural network would try to predict values that are numerically close to the true value. (i.e. If $Y = 5$, then $\\hat{Y} = 4$ would be way better than $\\hat{Y} = 1$. **This isn't actually what we want!**\n",
    "- I need to convert it to a categorical variable.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change y_train.\n",
    "y_train =\n",
    "y_test ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check y_train.\n",
    "y_train[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's fit a feedforward neural network to this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When instantiating our model, what do we first write?\n",
    "\n",
    "model ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/flattening_image.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "[Image source.](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/73_blog_image_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to \"align\" our neurons in a vertical array, we\n",
    "# add a \"Flatten\" layer. This will be required before adding\n",
    "# subsequent Dense layers. We need to specify the input shape.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/picture.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "<img src=\"./images/network.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "[Images by 3Blue1Brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a fully-connected, feed-forward neural net with:\n",
    "# - A 128-node hidden layer.\n",
    "# - A 32-node hidden layer.\n",
    "# - The appropriate output layer.\n",
    "\n",
    "model.add()\n",
    "model.add()\n",
    "model.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on training data.\n",
    "history = model.fit(X_train,\n",
    "                    y_train, \n",
    "                    batch_size=256,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=5,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out our train loss and test loss over epochs.\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Set figure size.\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Generate line plot of training, testing loss over epochs.\n",
    "plt.plot(train_loss, label='Training Loss', color='#185fad')\n",
    "plt.plot(test_loss, label='Testing Loss', color='orange')\n",
    "\n",
    "# Set title\n",
    "plt.title('Training and Testing Loss by Epoch', fontsize = 25)\n",
    "plt.xlabel('Epoch', fontsize = 18)\n",
    "plt.ylabel('Categorical Crossentropy', fontsize = 18)\n",
    "plt.xticks(np.arange(5), np.arange(5))\n",
    "\n",
    "plt.legend(fontsize = 18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>If my model were to be underfit, what might I do?</summary>\n",
    "\n",
    "- I could try decreasing the batch size.\n",
    "- I could try increasing the number of epochs.\n",
    "- I could try increasing the number of layers.\n",
    "- I could try increasing the number of nodes in each layer.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data.\n",
    "score = model.evaluate(X_test,\n",
    "                       y_test,\n",
    "                       verbose=1)\n",
    "\n",
    "labels = model.metrics_names\n",
    "\n",
    "# Show model performance.\n",
    "print(f'{labels[0]}: {score[0]}')\n",
    "print(f'{labels[1]}: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How many parameters are being fit in this model?</summary>\n",
    "\n",
    "- In our input layer, we have $28 * 28 = 784$ neurons.\n",
    "- In our first hidden layer, we have $128$ neurons.\n",
    "- In our second hidden layer, we have $32$ neurons.\n",
    "- In our output layer, we have $10$ neurons.\n",
    "- There is one bias value for each neuron in every hidden layer and output layer, which is $128 + 32 + 10 = 170$ bias parameters.\n",
    "- There is one weight value connecting each node from the input to first hidden layer, which is $784 * 128 = 100,352$ weight parameters.\n",
    "- There is one weight value connecting each node from the first hidden layer to the second hidden layer, which is $128 * 32 = 4,096$ weight parameters.\n",
    "- There is one weight value connecting each node from the second hidden layer to the output layer, which is $32 * 10 = 320$ weight parameters.\n",
    "- Adding these up, we get $170 + 100,352 + 4,096 + 320 \\approx 105,000$ parameters.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the model summary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What are some consequences of fitting a model with too many parameters?</summary>\n",
    "\n",
    "- Easy to overfit our model.\n",
    "- Learning is quite slow.\n",
    "- We need more data in order to meaningfully learn and fit a model!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "Convolutional neural networks are a great way to get around this issue of too many parameters. CNNs do some complicated math up front to \"compress our images,\" allowing us to learn far fewer parameters in later layers.\n",
    "\n",
    "A CNN will generally consist of three types of layers:\n",
    "- Convolutional Layer\n",
    "- Pooling Layer\n",
    "- Densely Connected Layer\n",
    "\n",
    "<img src=\"./images/cnn.jpeg\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "[Image source.](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)\n",
    "\n",
    "This isn't overly helpful if we're trying to learn what CNNs are/do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's fit a convolutional neural network to this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a CNN.\n",
    "cnn_model ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a convolutional layer.\n",
    "\n",
    "cnn_model.add(  # number of filters\n",
    "                # height/width of filter\n",
    "                # activation function \n",
    "              ) # shape of input (image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to specify the input shape in our first cell, just like we had to do earlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.add() # dimensions of region of pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.add(Conv2D(,\n",
    "                     kernel_size=,\n",
    "                     activation=))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.add(MaxPooling2D(pool_size=))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/pic1.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "<img src=\"./images/pic2.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "<img src=\"./images/pic3.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "<img src=\"./images/pic4.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "[Image by 3Blue1Brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a densely-connected layer with 128 neurons.\n",
    "cnn_model.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a final layer with 10 neurons.\n",
    "cnn_model.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "cnn_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on training data\n",
    "history = cnn_model.fit(X_train,\n",
    "                        y_train,\n",
    "                        batch_size=256,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        epochs=5,\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the model summary.\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out our train loss and test loss over epochs.\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "epoch_labels = history.epoch\n",
    "\n",
    "# Set figure size.\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Generate line plot of training, testing loss over epochs.\n",
    "plt.plot(train_loss, label='Training Loss', color='#185fad')\n",
    "plt.plot(test_loss, label='Testing Loss', color='orange')\n",
    "\n",
    "# Set title\n",
    "plt.title('Training and Testing Loss by Epoch', fontsize=25)\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.ylabel('Categorical Crossentropy', fontsize=18)\n",
    "plt.xticks(epoch_labels, epoch_labels)    # ticks, labels\n",
    "\n",
    "plt.legend(fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data.\n",
    "cnn_score = cnn_model.evaluate(X_test,\n",
    "                               y_test,\n",
    "                               verbose=1)\n",
    "\n",
    "cnn_labels = cnn_model.metrics_names\n",
    "\n",
    "# Compare CNN and FFNN models.\n",
    "print(f'CNN {cnn_labels[0]}  : {cnn_score[0]}')\n",
    "print(f'CNN {cnn_labels[1]}   : {cnn_score[1]}')\n",
    "print()\n",
    "print(f'FFNN {labels[0]} : {score[0]}')\n",
    "print(f'FFNN {labels[1]}  : {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layer\n",
    "\n",
    "The convolution layer is where we pass a filter over an image and do some calculation at each step. Specifically, we take pixels that are close to one another, then summarize them with one number. The goal of the convolution layer is to identify important features in our images, like edges.\n",
    "\n",
    "<img src=\"./images/convolution.gif\" alt=\"drawing\" width=\"550\"/>\n",
    "\n",
    "[Image source.](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)\n",
    "\n",
    "Our hyperparameters here are:\n",
    "- the number of filters to use. This is given by `filters = 6` in our example: six filters.\n",
    "- the dimensions of the filter. This is given by `kernel_size = (3, 3)` in our example: a 3x3 filter.\n",
    "\n",
    "**How many filters should we use?** Well, this is a hyperparameter. There's not one great answer, but the idea is that each filter can detect one type of feature in an image (like vertical edges). This may depend on the complexity of your images (simpler images require fewer filters) and usually requires trial and error to identify an adequate value of `filters`. [Source](https://stats.stackexchange.com/questions/196646/what-is-the-significance-of-the-number-of-convolution-filters-in-a-convolutional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding\n",
    "\n",
    "When we pass a filter over an image, each of the \"inside\" pixels is counted pretty frequently and thus gets \"represented\" more in the final model output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How many times does each corner get included in the \"output?\"</summary>\n",
    "\n",
    "- Right now, each corner gets included only once.\n",
    "</details>\n",
    "\n",
    "We can use **padding** to add a border of white cells around the edge of the image. This will allow pixels on the edge/in the corner to be included more frequently. (This might be good when doing computer vision for self-driving vehicles!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>In this MNIST digits case, do you think padding is a good idea or a bad idea?</summary>\n",
    "\n",
    "- Padding is probably a bad idea here. We're increasing the number of parameters we need to learn, but it's unlikely that we're getting important data from the corners/edges of the image. \n",
    "</details>\n",
    "\n",
    "[Let's visualize what the convolution operation looks like](https://ezyang.github.io/convolution-visualizer/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layer\n",
    "\n",
    "Remember that CNNs learn far fewer parameters than a regular feed-forward neural network. Most of the \"parameter reduction\" comes from the pooling layer.\n",
    "\n",
    "<img src=\"./images/maxpool.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "[Image source.](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)\n",
    "\n",
    "In Max Pooling, we pass a filter over an image. At each step, we take the maximum value and record it as part of the output.\n",
    "- Average Pooling exists, but is far less frequently used. [Andrew Ng](https://www.deeplearning.ai/deep-learning-specialization/) recommends using Max Pooling.\n",
    "- When pooling, we generally partition the result from the previous layer. That is, the filter does not usually overlap like it does in the convolutional layer.\n",
    "\n",
    "Our hyperparameters here are the **dimensions of the filter we use when pooling**. This is given by `pool_size = (2, 2)` in our example.\n",
    "\n",
    "##### Why use max pooling?\n",
    "1. Reduces the data dimensionality.\n",
    "2. Protects against overfitting by creating a more abstract representation.\n",
    "3. Provides some invariance by ignoring insignificant local changes in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Densely-Connected Layer\n",
    "The densely-connected layer is the exact same as in a normal feed-forward neural network, so we won't spend any time talking about that, **except: remember to pass a `Flatten()` layer before a `Dense()` layer!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a CNN.\n",
    "cnn_model_2 = Sequential()\n",
    "\n",
    "# Add a convolutional layer.\n",
    "cnn_model_2.add(Conv2D(filters=16,             # number of filters\n",
    "                       kernel_size=(3,3),      # height/width of filter\n",
    "                       activation='relu',      # activation function \n",
    "                       input_shape=(28,28,1))) # shape of input (image)\n",
    "\n",
    "# Add a pooling layer.\n",
    "cnn_model_2.add(MaxPooling2D(pool_size=(2,2))) # dimensions of region of pooling\n",
    "\n",
    "# Add another convolutional layer.\n",
    "cnn_model_2.add(Conv2D(64,\n",
    "                       kernel_size=(3,3),\n",
    "                       activation='relu'))\n",
    "\n",
    "# Add another pooling layer.\n",
    "cnn_model_2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# We have to remember to flatten to go from the \"box\" to the vertical line of nodes!\n",
    "cnn_model_2.add(Flatten())\n",
    "\n",
    "# Add a densely-connected layer with 64 neurons.\n",
    "cnn_model_2.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Let's try to avoid overfitting!\n",
    "cnn_model_2.add(Dropout(0.5))\n",
    "\n",
    "# Add a densely-connected layer with 32 neurons.\n",
    "cnn_model_2.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Let's try to avoid overfitting!\n",
    "cnn_model_2.add(Dropout(0.5))\n",
    "\n",
    "# Add a final layer with 10 neurons.\n",
    "cnn_model_2.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "cnn_model_2.compile(loss='categorical_crossentropy',\n",
    "                    optimizer='adam',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# Fit model on training data\n",
    "history = cnn_model_2.fit(X_train,\n",
    "                          y_train,\n",
    "                          batch_size=128,\n",
    "                          validation_data=(X_test, y_test),\n",
    "                          epochs=5,\n",
    "                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out our train loss and test loss over epochs.\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "epoch_labels = history.epoch\n",
    "\n",
    "# Set figure size.\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Generate line plot of training, testing loss over epochs.\n",
    "plt.plot(train_loss, label='Training Loss', color='#185fad')\n",
    "plt.plot(test_loss, label='Testing Loss', color='orange')\n",
    "\n",
    "# Set title\n",
    "plt.title('Training and Testing Loss by Epoch', fontsize=25)\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.ylabel('Categorical Crossentropy', fontsize=18)\n",
    "plt.xticks(epoch_labels, epoch_labels)    # ticks, labels\n",
    "\n",
    "plt.legend(fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data.\n",
    "cnn_2_score = cnn_model_2.evaluate(X_test,\n",
    "                                   y_test,\n",
    "                                   verbose=1)\n",
    "\n",
    "cnn_2_labels = cnn_model_2.metrics_names\n",
    "\n",
    "# Compare models.\n",
    "print(f'CNN 2 {cnn_2_labels[0]}  : {cnn_2_score[0]}')\n",
    "print(f'CNN 2 {cnn_2_labels[1]}   : {cnn_2_score[1]}')\n",
    "print()\n",
    "print(f'CNN 1 {cnn_labels[0]}  : {cnn_score[0]}')\n",
    "print(f'CNN 1 {cnn_labels[1]}   : {cnn_score[1]}')\n",
    "print()\n",
    "print(f'FFNN {labels[0]} : {score[0]}')\n",
    "print(f'FFNN {labels[1]}  : {score[1]}')\n",
    "\n",
    "cnn_model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Convolutional neural networks are uniquely suited to tackle image data.\n",
    "- Dealing with images usually presents high-dimensional challenges. (A 28x28 image is a pretty low-resolution image.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Why are convolutional neural networks better equipped to handle image data than non-CNNs?\n",
    "</summary>\n",
    "\n",
    "- CNNs are naturally set up to consider interactions among \"close pixels\" only and can drastically cuts down the number of parameters needed to learn. (Or get better performance for a given number of parameters!)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Can you think of other situations (i.e. not images) in which we might apply a convolutional neural network?</summary>\n",
    "\n",
    "- **Videos**. A video is really just a sequence of pictures, so we might use a 3D convolutional neural network. (Length of the picture, width of the picture, and depth of the picture is time.)\n",
    "- **Time series data**. Rather than passing a filter over neighboring pixels in pictures, what if we passed a filter over neighboring time periods in time series data?\n",
    "- **Natural language data**. Rather than passing a filter over neighboring pixels in pictures, what if we passed a filter over neighboring words or tokens in natural language data?\n",
    "- Convolutional neural networks exploit the inherent structure in data we pass in.\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
