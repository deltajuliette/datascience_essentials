{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Hyperparameters, GridSearch, and Pipelines\n",
    "\n",
    "_Authors: Kiefer Katovich, David Yerrington, Matt Brems, Noelle Brown_\n",
    "\n",
    "---\n",
    "\n",
    "![](https://snag.gy/aYcCt2.jpg)\n",
    "\n",
    "### Learning Objectives\n",
    "- Describe what the terms hyperparameters, GridSearch, and pipeline mean.\n",
    "- Apply `sklearn`'s `GridSearchCV` object.\n",
    "- Use attributes of the GridSearch object.\n",
    "- Describe the pitfalls of searching large hyperparameter spaces.\n",
    "- Build pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data.\n",
    "un_data = pd.read_csv('./data/UNdata.csv')\n",
    "\n",
    "# Examine first five rows.\n",
    "un_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## United Nations Data\n",
    "\n",
    "- `country`: the name of the nation\n",
    "- `region`: the region of the world (Africa, America, Asia, Europe, Oceania)\n",
    "- `lifeMale`: the life expectancy of males\n",
    "- `lifeFemale`: the life expectancy of females\n",
    "- `infantMortality`: the infant mortality rate (generally reported per 1,000 live births)\n",
    "- `GDPperCapita`: the Gross Domestic Product per person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set country to be the index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy region.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What is our reference category for this dummy variable?</summary>\n",
    "\n",
    "- Africa!\n",
    "- There is no dummy variable for Africa in our data, meaning that all dummy variables would be interpreted **relative to Africa**.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create $Y$ variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column with 1 if the female life expectancy is greater\n",
    "# than the male life expectancy.\n",
    "un_data['females_are_strong_as_hell'] =\n",
    "\n",
    "# The column name is a reference to the \n",
    "# Netflix series \"The Unbreakable Kimmy Schmidt.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What should we check next?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Do you have any concerns about the above?</summary>\n",
    "    \n",
    "- Our classes are severely unbalanced.\n",
    "- We should check out our tools for handling unbalanced classes. (e.g. moving our classification threshold, implement stratified $k$-fold cross-validation)\n",
    "- Given the relatively low sample size and the small number of the observations in the minority category here, it is unlikely that our model would be able to predict that a nation has a higher male life expectancy.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a column with 1 if the female life expectancy is 5\n",
    "# or more years longer than the male life expectancy.\n",
    "un_data['females_are_strong_as_hell'] = (un_data['lifeFemale'] >= (un_data['lifeMale'] + 5)).astype(int)\n",
    "\n",
    "# Check the thing we need to check!\n",
    "un_data['females_are_strong_as_hell'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We are interested in predicting whether or not the female life expectancy of a nation is at least five years greater than the male life expectancy.** This is a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up X and y.\n",
    "X = un_data.drop(['females_are_strong_as_hell', 'lifeMale', 'lifeFemale'], axis = 'columns')\n",
    "y = un_data['females_are_strong_as_hell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data into training and testing sets.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.33,\n",
    "                                                    random_state = 42,\n",
    "                                                    stratify = y) # Note the stratify argument here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Before we fit a k-Nearest Neighbors model, what do we need to do? Why?</summary>\n",
    "    \n",
    "- Standardize our data!\n",
    "- If we *don't* standardize our data, then features that have larger spreads (e.g. higher ranges or higher standard deviations) will have a disproportionate influence on our model.\n",
    "- If all of your variables are already on the same scale, then scaling is not necessary.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate.\n",
    "\n",
    "\n",
    "# Fit and transform.\n",
    "\n",
    "\n",
    "# Transform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Default kNN\n",
    "\n",
    "Below we fit a default `KNeighborsClassifier` to predict `y`. ([Here is the documentation.](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What is the default number of neighbors used in kNN?</summary>\n",
    "    \n",
    "- 5.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate.\n",
    "knn = \n",
    "\n",
    "# Fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What score is this?</summary>\n",
    "\n",
    "- Accuracy.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate against the baseline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Is selecting k = 5 a good choice? Is it the best choice?</summary>\n",
    "\n",
    "- We don't know!\n",
    "- $k$ is a hyperparameter.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are \"hyperparameters?\"\n",
    "\n",
    "Models often have built-in quantities that we can use to fine-tune our results. \n",
    "- What value of $k$ do we select?\n",
    "- What distance metric do we select?\n",
    "- Do we use LASSO or Ridge regularization?\n",
    "- What value of $\\alpha$ or $C$ do we use?\n",
    "\n",
    "These are quantities our model **cannot** learn... **we must decide on these ourselves**!\n",
    "\n",
    "> These are different from statistical parameters, which are quantities a model _can_ learn.\n",
    "\n",
    "However, different values for hyperparameters can result in substantially different models. \n",
    "- Let's [visualize fits for different values of $k$](http://scott.fortmann-roe.com/docs/BiasVariance.html) in $k$-nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>We want to find the optimal values for our hyperparameters. How do you think we might do this?</summary>\n",
    "\n",
    "- Try many different values of hyperparameters and see which ones perform the best on our data.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for the Best Hyperparameters\n",
    "\n",
    "Our default kNN performs quite poorly on the test data. But what if we changed the number of neighbors? The weighting? The distance metric?\n",
    "\n",
    "These are all hyperparameters of kNN. How would we do this manually? We would need to evaluate on the training data the set of hyperparameters that perform best, and then use this set of hyperparameters to fit the final model and score on the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One method of searching for the optimal set of hyperparameters is called GridSearching.**\n",
    "\n",
    "GridSearching gets its name from the fact that we are searching over a \"grid\" of hyperparameters. For example, imagine the `n_neighbors` hyperparameters as the columns and `weights` as the rows. This makes a grid. We check the accuracy for all combinations of hyperparameters on the grid.\n",
    "\n",
    "![](./images/grid.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `GridSearchCV`\n",
    "\n",
    "This would be an annoying process to have to do manually. Luckily `sklearn` comes in handy:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "```\n",
    "\n",
    "The `GridSearchCV` has a handful of important arguments:\n",
    "\n",
    "| Argument | Description |\n",
    "| --- | ---|\n",
    "| **`estimator`** | The sklearn instance of the model to fit on |\n",
    "| **`param_grid`** | A dictionary where keys are hyperparameters for the model and values are lists of values to test |\n",
    "| **`cv`** | The number of internal cross-validation folds to run for each set of hyperparameters |\n",
    "| **`n_jobs`** | How many cores to use on your computer to run the folds (-1 means use all cores) |\n",
    "| **`verbose`** | How much output to display (0 is none, 1 is limited, 2 is printouts for every internal fit) |\n",
    "\n",
    "\n",
    "Below is an example for how one might set up the GridSearch for our kNN:\n",
    "\n",
    "```python\n",
    "knn_parameters = {\n",
    "    'n_neighbors':[2,3],\n",
    "    'weights':['uniform','distance'],\n",
    "    'p':[1,2]\n",
    "}\n",
    "\n",
    "knn_gridsearcher = GridSearchCV(KNeighborsClassifier(), knn_parameters, verbose=1)\n",
    "knn_gridsearcher.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "**Try out the `sklearn` GridSearch below on the training data.** [You can find the GridSearchCV documentation here.](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of hyperparameters.\n",
    "# The keys MUST match the names of the arguments!\n",
    "knn_params = {\n",
    "    'n_neighbors': range(1, 51, 10),\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our GridSearchCV object.\n",
    "knn_gridsearch = GridSearchCV(, # What is the model we want to fit?\n",
    "                              , # What is the dictionary of hyperparameters?\n",
    "                              , # What number of folds in CV will we use?\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the GridSearchCV object to the data\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the Results of the GridSearch\n",
    "\n",
    "Once the GridSearch has fit (this can take awhile!) we can pull out a variety of information and useful objects from the GridSearch object, stored as attributes:\n",
    "\n",
    "| Property | Description |\n",
    "| --- | ---|\n",
    "| **`results.param_grid`** | Displays hyperparameters searched over. |\n",
    "| **`results.best_score_`** | Best mean cross-validated score achieved. |\n",
    "| **`results.best_estimator_`** | Reference to model with best score.  Is usable / callable. |\n",
    "| **`results.best_params_`** | The hyperparameters that have been found to perform with the best score. |\n",
    "| **`results.grid_scores_`** | Display score attributes with corresponding hyperparameters. | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print out the score.\n",
    "# from documentation: Mean cross-validated score of the best_estimator\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the set of hyperparameters that achieved the best score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best fit model on the test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's see everything!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(knn_gridsearch.cv_results_).sort_values('rank_test_score').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_df = pd.DataFrame(knn_gridsearch.cv_results_)\n",
    "gs_df = gs_df[gs_df['param_metric'] == 'euclidean']\n",
    "gs_df.plot(x='param_n_neighbors', y='mean_test_score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Word of Caution on GridSearching\n",
    "\n",
    "`sklearn` models often have many hyperparameters with many different possible values. It may be tempting to search over a wide variety of them. In general, this is not wise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Why not?</summary>\n",
    "\n",
    "- Remember that GridSearch searches over **all possible combinations of hyperparameters in the parameter dictionary!**\n",
    "\n",
    "Imagine that we had this as our parameter dictionary:\n",
    "\n",
    "```python\n",
    "parameter_grid = {\n",
    "    'n_neighbors': range(1, 151),\n",
    "    'weights': ['uniform', 'distance', custom_function],\n",
    "    'algorithm': ['ball_tree', 'kd_tree', 'brute', 'auto'],\n",
    "    'leaf_size': range(1, 152),\n",
    "    'metric': ['minkowski', 'euclidean'],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "```\n",
    "\n",
    "**How many different combinations will need to be tested?**\n",
    "\n",
    "| Parameter | Number of Chosen Values |\n",
    "| --- | --- |\n",
    "| **n_neighbors** | 150 |\n",
    "| **weights** | 3 |\n",
    "| **algorithm** | 4 |\n",
    "| **leaf_size** | 151 |\n",
    "| **metric** | 2 |\n",
    "| **p** | 2 |\n",
    "| <br>_150 \\* 3 \\* 4 \\* 151 \\* 2 \\* 2 = n combinations_ <br><br>| _1,087,200_ |\n",
    "\n",
    "If we select `cv = 5`, we would fit 1,087,200 models on five folds, meaning we fit 5,436,000 models!\n",
    "\n",
    "If you're not careful, GridSearching can quickly scale out of hand computationally.\n",
    "\n",
    "> **It is extremely important to understand what the hyperparameters do and think critically about what ranges are useful and relevant to your model!**\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief detour: estimators and transformers.\n",
    "**Estimators** and **transformers** are two types of classes in `sklearn`.\n",
    "\n",
    "We've seen several examples of each so far.\n",
    "\n",
    "### Scikit-Learn Estimators\n",
    "Estimators are essentially _models_. They fit this format:\n",
    "\n",
    "```python\n",
    "# Instantiate.\n",
    "model = LinearRegression(params)\n",
    "# Fit.\n",
    "model.fit(X_train, y_train)\n",
    "# Predict.\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "Estimators have a **fit** and **predict** method.\n",
    "\n",
    "### Scikit-Learn Transformers\n",
    "Transformers are not models. They transform your data using similar syntax to estimators. They work like this:\n",
    "\n",
    "```python\n",
    "# Instantiate.\n",
    "ss = StandardScaler(params)\n",
    "# Fit.\n",
    "ss.fit(X_train)\n",
    "# Transform.\n",
    "X_transformed = ss.transform(X_train)\n",
    "```\n",
    "\n",
    "Instead of `fit` and `predict`, they have **fit** and **transform** methods. In fact, since you fit and transform together so often, they have a shortcut:\n",
    "\n",
    "```python\n",
    "ss = StandardScaler(params)\n",
    "X_transformed = ss.fit_transform(X_train)\n",
    "```\n",
    "\n",
    "We've seen a few transformers, including `StandardScaler()` and `PolynomialFeatures()`. There's also `OneHotEncoder()` for dummy encoding and `LabelEncoder()` for factorizing variables. Later we'll see `PCA()`, which is also a transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is this relevant?\n",
    "\n",
    "Check out the [StandardScaler documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
    "\n",
    "Transformers may have hyperparameters as well - **but we can't GridSearch over a transformer**! There's no way to get an accuracy (or other) score from just a transformer, since a transformer can't predict!\n",
    "\n",
    "\n",
    "![](./images/grid.jpg)\n",
    "\n",
    "In addition, the acronym ETL, meaning \"extract, transform, load,\" is a very common one in data science. When we gather data from one or more places, there might be **a lot** of preprocessing going on.\n",
    "\n",
    "Oftentimes, we'll want to apply several transformers to a dataset, *then* build a model. \n",
    "- If you do all of these preprocessing steps independently, your code can be messy and it'll be prone to errors!\n",
    "- It can be challenging to consistently recreate this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "![](./images/pipe.png)\n",
    "\n",
    "Pipelines will allow us to do two things:\n",
    "1. Chain many transformers together before ending in an estimator.\n",
    "2. Allow us to GridSearch over a transformer's hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a StandardScaler + kNN pipeline.\n",
    "pipe ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get params - yes, you can GridSearchCV over these!\n",
    "# Notice the naming convention of pipe arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate pipeline object.\n",
    "pipe_2 = Pipeline([\n",
    "    ('ss', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionary of hyperparameters.\n",
    "pipe_2_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our GridSearchCV object.\n",
    "pipe_2_gridsearch = GridSearchCV(pipe_2, # What is the model we want to fit?\n",
    "                                 pipe_2_params, # What is the dictionary of hyperparameters?\n",
    "                                 cv=5, # What number of folds in CV will we use?\n",
    "                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the GridSearchCV object to the data.\n",
    "pipe_2_gridsearch.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out best score.\n",
    "# from documentation: Mean cross-validated score of the best_estimator\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "pipe_2_gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out best estimator.\n",
    "pipe_2_gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What would you conclude from this output?</summary>\n",
    "    \n",
    "- Our model performs slightly better when cross-validated on our training data than on our testing data, but the difference is pretty small.\n",
    "- There may be slight overfitting.\n",
    "- GridSearching gets us the best performing model on the training set; we always have to take care to not overfit!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What is the difference between hyperparameters and statistical parameters?</summary>\n",
    "    \n",
    "- Statistical parameters are quantities that a model can learn or estimate. Examples include $\\beta_0$ and $\\beta_1$ in a linear model.\n",
    "- Hyperparameters are quantities our model cannot learn, but affect the fit of our model. Examples include $k$ in $k$-nearest neighbors and $alpha$ in regularization.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## (BONUS) RandomizedSearchCV + Visualizing Results\n",
    "\n",
    "When you're exploring a particularly high number of different hyperparameters, it can be advantageous to do a randomized search instead of a GridSearch.\n",
    "\n",
    "`from sklearn.model_selection import RandomizedSearchCV`\n",
    "\n",
    "A good blog post on GridSearch, RandomizedSearch, and visualizing the outputs of these methods [can be found here](https://towardsdatascience.com/using-3d-visualizations-to-tune-hyperparameters-of-ml-models-with-python-ba2885eab2e9).\n",
    "\n",
    "Another good example on RandomizedSearch [here](https://github.com/justmarkham/scikit-learn-tips/blob/master/notebooks/17_randomized_search.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (BONUS) make_pipeline\n",
    "\n",
    "`make_pipeline` does the same thing as `pipeline`, but does not require you to name your steps!\n",
    "\n",
    "`from sklearn.pipeline import make_pipeline`\n",
    "\n",
    "See an explanation of the difference between the two [here](https://github.com/justmarkham/scikit-learn-tips/blob/master/notebooks/12_pipeline_vs_make_pipeline.ipynb) and see an example of it used [here](https://github.com/justmarkham/scikit-learn-tips/blob/master/notebooks/08_pipeline.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (BONUS) Named Steps\n",
    "\n",
    "GridSearch not giving you all of the information you need? Want to see what is happening in the intermediate steps in a pipeline? Use the `named_steps` attribute! An example of how to use this can be found [here](https://github.com/justmarkham/scikit-learn-tips/blob/master/notebooks/13_examine_pipeline_steps.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
