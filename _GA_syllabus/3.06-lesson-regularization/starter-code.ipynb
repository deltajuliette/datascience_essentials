{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Regularization\n",
    "\n",
    "_Authors:_ Tim Book, Matt Brems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "1. Describe what a loss function is.\n",
    "2. Define regularization.\n",
    "3. Describe and differentiate LASSO and Ridge regularization.\n",
    "4. Understand how regularization affects the bias-variance tradeoff.\n",
    "5. Implement LASSO regression and Ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "<details><summary>What is the bias-variance tradeoff?</summary>\n",
    "\n",
    "- Mean squared error can be decomposed into a bias component plus a variance component (plus a systematic error, but we don't have control over this part, so we often ignore it).\n",
    "- The bias-variance tradeoff refers to the fact that taking steps to minimize bias usually comes at the expense of an increase in variance. Similarly, taking steps to minimize variance usually comes at the expense of an increase in bias.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>What evidence/information would lead me to believe that my model suffers from high variance?</summary>\n",
    "    \n",
    "- After splitting my data into training and testing sets, if I see that my model performs way better on my training set than my testing set, this means that my model is not generalizing very well to \"new\" data.\n",
    "- An example might be where our training MSE is substantially lower than our testing MSE, or where our training R-squared is substantially higher than our testing R-squared.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is high variance bad?\n",
    "\n",
    "High variance is bad because it means that our model doesn't generalize well to new data. This means that our model looks as though it performs well on our training data but won't perform as well on new, unseen data.\n",
    "\n",
    "---\n",
    "<details><summary>How might we try to fix a model that suffers from high variance?</summary>\n",
    "\n",
    "- Gather more data. (Although this is usually expensive and time-consuming.)\n",
    "- Drop features.\n",
    "- Make our existing features less complex. (i.e. get rid of interaction terms or higher order terms.)\n",
    "- Choose a simpler model.\n",
    "- Regularization!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pop Math Quiz\n",
    "\n",
    "### Problem 1\n",
    "**What is the value of $b$ that minimizes...**\n",
    "\n",
    "$$ (y - b)^2 $$\n",
    "\n",
    "<details><summary></summary>\n",
    "When $b = y$, this expression has value 0. Since it's squared, it can't go below that.\n",
    "</details>\n",
    "\n",
    "### Problem 2\n",
    "**What is the value of $b$ that minimizes...**\n",
    "\n",
    "$$ (y - b)^2 + \\alpha b^2 $$\n",
    "\n",
    "where $\\alpha > 0$?\n",
    "\n",
    "This is more complicated, isn't it? You can use calculus and come up with an answer:\n",
    "    \n",
    "$$ \\hat{b} = \\frac{y}{1 + \\alpha} $$\n",
    "\n",
    "But what is the effect of $\\alpha$ on our solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of regularization\n",
    "\n",
    "---\n",
    "\n",
    "**Regularizing** regression models is to:\n",
    "- **automatically** avoid overfitting \n",
    "- **while** we fit our model\n",
    "- by adding a \"penalty\" to our loss function.\n",
    "\n",
    "### Before regularziation (OLS):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize: MSE} &= \\textstyle\\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2 \\\\ \\\\\n",
    "                     &= \\textstyle\\frac{1}{n}\\|\\mathbf{y} - \\hat{\\mathbf{y}}\\|^2 \\\\ \\\\\n",
    "                     &= \\textstyle\\frac{1}{n}\\|\\mathbf{y} - \\mathbf{X\\beta}\\|^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### After regularization (Ridge):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize: MSE + penalty} &= \\textstyle\\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2 + \\alpha \\sum \\beta_j^2 \\\\ \\\\\n",
    "                               &= \\textstyle\\frac{1}{n}\\|\\mathbf{y} - \\hat{\\mathbf{y}}\\|^2 + \\alpha \\|\\beta\\|^2 \\\\ \\\\\n",
    "                               &= \\textstyle\\frac{1}{n}\\|\\mathbf{y} - \\mathbf{X}\\hat{\\beta}\\|^2 + \\alpha \\|\\beta\\|^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Adding this penalty term onto the end and then minimizing has a similar effect to the one described above. That is, **ridge regression shrinks our regression coefficients closer to zero to make our model simpler**. We are accepting more bias in exchange for decreased variance. We'll be tasked with picking the \"best\" $\\alpha$ that optimizes this bias-variance tradeoff.\n",
    "\n",
    "### Other Variations\n",
    "\n",
    "| Name | Loss Function |\n",
    "| --- | --- |\n",
    "| OLS | MSE |\n",
    "| Ridge Regression | MSE + $\\alpha\\|\\beta\\|^2_2$ |\n",
    "| LASSO Regression | MSE + $\\alpha\\|\\beta\\|_1$ |\n",
    "| $L_q$-Regression | MSE + $\\alpha\\|\\beta\\|^q_q$ |\n",
    "\n",
    "### Sidenote on notation:\n",
    "We'll be using $\\alpha$ to denote our **regularization parameter**, since that's what Scikit-Learn uses. However, this is contrary to data science literature. It is normally denoted with a $\\lambda$. Why? Only Google knows.\n",
    "\n",
    "### [Neat parameter space visualization!](https://timothykbook.shinyapps.io/RegularizationPlot/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the effect of regularization?\n",
    "\n",
    "---\n",
    "\n",
    "**To demonstrate the effects of regularization, we will be using a dataset on wine quality.**\n",
    "\n",
    "### Load the wine .csv\n",
    "\n",
    "This version has red and white wines concatenated together and tagged with a binary 1/0 indicator (1 is red wine). There are many other variables purportedly related to the rated quality of the wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the wine .csv.\n",
    "wine = pd.read_csv('datasets/winequality_merged.csv')\n",
    "\n",
    "# Convert all columns to lowercase and replace spaces in column names.\n",
    "wine.columns = wine.columns.str.lower().str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>ph</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>red_wine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free_sulfur_dioxide  total_sulfur_dioxide  density    ph  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  red_wine  \n",
       "0      9.4        5         1  \n",
       "1      9.8        5         1  \n",
       "2      9.8        5         1  \n",
       "3      9.8        6         1  \n",
       "4      9.4        5         1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first five rows.\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6497, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How big is this dataset?\n",
    "wine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed_acidity           0\n",
       "volatile_acidity        0\n",
       "citric_acid             0\n",
       "residual_sugar          0\n",
       "chlorides               0\n",
       "free_sulfur_dioxide     0\n",
       "total_sulfur_dioxide    0\n",
       "density                 0\n",
       "ph                      0\n",
       "sulphates               0\n",
       "alcohol                 0\n",
       "quality                 0\n",
       "red_wine                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values.\n",
    "wine.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create X and y.\n",
    "X = wine.drop('quality', axis=1)\n",
    "y = wine['quality']\n",
    "\n",
    "# Instantiate our PolynomialFeatures object to create all two-way terms.\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "\n",
    "# Fit and transform our X data.\n",
    "X_overfit = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fixed_acidity',\n",
       " 'volatile_acidity',\n",
       " 'citric_acid',\n",
       " 'residual_sugar',\n",
       " 'chlorides',\n",
       " 'free_sulfur_dioxide',\n",
       " 'total_sulfur_dioxide',\n",
       " 'density',\n",
       " 'ph',\n",
       " 'sulphates',\n",
       " 'alcohol',\n",
       " 'red_wine',\n",
       " 'fixed_acidity^2',\n",
       " 'fixed_acidity volatile_acidity',\n",
       " 'fixed_acidity citric_acid',\n",
       " 'fixed_acidity residual_sugar',\n",
       " 'fixed_acidity chlorides',\n",
       " 'fixed_acidity free_sulfur_dioxide',\n",
       " 'fixed_acidity total_sulfur_dioxide',\n",
       " 'fixed_acidity density',\n",
       " 'fixed_acidity ph',\n",
       " 'fixed_acidity sulphates',\n",
       " 'fixed_acidity alcohol',\n",
       " 'fixed_acidity red_wine',\n",
       " 'volatile_acidity^2',\n",
       " 'volatile_acidity citric_acid',\n",
       " 'volatile_acidity residual_sugar',\n",
       " 'volatile_acidity chlorides',\n",
       " 'volatile_acidity free_sulfur_dioxide',\n",
       " 'volatile_acidity total_sulfur_dioxide',\n",
       " 'volatile_acidity density',\n",
       " 'volatile_acidity ph',\n",
       " 'volatile_acidity sulphates',\n",
       " 'volatile_acidity alcohol',\n",
       " 'volatile_acidity red_wine',\n",
       " 'citric_acid^2',\n",
       " 'citric_acid residual_sugar',\n",
       " 'citric_acid chlorides',\n",
       " 'citric_acid free_sulfur_dioxide',\n",
       " 'citric_acid total_sulfur_dioxide',\n",
       " 'citric_acid density',\n",
       " 'citric_acid ph',\n",
       " 'citric_acid sulphates',\n",
       " 'citric_acid alcohol',\n",
       " 'citric_acid red_wine',\n",
       " 'residual_sugar^2',\n",
       " 'residual_sugar chlorides',\n",
       " 'residual_sugar free_sulfur_dioxide',\n",
       " 'residual_sugar total_sulfur_dioxide',\n",
       " 'residual_sugar density',\n",
       " 'residual_sugar ph',\n",
       " 'residual_sugar sulphates',\n",
       " 'residual_sugar alcohol',\n",
       " 'residual_sugar red_wine',\n",
       " 'chlorides^2',\n",
       " 'chlorides free_sulfur_dioxide',\n",
       " 'chlorides total_sulfur_dioxide',\n",
       " 'chlorides density',\n",
       " 'chlorides ph',\n",
       " 'chlorides sulphates',\n",
       " 'chlorides alcohol',\n",
       " 'chlorides red_wine',\n",
       " 'free_sulfur_dioxide^2',\n",
       " 'free_sulfur_dioxide total_sulfur_dioxide',\n",
       " 'free_sulfur_dioxide density',\n",
       " 'free_sulfur_dioxide ph',\n",
       " 'free_sulfur_dioxide sulphates',\n",
       " 'free_sulfur_dioxide alcohol',\n",
       " 'free_sulfur_dioxide red_wine',\n",
       " 'total_sulfur_dioxide^2',\n",
       " 'total_sulfur_dioxide density',\n",
       " 'total_sulfur_dioxide ph',\n",
       " 'total_sulfur_dioxide sulphates',\n",
       " 'total_sulfur_dioxide alcohol',\n",
       " 'total_sulfur_dioxide red_wine',\n",
       " 'density^2',\n",
       " 'density ph',\n",
       " 'density sulphates',\n",
       " 'density alcohol',\n",
       " 'density red_wine',\n",
       " 'ph^2',\n",
       " 'ph sulphates',\n",
       " 'ph alcohol',\n",
       " 'ph red_wine',\n",
       " 'sulphates^2',\n",
       " 'sulphates alcohol',\n",
       " 'sulphates red_wine',\n",
       " 'alcohol^2',\n",
       " 'alcohol red_wine',\n",
       " 'red_wine^2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly.get_feature_names(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6497, 90)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the dimensions of X_overfit.\n",
    "X_overfit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's split our data up into training and testing sets. Why do we split our data into training and testing sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test splits.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_overfit,\n",
    "    y,\n",
    "    test_size=0.7,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale our data.\n",
    "# Relabeling scaled data as \"Z\" is common.\n",
    "sc = StandardScaler()\n",
    "Z_train = sc.fit_transform(X_train)\n",
    "Z_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_train shape is: (1949, 90)\n",
      "y_train shape is: (1949,)\n",
      "Z_test shape is: (4548, 90)\n",
      "y_test shape is: (4548,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Z_train shape is: {Z_train.shape}')\n",
    "print(f'y_train shape is: {y_train.shape}')\n",
    "print(f'Z_test shape is: {Z_test.shape}')\n",
    "print(f'y_test shape is: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing predictors is required\n",
    "\n",
    "Let's remind ourselves of our new loss function:\n",
    "\n",
    "$$MSE + \\alpha \\|\\beta\\|^2$$\n",
    "\n",
    "<details><summary>Why do you think regularization is required?</summary>\n",
    "Recall that the size of each coefficient depends on the scale of its corresponding variable. Our penalty term depends on these coefficients. Scaling is required so that the regularization penalizes each variable equally fairly.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But First: OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the appropriate library and fit our OLS model.\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols = LinearRegression() # Start  model\n",
    "ols.fit(Z_train, y_train) # Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40698195242682933\n",
      "0.22021547039851397\n"
     ]
    }
   ],
   "source": [
    "# How does the model score on the training and test data?\n",
    "print(ols.score(Z_train, y_train))\n",
    "print(ols.score(Z_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(THREAD) What do these $R^2$s tell you?\n",
    "\n",
    "**Note**: Train vs. test scores differ by a lot - Model has not generalised well, so it may well be overfit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And Now: Ridge\n",
    "\n",
    "### Let's think about this...\n",
    "\n",
    "$$ \\|\\mathbf{y} - \\mathbf{X}\\beta\\|^2 + \\alpha\\|\\beta\\|^2 $$\n",
    "\n",
    "<details><summary>What's the optimal value of $\\beta$ when $\\alpha = 0$?</summary>\n",
    "Our problem reduces to OLS, so it's the good old fashioned OLS solution! For the math nerds playing along from home, that's:\n",
    "    \n",
    "$$ \\hat{\\beta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} $$\n",
    "</details>\n",
    "\n",
    "<details><summary>What's the optimal value of $\\beta$ when $\\alpha = \\infty$?</summary>\n",
    "Anything besides $\\hat{\\beta} = \\mathbf{0}$ will cause our whole loss function to be $\\infty$. So, it must be that $\\hat{\\beta} = \\mathbf{0}$!\n",
    "</details>\n",
    "\n",
    "<details><summary>Some facts...</summary>\n",
    "$\\alpha$ is a constant for the _strength_ of the regularization parameter. The higher the value, the greater the impact of this new component in the loss function. If the value was zero, we would revert back to just the least squares loss function. If the value was a billion, however, the residual sum of squares component would have a much smaller effect on the loss/cost than the regularization term.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can look at a traceplot to see this:\n",
    "\n",
    "![](./imgs/ridge-trace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, so which $\\alpha$ is best?\n",
    "\n",
    "We'll primarily choose the optimal $\\alpha$ via **cross validation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regressor lives here:\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37708346347575206\n",
      "0.2442363970320075\n"
     ]
    }
   ],
   "source": [
    "# Instantiate.\n",
    "ridge_model = Ridge(alpha=10)\n",
    "\n",
    "# Fit.\n",
    "ridge_model.fit(Z_train, y_train) # Don't need to transform values again\n",
    "\n",
    "# Evaluate model using R2.\n",
    "print(ridge_model.score(Z_train, y_train))\n",
    "print(ridge_model.score(Z_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(THREAD) What do these $R^2$s tell you?\n",
    "\n",
    "**Note**: Still not as close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute-forcing the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a list of ridge alphas to check.\n",
    "# np.logspace generates 100 values equally between 0 and 5,\n",
    "# then converts them to alphas between 10^0 and 10^5.\n",
    "r_alphas = np.logspace(0,5,100)\n",
    "\n",
    "# Cross-validate over our list of ridge alphas.\n",
    "ridge_cv = RidgeCV(alphas=r_alphas, scoring='r2', cv=5).fit(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104.76157527896652\n"
     ]
    }
   ],
   "source": [
    "# Here is the optimal value of alpha\n",
    "print(ridge_cv.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `ridge_model` object is actually already the model with the optimal $\\alpha$. Let's get the corresponding value of $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34845153627219094\n",
      "0.29488871636997804\n"
     ]
    }
   ],
   "source": [
    "print(ridge_cv.score(Z_train, y_train))\n",
    "print(ridge_cv.score(Z_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(THREAD) What do these $R^2$s tell you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the LASSO\n",
    "\n",
    "LASSO regression is largely the same as ridge, except with a different penalty term.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize: MSE + penalty} &= \\textstyle\\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2 + \\alpha \\sum |\\beta_j| \\\\ \\\\\n",
    "                               &= \\textstyle\\frac{1}{n}\\|\\mathbf{y} - \\hat{\\mathbf{y}}\\|^2 + \\alpha \\|\\beta\\|_1 \\\\ \\\\\n",
    "                               &= \\textstyle\\frac{1}{n}\\|\\mathbf{y} - \\mathbf{X}\\hat{\\beta}\\|^2 + \\alpha \\|\\beta\\|_1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The penalty is now made up from the **$\\mathcal{l}_1$-norm**, otherwise known as **Manhattan distance**. It is simply the absolute sum of the vector components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The LASSO traceplot looks a little different...\n",
    "But I don't want to show it to you yet! We'll see it soon and discuss what LASSO actually does differently from Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports similar to Ridge\n",
    "from sklearn.linear_model import Lasso, LassoCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== OLS =======\n",
      "0.40698195242682933\n",
      "0.22021547039851397\n",
      "\n",
      "===== Ridge ======\n",
      "0.34845153627219094\n",
      "0.29488871636997804\n"
     ]
    }
   ],
   "source": [
    "# Reminders\n",
    "print(\" OLS \".center(18, \"=\"))\n",
    "print(ols.score(Z_train, y_train))\n",
    "print(ols.score(Z_test, y_test))\n",
    "print()\n",
    "print(\" Ridge \".center(18, \"=\"))\n",
    "print(ridge_cv.score(Z_train, y_train))\n",
    "print(ridge_cv.score(Z_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a list of Lasso alphas to check.\n",
    "l_alphas = np.logspace(-3,0,100)\n",
    "\n",
    "# Cross-validate over our list of Lasso alphas.\n",
    "lasso_cv = LassoCV(alphas=l_alphas, cv=5, max_iter=50000).fit(Z_train, y_train) # Fitting 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104.76157527896652"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is the optimal value of alpha\n",
    "ridge_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3366703928164062\n",
      "0.2855544326156594\n"
     ]
    }
   ],
   "source": [
    "print(lasso_cv.score(Z_train, y_train))\n",
    "print(lasso_cv.score(Z_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge vs LASSO, what's the diff?!\n",
    "Let's check out the coefficients of the Lasso and Ridge models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.41082332e-04, -6.73622850e-02,  4.11979658e-03,  3.58118418e-02,\n",
       "       -7.86169814e-03,  7.96837868e-02,  4.07202575e-02, -6.40134004e-02,\n",
       "        1.67657002e-03,  2.63542441e-02,  4.89229448e-02, -5.96326152e-04,\n",
       "        3.97892719e-03, -3.38291402e-02, -4.33716705e-03,  1.37005906e-01,\n",
       "        3.07581441e-02,  3.60944933e-02, -6.74979641e-03,  6.29819066e-04,\n",
       "        8.10392864e-03,  3.73389457e-02,  6.71509211e-03,  7.15269227e-02,\n",
       "        1.12923745e-02, -1.19654956e-02, -7.94404967e-02, -1.56703211e-02,\n",
       "       -3.95481031e-03, -4.59646775e-02, -6.81012094e-02, -6.04899669e-02,\n",
       "       -3.27140260e-04,  4.46369117e-02,  7.03665877e-02, -5.18587000e-02,\n",
       "        2.51915541e-02,  3.39269447e-02,  4.95212161e-02, -2.47711305e-02,\n",
       "        3.96451349e-03,  6.67705387e-04, -3.08860641e-02,  2.35098736e-02,\n",
       "        3.55708420e-02,  3.93321581e-02,  4.86155004e-02, -3.50408563e-02,\n",
       "        5.62724938e-02,  3.71036473e-02, -2.69137546e-02, -1.40643736e-02,\n",
       "       -1.15979037e-02,  8.78107644e-05,  3.67820396e-02,  3.99396816e-02,\n",
       "       -2.57914127e-02, -7.05148813e-03, -3.56592158e-02, -6.10657197e-02,\n",
       "       -3.51923177e-02,  5.83693665e-02, -2.31654728e-01, -7.84090377e-02,\n",
       "        7.82032866e-02,  7.79743584e-02, -6.63106805e-03,  8.87411180e-02,\n",
       "       -1.25820489e-01, -5.84158917e-02,  3.98653270e-02,  4.10661964e-02,\n",
       "       -9.35719900e-02,  3.43253957e-02,  1.76071648e-02, -6.19585469e-02,\n",
       "       -5.50871730e-03,  2.52242484e-02,  4.58625104e-02, -3.51486916e-04,\n",
       "        2.81993685e-03,  5.21772772e-02,  7.11282800e-02, -2.51823938e-02,\n",
       "       -4.76600158e-02,  9.43845468e-02,  1.79373582e-02,  1.14815172e-01,\n",
       "        2.83989485e-02, -5.96326152e-04])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_cv.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        0.16835564,  0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        ,  0.        , -0.        ,  0.06311839,  0.        ,\n",
       "       -0.        , -0.05120041, -0.        , -0.        , -0.07081332,\n",
       "       -0.16039109, -0.        , -0.        , -0.        ,  0.00492753,\n",
       "       -0.01006693,  0.        ,  0.        ,  0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        ,  0.01658083,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        ,  0.00238066, -0.28613203, -0.        ,  0.        ,\n",
       "        0.13542067,  0.        ,  0.25586092, -0.06659136, -0.        ,\n",
       "        0.        ,  0.        , -0.05394452,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.01057707,  0.        , -0.        ,\n",
       "        0.11516198,  0.        ,  0.29526458,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_cv.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cliffsnotes: L.A.S.S.O.\n",
    "LASSO is actually an acronym:\n",
    "\n",
    "* **L**east\n",
    "* **A**bsolute\n",
    "* **S**hrinkage and\n",
    "* **S**election\n",
    "* **O**perator\n",
    "\n",
    "**SHRINKAGE**: Higher $\\alpha$ \"shrinks\" $\\beta$ towards $\\mathbf{0}$.\n",
    "\n",
    "**SELECTION**: Higher $\\alpha$ zeros out small $\\beta$s.\n",
    "\n",
    "![](../imgs/lasso-trace.svg)\n",
    "\n",
    "## So, um, what was LASSO doing here?\n",
    "If you're an ultra math nerd, you might have noticed something fishy about our \"penalty parameter\" $\\alpha$. We're doing an optimization problem, so actually, this $\\alpha$ is a **Lagrange multiplier**. This means that optimizing our loss function:\n",
    "\n",
    "$$ \\|\\mathbf{y} - \\mathbf{X}\\beta\\|^2 + \\alpha\\|\\beta\\|_1 $$\n",
    "\n",
    "is equivalent to optimizing the **constrained loss function**:\n",
    "\n",
    "$$ \\|\\mathbf{y} - \\mathbf{X}\\beta\\|^2 \\quad \\text{such that} \\quad \\|\\beta\\|_1 \\le t $$\n",
    "\n",
    "## [BRING IN THE APP!](https://timothykbook.shinyapps.io/RegularizationPlot/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularizing Logistic Regression: You've been doing it all along!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at the LogisticRegression documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization is the hidden default for logistic regression. What a pain!\n",
    "Unless regularization is necessary, **it should not be done!!** (It makes interpreting the coefficients much more difficult.) In newer version of Scikit-Learn, you can finally turn this feature off!\n",
    "\n",
    "If you _do_ want to regularize, note that there is a much friendlier `LogisticRegressionCV` we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression(penalty='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=200,\n",
    "    n_informative=15,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=1e9, solver='lbfgs')\n",
    "logreg.fit(X_train_sc, y_train)\n",
    "\n",
    "# Overfit!\n",
    "print(logreg.score(X_train_sc, y_train))\n",
    "print(logreg.score(X_test_sc, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_cv = LogisticRegressionCV(Cs=10, cv=5, penalty=\"l1\", solver=\"liblinear\")\n",
    "logreg_cv.fit(X_train_sc, y_train)\n",
    "\n",
    "print(logreg_cv.score(X_train_sc, y_train))\n",
    "print(logreg_cv.score(X_test_sc, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_cv.C_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elephant in the Room: Categorical Variables\n",
    "Think about it. What does it mean to scale a binary variable? How about a categorical variable dummified into several columns? What does it mean to shrink the coefficients associated with these columns? What happens if the LASSO zeros out one category, but not others? I don't know, either.\n",
    "\n",
    "It turns out, it's not a great idea to combine scaling and categorical data. It often just doesn't make sense to do. This is true for all algorithms where we need to scale, including kNN. So what do we do? A few options:\n",
    "\n",
    "* Set separate regularization parameters for each x-variable (not available in Scikit-Learn).\n",
    "* Carry out the _grouped LASSO_ technique (not available in Scikit-Learn, and doesn't solve all problems anyway).\n",
    "* Manually decide on a scale for these variables (time consuming, unintuitive, still doesn't work with regularization).\n",
    "* Don't use those variables (but you want them!).\n",
    "* Just do it anyway. Who knows, it'll probably be fine! (¯\\_(ツ)_/¯)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Notes\n",
    "- The $\\alpha$ hyperparameter for regularization is **unrelated** to significance level in hypothesis testing.\n",
    "- In certain resources, including [ISLR](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf), you'll see that $\\lambda$ is used instead of $\\alpha$ for regularization strength.\n",
    "- We must standardize before regularizing, but regularization and standardization are not the same things!\n",
    "- **FROM NOW ON, YOU MUST PAY ATTENTION TO REGULARIZATION WHEN CONDUCTING LOGISTIC REGRESSION!!!**\n",
    "- Ridge regression is sometimes called **weight decay**, but usually only when regularizing neural networks.\n",
    "- LASSO regression is sometimes called **basis pursuit**, but that's very old fashioned.\n",
    "- The y-intercept for these models are not regularized.\n",
    "\n",
    "## Recap\n",
    "- Regularization is used when evidence suggests our model is suffering from high error due to variance.\n",
    "- Evidence to suggest our model suffers from high error due to variance includes substantially better performance on our training set than our testing set.\n",
    "- LASSO tends to be \"more brutal\" than Ridge regularization in that it will zero out coefficients.\n",
    "- If you want to combine LASSO and Ridge regularization, there is a technique called \"ElasticNet\" that does exactly this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet Regression (bonus)\n",
    "\n",
    "---\n",
    "\n",
    "Can't decide?\n",
    "\n",
    "![](../imgs/why-not-both.jpg)\n",
    "\n",
    "The Elastic Net combines the Ridge and Lasso penalties.  It adds *both* penalties to the loss function:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "SSE + Ridge + Lasso &=& \\sum_{i=1}^n \\left(y_i - \\hat{y}_i\\right)^2 + \\alpha\\left[\\rho\\sum_{j=1}^p |\\beta_j| + (1-\\rho)\\sum_{j=1}^p \\beta_j^2\\right] \\\\\n",
    "&=& \\|\\mathbf{y} - \\mathbf{X}\\beta\\|^2 + \\alpha\\left(\\rho\\|\\beta\\|_1 + (1 - \\rho)\\|\\beta\\|^2\\right)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "\n",
    "In the elastic net, the effect of the ridge versus the lasso is balanced by the $\\rho$ parameter.  It is the ratio of Lasso penalty to Ridge penalty and must be between zero and one.\n",
    "\n",
    "`ElasticNet` in sklearn has two parameters:\n",
    "- `alpha`: the regularization strength.\n",
    "- `l1_ratio`: the amount of L1 vs L2 penalty (i.e., $\\rho$). An l1_ratio of 0 is equivalent to the Ridge, whereas an l1_ratio of 1 is equivalent to the Lasso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the coefficients with both alpha values and an l1_ratio of 0.05. Lasso can \"overpower\" the Ridge penalty in some datasets, and so rather than an equal balance I'm just adding a little bit of Lasso in.\n",
    "- Using a $\\rho$ value below 0.05 can empirically cause issues in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a list of alphas to check.\n",
    "enet_alphas = np.linspace(0.5, 1.0, 100)\n",
    "\n",
    "# Set up our l1 ratio. (What does this do?)\n",
    "enet_ratio = 0.5\n",
    "\n",
    "# Instantiate model.\n",
    "enet_model = ElasticNetCV(alphas=enet_alphas, l1_ratio=enet_ratio, cv=5)\n",
    "\n",
    "# Fit model using optimal alpha.\n",
    "enet_model = enet_model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions.\n",
    "enet_model_preds = enet_model.predict(X_test)\n",
    "enet_model_preds_train = enet_model.predict(X_train)\n",
    "\n",
    "# Evaluate model.\n",
    "print(enet_model.score(X_train, y_train))\n",
    "print(enet_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the optimal value of alpha.\n",
    "enet_model.alpha_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "state": {
    "03860d2d80204ca295d01e93e8e99474": {
     "views": [
      {
       "cell_index": 41
      }
     ]
    },
    "b535fb165fa343b297ba42fb4a55c6fa": {
     "views": [
      {
       "cell_index": 34
      }
     ]
    },
    "f5d5ef714eee4c61b085a3bb6b96cd73": {
     "views": [
      {
       "cell_index": 55
      }
     ]
    },
    "fdc5e91596ea49fa84bf4aed6d37b849": {
     "views": [
      {
       "cell_index": 47
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
