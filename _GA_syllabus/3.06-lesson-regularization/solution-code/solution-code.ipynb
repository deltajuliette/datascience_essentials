{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Regularization\n",
    "\n",
    "_Authors:_ Tim Book, Matt Brems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "1. Describe what a loss function is.\n",
    "2. Define regularization.\n",
    "3. Describe and differentiate LASSO and Ridge regularization.\n",
    "4. Understand how regularization affects the bias-variance tradeoff.\n",
    "5. Implement LASSO regression and Ridge regression.\n",
    "\n",
    "[Optional read](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a) from _Medium_. I found this immensly helpful in preparation for the intuition deck we just covered on this topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "<details><summary>What is the bias-variance tradeoff?</summary>\n",
    "\n",
    "- Mean squared error can be decomposed into a bias component plus a variance component (plus a systematic error, but we don't have control over this part, so we often ignore it).\n",
    "    <center>$E[SSE] = \\text{bias}^2 + variance + \\sigma^2$</center>\n",
    "- The bias-variance tradeoff refers to the fact that taking steps to minimize bias usually comes at the expense of an increase in variance (and possibly making a complex, overfit model). Similarly, taking steps to minimize variance usually comes at the expense of an increase in bias (and possibly making a too simplistic underfit model).\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>What evidence/information would lead me to believe that my model suffers from high variance?</summary>\n",
    "    \n",
    "- After splitting my data into training and testing sets, if I see that my model <b>performs way better on my training set than my testing set, this means that my model is not generalizing very well to \"new\" data</b>.\n",
    "- An example might be where our training MSE is substantially lower than our testing MSE, or where our training R-squared is substantially higher than our testing R-squared. In other words, <i>training vs testing metrics are quite different</i>.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is high variance bad?\n",
    "\n",
    "High variance is bad because it means that our model <b>doesn't generalize well to new data</b>. This means that our model looks as though it performs well on our training data but won't perform as well on new, unseen data.\n",
    "\n",
    "---\n",
    "<details><summary>How might we try to fix a model that suffers from high variance?</summary>\n",
    "\n",
    "There are multiple ways:\n",
    "- Gather more data. (Although this is usually expensive and time-consuming.)\n",
    "- Drop <i>(irrelevant)</i> features.\n",
    "- Make our existing features less complex. (i.e. get rid of interaction terms or higher order terms.)\n",
    "- Choose a simpler model.\n",
    "- <b>Regularization!</b>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pop Math Quiz\n",
    "\n",
    "### Problem 1\n",
    "**What is the least value of $b$ that minimizes the following term...**\n",
    "\n",
    "$$ f = (y - b)^2 $$\n",
    "\n",
    "<details><summary></summary>\n",
    "When $b = y$, this expression has value 0. Since it's squared, it can't go below that.\n",
    "</details>\n",
    "\n",
    "### Problem 2\n",
    "**What is the value of $b$ that minimizes...**\n",
    "\n",
    "$$ f = (y - b)^2 + \\alpha b^2 $$\n",
    "\n",
    "where $\\alpha > 0$?\n",
    "\n",
    "<details><summary></summary>\n",
    "This is more complicated, isn't it? You can use differential calculus and come up with an answer:\n",
    "\n",
    "$$ \\frac{\\partial{f}}{\\partial{b}} = 2(y - b)(0 - 1) + 2\\alpha b $$\n",
    "\n",
    "To find the minima of f: partial derivative w.r.t b --> 0 --> $\\hat b$\n",
    "    \n",
    "$$ 0 = 2(\\hat b - y) + 2\\alpha \\hat b $$\n",
    "    \n",
    "$$ \\hat{b} = \\frac{y}{1 + \\alpha} $$\n",
    "\n",
    "But what is the effect of $\\alpha$ on our solution?\n",
    "- when $\\alpha$ increases, $\\hat{b}$ drops  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of regularization\n",
    "\n",
    "---\n",
    "\n",
    "**Regularizing** regression models is to:\n",
    "- **automatically** avoid ***overfitting*** while we _fit our model_ by adding a `penalty` to our loss function.\n",
    "\n",
    "### _Before_ regularization (OLS - ordinary least squares):\n",
    "where ||v|| is the notation for the norm of a vector v and is calculated as the sum of the absolute vector values.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize: MSE} &= \\textstyle\\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2 \\\\ \\\\\n",
    "                     &= \\textstyle\\frac{1}{n}\\|\\mathbf{y} - \\hat{\\mathbf{y}}\\|^2 \\\\ \\\\\n",
    "                     &= \\textstyle\\frac{1}{n}\\|\\mathbf{y} - \\mathbf{X\\beta}\\|^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### _After_ regularization (Ridge):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize: MSE + penalty} &= \\textstyle\\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2 + \\alpha \\sum \\beta_j^2 \\\\ \\\\\n",
    "                               &= \\textstyle\\frac{1}{n}\\|\\mathbf{y} - \\hat{\\mathbf{y}}\\|^2 + \\alpha \\|\\beta\\|^2 \\\\ \\\\\n",
    "                               &= \\textstyle\\frac{1}{n}\\|\\mathbf{y} - \\mathbf{X}\\hat{\\beta}\\|^2 + \\alpha \\|\\beta\\|^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Adding this penalty term onto the end and then minimizing has a similar effect to the one described above in Problem 2. That is, **ridge regression shrinks our regression coefficients closer to zero to make our model simpler**. We are accepting more bias in exchange for decreased variance. We'll be tasked with picking the \"best\" $\\alpha$ that optimizes this bias-variance tradeoff.\n",
    "\n",
    "### Other Variations\n",
    "\n",
    "| Name | Loss Function |\n",
    "| --- | --- |\n",
    "| OLS | MSE |\n",
    "| Ridge Regression (L2 Norm) | MSE + $\\alpha\\|\\beta\\|^2_2$ |\n",
    "| LASSO Regression (L1 Norm) | MSE + $\\alpha\\|\\beta\\|_1$ |\n",
    "| $L_q$-Regression | MSE + $\\alpha\\|\\beta\\|^q_q$ |\n",
    "\n",
    "### Sidenote on notation:\n",
    "We'll be using $\\alpha$ to denote our **regularization parameter**, since that's what Scikit-Learn uses. However, this is contrary to data science literature. It is normally denoted with a $\\lambda$. Why? Only Google knows.\n",
    "\n",
    "### [Neat parameter space visualization!](https://timothykbook.shinyapps.io/RegularizationPlot/)\n",
    "Increasing regularization parameter, shrinks coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the effect of regularization?\n",
    "\n",
    "---\n",
    "\n",
    "**To demonstrate the effects of regularization, we will be using a dataset on wine quality.**\n",
    "\n",
    "### Load the wine .csv\n",
    "\n",
    "This version has red and white wines concatenated together and tagged with a binary 1/0 indicator (1 is red wine). There are many other variables purportedly related to the rated quality of the wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid') # optional, just plots charts on a darker (gray) canvas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the wine .csv.\n",
    "wine = pd.read_csv('datasets/winequality_merged.csv')\n",
    "\n",
    "# Convert all columns to lowercase and replace spaces in column names with _\n",
    "wine.columns = wine.columns.str.lower().str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>ph</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>red_wine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free_sulfur_dioxide  total_sulfur_dioxide  density    ph  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  red_wine  \n",
       "0      9.4        5         1  \n",
       "1      9.8        5         1  \n",
       "2      9.8        5         1  \n",
       "3      9.8        6         1  \n",
       "4      9.4        5         1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first five rows.\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6497, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How big is this dataset?\n",
    "wine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed_acidity           0\n",
       "volatile_acidity        0\n",
       "citric_acid             0\n",
       "residual_sugar          0\n",
       "chlorides               0\n",
       "free_sulfur_dioxide     0\n",
       "total_sulfur_dioxide    0\n",
       "density                 0\n",
       "ph                      0\n",
       "sulphates               0\n",
       "alcohol                 0\n",
       "quality                 0\n",
       "red_wine                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values.--> there are none\n",
    "wine.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures # to generate polynomial and interaction terms\n",
    "\n",
    "# Create X and y.\n",
    "X = wine.drop('quality', axis=1)# take all cols except, 'quality' --> response\n",
    "y = wine['quality']\n",
    "\n",
    "# Instantiate our PolynomialFeatures object to create all two-way terms. degree=2 by default\n",
    "# meaning, for [a,b]-->[a,b,a^2,b^2,ab]\n",
    "# include_bias=False, because we don't need to manually create intercept col for sklearn\n",
    "poly = PolynomialFeatures(include_bias=False)\n",
    "# Fit and transform our X data.\n",
    "X_overfit = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6497, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape # num of feature cols, leaving 'quality'-->12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fixed_acidity',\n",
       " 'volatile_acidity',\n",
       " 'citric_acid',\n",
       " 'residual_sugar',\n",
       " 'chlorides',\n",
       " 'free_sulfur_dioxide',\n",
       " 'total_sulfur_dioxide',\n",
       " 'density',\n",
       " 'ph',\n",
       " 'sulphates',\n",
       " 'alcohol',\n",
       " 'red_wine',\n",
       " 'fixed_acidity^2',\n",
       " 'fixed_acidity volatile_acidity',\n",
       " 'fixed_acidity citric_acid',\n",
       " 'fixed_acidity residual_sugar',\n",
       " 'fixed_acidity chlorides',\n",
       " 'fixed_acidity free_sulfur_dioxide',\n",
       " 'fixed_acidity total_sulfur_dioxide',\n",
       " 'fixed_acidity density',\n",
       " 'fixed_acidity ph',\n",
       " 'fixed_acidity sulphates',\n",
       " 'fixed_acidity alcohol',\n",
       " 'fixed_acidity red_wine',\n",
       " 'volatile_acidity^2',\n",
       " 'volatile_acidity citric_acid',\n",
       " 'volatile_acidity residual_sugar',\n",
       " 'volatile_acidity chlorides',\n",
       " 'volatile_acidity free_sulfur_dioxide',\n",
       " 'volatile_acidity total_sulfur_dioxide',\n",
       " 'volatile_acidity density',\n",
       " 'volatile_acidity ph',\n",
       " 'volatile_acidity sulphates',\n",
       " 'volatile_acidity alcohol',\n",
       " 'volatile_acidity red_wine',\n",
       " 'citric_acid^2',\n",
       " 'citric_acid residual_sugar',\n",
       " 'citric_acid chlorides',\n",
       " 'citric_acid free_sulfur_dioxide',\n",
       " 'citric_acid total_sulfur_dioxide',\n",
       " 'citric_acid density',\n",
       " 'citric_acid ph',\n",
       " 'citric_acid sulphates',\n",
       " 'citric_acid alcohol',\n",
       " 'citric_acid red_wine',\n",
       " 'residual_sugar^2',\n",
       " 'residual_sugar chlorides',\n",
       " 'residual_sugar free_sulfur_dioxide',\n",
       " 'residual_sugar total_sulfur_dioxide',\n",
       " 'residual_sugar density',\n",
       " 'residual_sugar ph',\n",
       " 'residual_sugar sulphates',\n",
       " 'residual_sugar alcohol',\n",
       " 'residual_sugar red_wine',\n",
       " 'chlorides^2',\n",
       " 'chlorides free_sulfur_dioxide',\n",
       " 'chlorides total_sulfur_dioxide',\n",
       " 'chlorides density',\n",
       " 'chlorides ph',\n",
       " 'chlorides sulphates',\n",
       " 'chlorides alcohol',\n",
       " 'chlorides red_wine',\n",
       " 'free_sulfur_dioxide^2',\n",
       " 'free_sulfur_dioxide total_sulfur_dioxide',\n",
       " 'free_sulfur_dioxide density',\n",
       " 'free_sulfur_dioxide ph',\n",
       " 'free_sulfur_dioxide sulphates',\n",
       " 'free_sulfur_dioxide alcohol',\n",
       " 'free_sulfur_dioxide red_wine',\n",
       " 'total_sulfur_dioxide^2',\n",
       " 'total_sulfur_dioxide density',\n",
       " 'total_sulfur_dioxide ph',\n",
       " 'total_sulfur_dioxide sulphates',\n",
       " 'total_sulfur_dioxide alcohol',\n",
       " 'total_sulfur_dioxide red_wine',\n",
       " 'density^2',\n",
       " 'density ph',\n",
       " 'density sulphates',\n",
       " 'density alcohol',\n",
       " 'density red_wine',\n",
       " 'ph^2',\n",
       " 'ph sulphates',\n",
       " 'ph alcohol',\n",
       " 'ph red_wine',\n",
       " 'sulphates^2',\n",
       " 'sulphates alcohol',\n",
       " 'sulphates red_wine',\n",
       " 'alcohol^2',\n",
       " 'alcohol red_wine',\n",
       " 'red_wine^2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly.get_feature_names(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6497, 90)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the dimensions of X_overfit.\n",
    "# getting poly features on 12 cols should result in-->12_original+12_squared_of_original+multipliers\n",
    "# now we have gone from just 12 to 90 features to fit for learning vs 'quality'!!\n",
    "X_overfit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's split our data up into training and testing sets. \n",
    "\n",
    "***Share your take on why do we split our data into training and testing sets?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test splits using X_overfit, y defined above.\n",
    "# setting random_state helps with repeatability on the splits, instead of diff splits each time\n",
    "# note that setting such a high test_size vs train is rarely practised, it is done so here for simulating a poor model learning here\n",
    "# default test_size in sklearn=25%. rule of thumb: train_size>test_size\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_overfit, y, test_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scale our data to standardize different feature magnitudes.\n",
    "# Relabeling scaled data as \"Z\" is common.(Z-score-Scaling topic in feature engineering lesson)\n",
    "# note that we will only \"transform()\" test, applying the learning achieved by \"fitting\" on train \n",
    "sc = StandardScaler()\n",
    "Z_train = sc.fit_transform(X_train)\n",
    "Z_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_train shape is: (1949, 90)\n",
      "y_train shape is: (1949,)\n",
      "Z_test shape is: (4548, 90)\n",
      "y_test shape is: (4548,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Z_train shape is: {Z_train.shape}')\n",
    "print(f'y_train shape is: {y_train.shape}')\n",
    "print(f'Z_test shape is: {Z_test.shape}')\n",
    "print(f'y_test shape is: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Standardizing predictors is required\n",
    "\n",
    "Let's remind ourselves of our new loss function _(with penalty)_. This is the L2 Regularization:\n",
    "\n",
    "$$MSE + \\alpha \\|\\beta\\|^2$$\n",
    "\n",
    "<details><summary>Why do you think scaling(/standardizing) the featrues is required in a regularization context?</summary>\n",
    "    Recall that the size of each <b>coefficient</b> ($\\beta$) depends on the scale of its corresponding <b>variable</b>. Our <b>penalty term</b> depends on these <b>coefficients</b>. Scaling is required so that the <b><i>regularization penalizes each variable equally fairly</b></i>.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But First: OLS(ordinary least squares/linear regression model)\n",
    "Fitting and evaluating **without** any regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Fact:*** \n",
    "\n",
    "*Why is linear regression referred to as least squares?*\n",
    "The Least Squares Regression Line is the line that makes the vertical distance from the data points to the regression line as small as possible. It's called a “least squares” because the best line of fit is one that minimizes the variance (the sum of squares of the errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the appropriate library and fit our OLS model.\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols = LinearRegression() # instantiate\n",
    "ols.fit(Z_train, y_train) # model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4069819524268312\n",
      "0.22021547039841416\n"
     ]
    }
   ],
   "source": [
    "# How does the model score (R^2) on the training and test data?\n",
    "print(ols.score(Z_train, y_train))\n",
    "print(ols.score(Z_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What do these $R^2$s tell you?</summary>\n",
    "    <b><i>Train vs test score differs by a lot - model hasn't generalized well or in other words, is overfit!</b></i>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.25749886e+01, -3.02971610e+01, -5.01103963e+01, -1.31671753e+02,\n",
       "       -6.05594065e+01, -6.69469156e+01,  1.06254306e+02,  1.17149975e+02,\n",
       "       -3.63264002e+01, -5.28038087e+01,  9.10286690e+01, -2.06348431e-01,\n",
       "       -7.51604769e-01, -1.51877470e-01, -2.69504397e-01, -6.37134850e-01,\n",
       "       -1.46230396e+00, -5.61003144e-01,  6.66040229e-01,  6.43672195e+01,\n",
       "       -1.63465852e-01, -2.26602773e-01,  4.96393198e-01,  3.61062695e-01,\n",
       "       -1.51450896e-02, -5.61980374e-02, -2.76987557e-01,  3.72765182e-02,\n",
       "       -8.77421185e-03,  2.51096607e-01,  2.84960544e+01,  3.51506311e-01,\n",
       "       -1.52413698e-01,  1.45769693e+00,  2.63918881e-01, -1.26622223e-01,\n",
       "       -2.43408218e-01,  1.68109546e-01,  8.29009668e-02,  4.50602888e-02,\n",
       "        5.04419929e+01, -9.30023851e-01, -1.83270244e-01,  1.19871740e+00,\n",
       "       -1.10452250e-01, -1.13190897e+00, -1.43140646e-01, -3.35884795e-01,\n",
       "        6.51264578e-01,  1.37020668e+02, -4.10760519e+00, -4.42930669e-01,\n",
       "        1.96024882e+00, -6.01626316e-04, -7.61869531e-02,  2.23652985e-02,\n",
       "        9.85787647e-03,  6.45538000e+01, -2.87536586e+00, -4.29803772e-01,\n",
       "        3.14710047e-01,  5.30273571e-01, -4.25254962e-01, -6.13732452e-02,\n",
       "        6.79066188e+01, -5.44968749e-01, -1.30674394e-02,  8.75062467e-01,\n",
       "       -2.23554697e-01, -1.58264426e-01, -1.07925318e+02,  1.68698251e+00,\n",
       "       -2.19329916e-01, -9.65456965e-01,  7.90225966e-03, -1.20302370e+02,\n",
       "        3.71343238e+01,  5.29574543e+01, -9.05710724e+01,  1.21392830e+00,\n",
       "       -3.62151613e-01,  3.41483119e-01,  6.92294829e-01, -2.04858829e+00,\n",
       "       -1.44178773e-01,  7.05009344e-01, -1.52124346e-01, -7.72992111e-01,\n",
       "        1.02063677e+00, -2.06348431e-01])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols.coef_ # Estimated coefficients for the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ols.coef_) # there are 90! corresponding to each our features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And Now: Ridge\n",
    "\n",
    "### Let's think about this...\n",
    "\n",
    "$$ Loss function = MSE + \\alpha\\|\\beta\\|^2$$ \n",
    "\n",
    "$$ Loss function =\\frac{1}{n}\\|\\mathbf{y} - \\mathbf{X}\\beta\\|^2 + \\alpha\\|\\beta\\|^2 $$\n",
    "\n",
    "<details><summary>What's the optimal value of $\\beta$ when $\\alpha = 0$?</summary>\n",
    "Our problem <b>reduces to OLS solution</b>, because the penalty term becomes 0!\n",
    "\n",
    "To minimize the error, above equation can be written as:\n",
    "\n",
    "$$ 0 =\\|\\mathbf{y} - \\mathbf{X}\\hat\\beta\\|^2 + 0 $$\n",
    "\n",
    "With some matrix math, we can calculate $\\hat\\beta$ as: <a href=\"https://online.stat.psu.edu/stat462/node/132/\">(link for further reference)</a>\n",
    "\n",
    "$$ \\hat{\\beta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} $$\n",
    "</details>\n",
    "\n",
    "<details><summary>What's the optimal value of $\\beta$ when $\\alpha = \\infty$?</summary>\n",
    "When $\\alpha = \\infty$, Anything besides $\\hat{\\beta} = \\mathbf{0}$ will cause our whole loss function to be <b>$\\infty$</b>. That is not ideal as we want as minimal errors as possible for our model. So, it must be that $\\hat{\\beta} = \\mathbf{0}$!\n",
    "</details>\n",
    "\n",
    "<details><summary>Some facts...</summary>\n",
    "$\\alpha$ is a constant for the <i>strength</i> of the regularization parameter. The <b>higher the value, the greater the impact</b> of this new component in the loss function. If the value was <b>zero, we would revert back to just the least squares</b> loss function. If the value was a billion, however, the residual sum of squares component would have a much smaller effect on the loss/cost than the regularization term.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can look at a traceplot to visualize above:\n",
    "Similar to visualisation on slide#15 on \"Regularization\" deck: $\\beta$ increases as $\\alpha$ decreases (x-axis left to right is reducing value of $\\alpha$)\n",
    "\n",
    "![](../imgs/ridge-trace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, so which $\\alpha$ is best?\n",
    "\n",
    "We'll primarily choose the **optimal** $\\alpha$ via **cross validation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear least squares with L2 regularization\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37708346347575306\n",
      "0.24423639703199995\n"
     ]
    }
   ],
   "source": [
    "# Instantiate.\n",
    "# alpha is the Regularization strength. we're passing 10, instead of the default 1 for stronger regularization.\n",
    "ridge_model = Ridge(alpha=10)\n",
    "\n",
    "# Fit.\n",
    "ridge_model.fit(Z_train, y_train)\n",
    "\n",
    "# Evaluate model using R2.\n",
    "print(ridge_model.score(Z_train, y_train))\n",
    "print(ridge_model.score(Z_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What do these $R^2$s tell you?</summary>\n",
    "<b><i>Train vs test score delta has definitely improved vs traditional OLS previously, but still isn't close, so still overfit to some degree.</b></i>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute-forcing the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression with \"built-in\" cross-validation (advancing from above approach w/o cv)\n",
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a list of ridge alphas to check.\n",
    "# np.logspace generates 100 values equally between 0 and 5,\n",
    "# then converts them to alphas between 10^0 and 10^5 (that is, in logscale).\n",
    "r_alphas = np.logspace(0, 5, 100)\n",
    "\n",
    "# Cross-validate over our list of ridge alphas.\n",
    "# alphas: pass an Array of alpha values to try. It is still the Regularization strength\n",
    "ridge_cv = RidgeCV(alphas=r_alphas, scoring='r2', cv=5).fit(Z_train, y_train)# fitting 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3484515362721913\n",
      "0.29488871636997716\n"
     ]
    }
   ],
   "source": [
    "print(ridge_cv.score(Z_train, y_train))\n",
    "print(ridge_cv.score(Z_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104.76157527896652"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the optimal value of alpha from ridge cv\n",
    "ridge_cv.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What do these $R^2$s tell you?</summary>\n",
    "<b><i>Train vs test score delta has definitely improved vs non-CV ridge regression</b></i>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the LASSO\n",
    "\n",
    "LASSO regression (or, a.k.a L1 regression) is largely the same as ridge, except with a **different penalty term** (_absolute value_ of coefficients instead of _squared_).\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{minimize: MSE + penalty} &= \\textstyle\\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2 + \\alpha \\sum |\\beta_j| \\\\ \\\\\n",
    "                               &= \\textstyle\\frac{1}{n}\\|\\mathbf{y} - \\hat{\\mathbf{y}}\\|^2 + \\alpha \\|\\beta\\|_1 \\\\ \\\\\n",
    "                               &= \\textstyle\\frac{1}{n}\\|\\mathbf{y} - \\mathbf{X}\\hat{\\beta}\\|^2 + \\alpha \\|\\beta\\|_1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The penalty is now made up from the **$\\mathcal{l}_1$-norm**, otherwise known as **Manhattan distance**. It is simply the absolute sum of the vector components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The LASSO traceplot looks a little different...\n",
    "We'll see it later and discuss what LASSO actually does differently from Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports similar to Ridge, this time for Lasso instead\n",
    "from sklearn.linear_model import Lasso, LassoCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== OLS =======\n",
      "0.4069819524268312\n",
      "0.22021547039841416\n",
      "\n",
      "===== Ridge ======\n",
      "0.3484515362721913\n",
      "0.29488871636997716\n"
     ]
    }
   ],
   "source": [
    "# Reminder of results from evaluations before this\n",
    "print(\" OLS \".center(18, \"=\"))# syntax: str.center(width, fillchar=' ')\n",
    "print(ols.score(Z_train, y_train))\n",
    "print(ols.score(Z_test, y_test))\n",
    "print()\n",
    "print(\" Ridge \".center(18, \"=\"))\n",
    "print(ridge_cv.score(Z_train, y_train))\n",
    "print(ridge_cv.score(Z_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a list of Lasso alphas to check.\n",
    "# np.logspace generates 100 values equally between -3 and 0,\n",
    "# then converts them to alphas between 10^-3 and 1 (that is, in logscale).\n",
    "l_alphas = np.logspace(-3, 0, 100)\n",
    "\n",
    "# Cross-validate over our list of Lasso alphas.\n",
    "lasso_cv = LassoCV(alphas=l_alphas, cv=5, max_iter=50000).fit(Z_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3366703928164062\n",
      "0.2855544326156588\n"
     ]
    }
   ],
   "source": [
    "print(lasso_cv.score(Z_train, y_train))\n",
    "print(lasso_cv.score(Z_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007564633275546291"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the optimal value of alpha from ridge cv\n",
    "lasso_cv.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge vs LASSO, what's the diff?!\n",
    "Let's check out the **coefficients** of the Lasso and Ridge models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.41082332e-04, -6.73622850e-02,  4.11979658e-03,  3.58118418e-02,\n",
       "       -7.86169814e-03,  7.96837868e-02,  4.07202575e-02, -6.40134004e-02,\n",
       "        1.67657002e-03,  2.63542441e-02,  4.89229448e-02, -5.96326152e-04,\n",
       "        3.97892719e-03, -3.38291402e-02, -4.33716705e-03,  1.37005906e-01,\n",
       "        3.07581441e-02,  3.60944933e-02, -6.74979641e-03,  6.29819066e-04,\n",
       "        8.10392864e-03,  3.73389457e-02,  6.71509211e-03,  7.15269227e-02,\n",
       "        1.12923745e-02, -1.19654956e-02, -7.94404967e-02, -1.56703211e-02,\n",
       "       -3.95481031e-03, -4.59646775e-02, -6.81012094e-02, -6.04899669e-02,\n",
       "       -3.27140260e-04,  4.46369117e-02,  7.03665877e-02, -5.18587000e-02,\n",
       "        2.51915541e-02,  3.39269447e-02,  4.95212161e-02, -2.47711305e-02,\n",
       "        3.96451349e-03,  6.67705387e-04, -3.08860641e-02,  2.35098736e-02,\n",
       "        3.55708420e-02,  3.93321581e-02,  4.86155004e-02, -3.50408563e-02,\n",
       "        5.62724938e-02,  3.71036473e-02, -2.69137546e-02, -1.40643736e-02,\n",
       "       -1.15979037e-02,  8.78107644e-05,  3.67820396e-02,  3.99396816e-02,\n",
       "       -2.57914127e-02, -7.05148813e-03, -3.56592158e-02, -6.10657197e-02,\n",
       "       -3.51923177e-02,  5.83693665e-02, -2.31654728e-01, -7.84090377e-02,\n",
       "        7.82032866e-02,  7.79743584e-02, -6.63106805e-03,  8.87411180e-02,\n",
       "       -1.25820489e-01, -5.84158917e-02,  3.98653270e-02,  4.10661964e-02,\n",
       "       -9.35719900e-02,  3.43253957e-02,  1.76071648e-02, -6.19585469e-02,\n",
       "       -5.50871730e-03,  2.52242484e-02,  4.58625104e-02, -3.51486916e-04,\n",
       "        2.81993685e-03,  5.21772772e-02,  7.11282800e-02, -2.51823938e-02,\n",
       "       -4.76600158e-02,  9.43845468e-02,  1.79373582e-02,  1.14815172e-01,\n",
       "        2.83989485e-02, -5.96326152e-04])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_cv.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.        , -0.        , -0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        0.16835564,  0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        ,  0.        , -0.        ,  0.06311839,  0.        ,\n",
       "       -0.        , -0.05120041, -0.        , -0.        , -0.07081332,\n",
       "       -0.16039109, -0.        , -0.        , -0.        ,  0.00492753,\n",
       "       -0.01006693,  0.        ,  0.        ,  0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        ,  0.01658083,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "       -0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        ,  0.00238066, -0.28613203, -0.        ,  0.        ,\n",
       "        0.13542067,  0.        ,  0.25586092, -0.06659136, -0.        ,\n",
       "        0.        ,  0.        , -0.05394452,  0.        ,  0.        ,\n",
       "       -0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.01057707,  0.        , -0.        ,\n",
       "        0.11516198,  0.        ,  0.29526458,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_cv.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By comparing the coef from ridge vs lasso, we can conclude that Lasso is effective in use for **feature selection** as it shrinks the coefs of those features that are **not relevant or not a good predictor** of response, to zero. And, only leaves the strongly correlated features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cliffsnotes: L.A.S.S.O.\n",
    "LASSO is actually an acronym:\n",
    "\n",
    "* **L**east\n",
    "* **A**bsolute\n",
    "* **S**hrinkage and\n",
    "* **S**election\n",
    "* **O**perator\n",
    "\n",
    "**SHRINKAGE**: Higher $\\alpha$ \"shrinks\" $\\beta$ towards $\\mathbf{0}$.\n",
    "\n",
    "**SELECTION**: Higher $\\alpha$ zeros out small $\\beta$s.\n",
    "\n",
    "![](../imgs/lasso-trace.svg)\n",
    "\n",
    "## Mathematical interpretation of what LASSO was doing here\n",
    "Mathematically, in Lasso, We're doing an optimization problem, so actually, this $\\alpha$ is a **Lagrange multiplier**. This means that optimizing our loss function:\n",
    "\n",
    "$$ \\|\\mathbf{y} - \\mathbf{X}\\beta\\|^2 + \\alpha\\|\\beta\\|_1 $$\n",
    "\n",
    "is equivalent to optimizing the **constrained loss function**:\n",
    "\n",
    "$$ \\|\\mathbf{y} - \\mathbf{X}\\beta\\|^2 \\quad \\text{such that} \\quad \\|\\beta\\|_1 \\le t $$ _(we explained this concept with a constant, s while covering the intuition deck)_\n",
    "\n",
    "## [TRY OUT THIS APP!](https://timothykbook.shinyapps.io/RegularizationPlot/) \n",
    "to see for yourself on changing $\\alpha$ or $\\lambda$'s impact on coefficient $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now onto Regularizing Logistic Regression\n",
    "_Note: we'll cover more on logistic regression in a \"**classification**\" machine learning context in future lessons_\n",
    "For now, we just take it that it is yet another variation of linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at the LogisticRegression documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'lbfgs',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noticed the parameter, `penalty`? Regularization is the hidden default for logistic regression!\n",
    "Unless regularization is necessary, **it should not be done!!** (It makes interpreting the coefficients much more difficult.) In newer version of Scikit-Learn, 0.21 and higher, you can disable regularization by passing in `penalty='none'`! \n",
    "\n",
    "If you _do_ want to regularize, note that there is a much friendlier `LogisticRegressionCV` we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random classification problem\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.4405465  -0.77063522 -0.48266557 ...  1.84166626 -0.91101634\n",
      "   0.96037298]\n",
      " [-0.77339715 -0.21886082 -2.11639665 ... -0.55028366  0.63370087\n",
      "   0.71023287]\n",
      " [-2.61619714 -0.90948965 -3.21558029 ...  0.16634024  0.51847037\n",
      "   0.03842324]\n",
      " ...\n",
      " [-0.06833379 -1.28884401 -0.77598478 ...  0.69427821 -0.36789168\n",
      "   1.14419823]\n",
      " [ 0.25399511 -0.61331032 -0.23905231 ... -0.69668623 -0.10576577\n",
      "  -2.77347662]\n",
      " [-0.25432662  0.74350343 -0.14684419 ...  0.17122285  0.47578506\n",
      "   0.3489792 ]] [1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0\n",
      " 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0\n",
      " 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1\n",
      " 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0\n",
      " 1 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0\n",
      " 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0\n",
      " 0 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0\n",
      " 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1\n",
      " 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 1\n",
      " 0 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0\n",
      " 0 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0\n",
      " 0 1 1 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0\n",
      " 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1\n",
      " 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0\n",
      " 1 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1\n",
      " 1 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 1\n",
      " 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0\n",
      " 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 0\n",
      " 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1\n",
      " 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 1 0\n",
      " 0 0 1 1 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 0 0\n",
      " 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1\n",
      " 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0\n",
      " 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0\n",
      " 0 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "# create dataset with 1000 rows, 200 cols only 15 of which are informative features-->to simulate for a noisy dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=200, n_informative=15, random_state=123)\n",
    "print(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "# scaling standardization\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9306666666666666\n",
      "0.636\n"
     ]
    }
   ],
   "source": [
    "# logistic regression model training\n",
    "# C: Inverse of regularization strength (1/alpha), smaller values specify stronger regularization\n",
    "# here 1e9 or 10^9: large value to depreciate regularization\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_sc, y_train)\n",
    "\n",
    "# See the gap in training vs testing metric! Overfit!\n",
    "print(logreg.score(X_train_sc, y_train))\n",
    "print(logreg.score(X_test_sc, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9306666666666666\n",
      "0.636\n"
     ]
    }
   ],
   "source": [
    "# we arrive at the same outcome by setting penalty='none'\n",
    "logreg_pen_none = LogisticRegression(penalty='none')\n",
    "logreg_pen_none.fit(X_train_sc, y_train)\n",
    "\n",
    "# See the gap in training vs testing metric! Overfit!\n",
    "print(logreg_pen_none.score(X_train_sc, y_train))\n",
    "print(logreg_pen_none.score(X_test_sc, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8146666666666667\n",
      "0.812\n"
     ]
    }
   ],
   "source": [
    "# logistic regression_cv (5-fold cv) model training\n",
    "# sklearn recommendation on solver from documentation: \"liblinear\" for small datasets,\n",
    "# if not specified, default solver \"lbgs\" is the algorithm to use in optimization problem with \"l2\" penalty \n",
    "logreg_cv = LogisticRegressionCV(cv=5, penalty=\"l1\", solver=\"liblinear\")\n",
    "logreg_cv.fit(X_train_sc, y_train)\n",
    "\n",
    "# we don't see overfitting now! training-testing metrics are close\n",
    "print(logreg_cv.score(X_train_sc, y_train))\n",
    "print(logreg_cv.score(X_test_sc, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04641589])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the optimal value of \"C\" for class 1, inverse of \"alpha\"\n",
    "logreg_cv.C_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression CV sklearn [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elephant in the Room: Categorical Variables\n",
    "Think about it. What does it mean to scale a binary variable? _(Our response, `y` in a binary classification problem either takes the value 1 or 0, this is also referred to as class 1 or class 0.)_ How about a categorical variable dummified into several columns? What does it mean to shrink the coefficients associated with these columns? What happens if the LASSO zeros out one category, but not others? Not sure, either.\n",
    "\n",
    "It turns out, it's **not a great idea to combine scaling and categorical data**. It often just doesn't make sense to do. This is true for all algorithms where we need to scale. So what do we do? A few options:\n",
    "\n",
    "* Set separate regularization parameters for each x-variable (not available in Scikit-Learn).\n",
    "* Carry out the _grouped LASSO_ technique (not available in Scikit-Learn, and doesn't solve all problems anyway).\n",
    "* Manually decide on a scale for these variables (time consuming, unintuitive, still doesn't work with regularization).\n",
    "* Don't use those variables (but you want them!).\n",
    "* Just do it anyway. Who knows, it'll probably be fine! (¯\\_(ツ)_/¯)\n",
    "\n",
    "In the bonus section, we'll explore `Elastic Net` regularization using the same classification dataset but **without scaling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Notes\n",
    "- The $\\alpha$ hyperparameter for regularization is **unrelated** to significance level in hypothesis testing.\n",
    "- In certain resources, including [ISLR](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf), you'll see that $\\lambda$ is used instead of $\\alpha$ for regularization strength.\n",
    "- We must standardize before regularizing, but regularization and standardization are not the same things!\n",
    "- **FROM NOW ON, YOU MUST PAY ATTENTION TO REGULARIZATION WHEN CONDUCTING LOGISTIC REGRESSION!!!**\n",
    "- Ridge regression is sometimes called **weight decay**, but usually only when regularizing neural networks.\n",
    "- LASSO regression is sometimes called **basis pursuit**, but that's very old fashioned.\n",
    "- The y-intercept for these models are not regularized.\n",
    "\n",
    "## Recap\n",
    "- Regularization is used when evidence suggests our model is suffering from high error due to variance.\n",
    "- Evidence to suggest our model suffers from high error due to variance includes substantially better performance on our training set than our testing set.\n",
    "- LASSO tends to be \"more brutal\" than Ridge regularization in that it will zero out coefficients.\n",
    "- If you want to combine LASSO and Ridge regularization, there is a technique called \"ElasticNet\" that does exactly this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet Regression (bonus)\n",
    "\n",
    "---\n",
    "\n",
    "Can't decide?\n",
    "\n",
    "![](../imgs/why-not-both.jpg)\n",
    "\n",
    "The Elastic Net combines the Ridge and Lasso penalties.  It adds *both* penalties to the loss function:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "SSE + Ridge + Lasso &=& \\sum_{i=1}^n \\left(y_i - \\hat{y}_i\\right)^2 + \\alpha\\left[\\rho\\sum_{j=1}^p |\\beta_j| + (1-\\rho)\\sum_{j=1}^p \\beta_j^2\\right] \\\\\n",
    "&=& \\|\\mathbf{y} - \\mathbf{X}\\beta\\|^2 + \\alpha\\left(\\rho\\|\\beta\\|_1 + (1 - \\rho)\\|\\beta\\|^2\\right)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "\n",
    "In the elastic net, the effect of the ridge versus the lasso is balanced by the $\\rho$ (L1 ratio) parameter.  It is the ratio of Lasso penalty to Ridge penalty and must be between zero and one.\n",
    "\n",
    "`ElasticNet` in sklearn has two parameters:\n",
    "- `alpha`: the regularization strength.\n",
    "- `l1_ratio`: the amount of L1 vs L2 penalty (i.e., $\\rho$). An l1_ratio of **0 is equivalent to the Ridge**, whereas an **l1_ratio of 1 is equivalent to the Lasso**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the coefficients with both alpha values and an l1_ratio of 0.5 (note that an l1_ration of 0.5 is the default, so as to balance between ridge and lasso, so we don't need to explicitly declare l1_ratio=0.5).\n",
    "- Using a $\\rho$ value below 0.05 can empirically cause issues in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21246113948141954\n",
      "0.21368840628470087\n"
     ]
    }
   ],
   "source": [
    "# Set up a list of alphas to check.\n",
    "enet_alphas = np.linspace(0.5, 1.0, 100)# Return evenly spaced numbers over a specified interval\n",
    "\n",
    "# Instantiate model.\n",
    "enet_model = ElasticNetCV(alphas=enet_alphas, cv=5)\n",
    "\n",
    "# Fit model using optimal alpha.\n",
    "enet_model = enet_model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions.\n",
    "enet_model_preds = enet_model.predict(X_test)\n",
    "enet_model_preds_train = enet_model.predict(X_train)\n",
    "\n",
    "# Evaluate model.\n",
    "print(enet_model.score(X_train, y_train))\n",
    "print(enet_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is the optimal value of alpha.\n",
    "enet_model.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "state": {
    "03860d2d80204ca295d01e93e8e99474": {
     "views": [
      {
       "cell_index": 41
      }
     ]
    },
    "b535fb165fa343b297ba42fb4a55c6fa": {
     "views": [
      {
       "cell_index": 34
      }
     ]
    },
    "f5d5ef714eee4c61b085a3bb6b96cd73": {
     "views": [
      {
       "cell_index": 55
      }
     ]
    },
    "fdc5e91596ea49fa84bf4aed6d37b849": {
     "views": [
      {
       "cell_index": 47
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
