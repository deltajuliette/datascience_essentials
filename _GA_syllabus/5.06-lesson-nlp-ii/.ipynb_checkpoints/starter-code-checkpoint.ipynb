{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# NLP II: `CountVectorizer`, `TfidfVectorizer`, and Modeling\n",
    "\n",
    "_Authors: Dave Yerrington (SF), Justin Pounders (ATL), Riley Dallas (ATX), Matt Brems (DC), Noelle Brown (DEN)_\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://snag.gy/uvESGH.jpg\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "### $$\n",
    "\\begin{eqnarray*}\n",
    "\\textbf{Fun Fact:  } \\text{Word Clouds} &\\neq& \\text{Data Science}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "[If you want to generate a word cloud in the shape of something **for art only**, check here.](https://medium.com/hackernoon/what-real-fake-news-says-about-obamas-presidency-4bf42be71ff1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Objectives\n",
    "---\n",
    "\n",
    "- Extract features from unstructured text by fitting and transforming with `CountVectorizer` and `TfidfVectorizer`.\n",
    "- Describe how CountVectorizers and TF-IDFVectorizers work.\n",
    "- Understand `stop_words`, `max_features`, `min_df`, `max_df`, and `ngram_range`.\n",
    "- Implement `CountVectorizer` and `TfidfVectorizer` in a spam classification model.\n",
    "- Use `GridSearchCV` and `Pipeline` with `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "# Import CountVectorizer and TFIDFVectorizer from feature_extraction.text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reminder of the Data Science Process\n",
    "1. Define problem.\n",
    "2. Gather data.\n",
    "3. Explore data.\n",
    "    - Yes, we can still do EDA with text data!\n",
    "    - We also have to pre-process our text data to prepare it for modeling.\n",
    "4. Model with data.\n",
    "5. Evaluate model.\n",
    "6. Answer problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Text Feature Extraction\n",
    "\n",
    "The models we've learned, like linear regression, logistic regression, and k-nearest neighbors, take in an `X` and a `y` variable.\n",
    "- `X` is a matrix/dataframe of real numbers.\n",
    "- `y` is a vector/series of real numbers.\n",
    "\n",
    "Text data (also called natural language data) is not already organized as a matrix or vector of real numbers. We say that this data is **unstructured**.\n",
    "\n",
    "> This lesson will focus on how to transform our unstructured text data into a numeric `X` matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classification Model\n",
    "\n",
    "One common application of NLP is predicting \"spam\" vs. \"ham,\" or \"spam\" vs. \"not spam.\"\n",
    "\n",
    "Can we predict real vs. promotional texts just based on what is written?\n",
    "\n",
    "> This data set was taken from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data.\n",
    "spam = pd.read_csv('./data/SMSSpamCollection',\n",
    "                 sep='\\t',\n",
    "                 names=['label', 'message'])\n",
    "\n",
    "# Check out first five rows.\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic terminology\n",
    "\n",
    "---\n",
    "\n",
    "- A collection of text is a **document**. \n",
    "    - You can think of a document as a row in your feature matrix.\n",
    "- A collection of documents is a **corpus**. \n",
    "    - You can think of your full dataframe as the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>In this specific example, what is a document?</summary>\n",
    "    \n",
    "- Each text message in our data set is one document. \n",
    "- There are 5,572 documents in our corpus.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get our data\n",
    "---\n",
    "\n",
    "Convert ham/spam into binary labels:\n",
    "- 0 for ham\n",
    "- 1 for spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label column\n",
    "spam['label'] = spam['label'].map({'ham': 0, 'spam': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up our data for modeling:\n",
    "- `X` will be the `message` column. **NOTE**: `CountVectorizer` requires a vector, so make sure you set `X` to be a `pandas` Series, **not** a DataFrame.\n",
    "- `y` will be the `label` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spam['message']\n",
    "y = spam['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.865937\n",
       "1    0.134063\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what we need to check in a classification problem.\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into the training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)\n",
    "\n",
    "#Stratify because outcomes are not equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "\n",
    "Let's review some of the pre-processing steps for text data:\n",
    "\n",
    "- Remove special characters\n",
    "- Tokenizing\n",
    "- Lemmatizing/Stemming\n",
    "- Stop word removal\n",
    "\n",
    "`CountVectorizer` actually can do a lot of this for us! It is important to keep these steps in mind in case you want to change the default methods used for each of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `CountVectorizer`\n",
    "---\n",
    "\n",
    "The easiest way for us to convert text data into a structured, numeric `X` dataframe is to use `CountVectorizer`.\n",
    "\n",
    "- **Count**: Count up how many times a token is observed in a given document.\n",
    "- **Vectorizer**: Create a column (also known as a vector) that stores those counts.\n",
    "\n",
    "![](./images/countvectorizer2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a CountVectorizer.\n",
    "cvec = CountVectorizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the vectorizer on our corpus.\n",
    "X_train_vec = cvec.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3733x6935 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 49422 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vec\n",
    "# A sparse matrix comprises mostly of zero values; By only storing the non-zero elements in our computer's memory, we can save a lot of space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Here's how a sparse matrix looks like\n",
    "print(X_train_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Printing the first row\n",
    "print(X_train_vec.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "853 am\n",
      "2888 going\n",
      "3155 home\n",
      "3407 its\n",
      "3977 me\n",
      "4368 now\n",
      "4462 only\n",
      "6754 with\n",
      "6885 yesterday\n"
     ]
    }
   ],
   "source": [
    "# Let's print the words that are actually non-zero in the first row\n",
    "for index, element in enumerate(X_train_vec.toarray()[0]):\n",
    "    if element !=0:\n",
    "        print(index, cvec.get_feature_names()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/countvectorizer.png\" alt=\"drawing\" width=\"750\"/>\n",
    "\n",
    "[Source](https://towardsdatascience.com/nlp-learning-series-part-2-conventional-methods-for-text-classification-40f2839dd061)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have unstructured text data, there is a lot of information in that text data.\n",
    "- When we force unstructured text data to follow a \"spreadsheet\" or \"dataframe\" structure, we might lose some of that information.\n",
    "- For example, CountVectorizer creates a vector (column) for each token and counts up the number of occurrences of each token in each document.\n",
    "\n",
    "Our tokens are now stored as a **bag-of-words**. This is a simplified way of looking at and storing our data. \n",
    "- Bag-of-words representations discard grammar, order, and structure in the text but track occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we could fit a model (like a logistic regression model or $k$-nearest neighbors model) using our transformed data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What might be some of the advantages of using this bag-of-words approach when modeling?</summary>\n",
    "\n",
    "- Efficient to store.\n",
    "- Efficient to model.\n",
    "- Keeps a decent amount of information.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What might be some of the disadvantages of using this bag-of-words approach when modeling?</summary>\n",
    "\n",
    "- Since bag-of-words models discard grammar, order, structure, and context, we lose a decent amount of information.\n",
    "- Phrases like \"not bad\" or \"not good\" won't be interpreted properly.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, let's examine some of the different hyperparameters of `CountVectorizer`:\n",
    "- `stop_words`\n",
    "- `max_features`, `max_df`, `min_df`\n",
    "- `ngram_range`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Remind me: what is a hyperparameter?</summary>\n",
    "\n",
    "- A hyperparameter is a built-in option that affects our model, but our model cannot learn these from our data!\n",
    "- Examples of hyperparameters include:\n",
    "    - the value of $k$ and the distance metric in $k$-nearest neighbors,\n",
    "    - our regularization constants $\\alpha$ or $C$ in linear and logistic regression.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "---\n",
    "\n",
    "Some words are so common that they may not provide legitimate information about the $Y$ variable we're trying to predict.\n",
    "\n",
    "Let's see what our top-occurring words are right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert X_train into a DataFrame.\n",
    "# We will not actually use this for modeling,\n",
    "# this is just to visualize what is happening\n",
    "X_train_df = pd.DataFrame(X_train_vec.toarray(), \n",
    "                          columns=cvec.get_feature_names())\n",
    "\n",
    "# plot top occuring words\n",
    "X_train_df.sum().sort_values(ascending=False).head(10).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What do you think of the top 10 words?</summary>\n",
    "\n",
    "- These are pretty much all stop words!\n",
    "- Using a bag-of-words approach with these words in there might not add anything meaningful to our analysis.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'fire', 'five', 'least', 'not', 'now', 'through', 'else', 'us', 'every', 'get', 'whose', 'myself', 'another', 'before', 'since', 'anyway', 'four', 'back', 'against', 'etc', 'much', 'always', 'for', 'below', 'at', 'whenever', 'to', 'whoever', 'top', 'towards', 'hers', 'we', 'whereafter', 'itself', 'next', 'one', 'via', 'ten', 'will', 'become', 'most', 'my', 'with', 'together', 'cant', 'could', 'con', 'whither', 'any', 'ie', 'nine', 'or', 'seems', 'but', 'put', 'several', 'both', 'amount', 'less', 'however', 'were', 'whole', 'formerly', 'move', 'if', 'above', 'part', 'side', 'while', 'in', 'ever', 'about', 'of', 'what', 'further', 'thence', 'ourselves', 'was', 'nothing', 'amongst', 'sincere', 'without', 'please', 'had', 'himself', 'yours', 'cry', 'six', 'thereby', 'hence', 'a', 'give', 'bill', 'seeming', 'same', 'others', 'third', 'who', 're', 'around', 'inc', 'yourselves', 'made', 'yet', 'he', 'might', 'because', 'bottom', 'thereupon', 'beside', 'neither', 'system', 'when', 'either', 'be', 'indeed', 'describe', 'ltd', 'many', 'they', 'each', 'anyone', 'only', 'though', 'them', 'her', 'elsewhere', 'whereby', 'whatever', 'former', 'therefore', 'go', 'became', 'sometimes', 'interest', 'the', 'everyone', 'nobody', 'which', 'their', 'sixty', 'wherein', 'almost', 'latterly', 'under', 'up', 'and', 'anything', 'you', 'couldnt', 'thin', 'must', 'why', 'some', 'after', 'mine', 'how', 'also', 'done', 'fifteen', 'as', 'nevertheless', 'noone', 'eg', 'perhaps', 'on', 'whereas', 'it', 'due', 'our', 'between', 'de', 'everywhere', 'she', 'thus', 'even', 'hereby', 'yourself', 'very', 'ours', 'hereupon', 'should', 'seem', 'per', 'thick', 'eight', 'herein', 'hasnt', 'twenty', 'find', 'un', 'two', 'show', 'from', 'toward', 'seemed', 'few', 'within', 'here', 'thru', 'have', 'keep', 'see', 'someone', 'fifty', 'over', 'during', 'twelve', 'everything', 'whom', 'do', 'anyhow', 'still', 'where', 'hereafter', 'its', 'meanwhile', 'becoming', 'never', 'whence', 'wherever', 'well', 'him', 'thereafter', 'first', 'somewhere', 'name', 'empty', 'those', 'are', 'once', 'would', 'behind', 'can', 'me', 'found', 'out', 'down', 'has', 'his', 'anywhere', 'mill', 'am', 'all', 'last', 'hundred', 'until', 'no', 'there', 'too', 'themselves', 'may', 'throughout', 'nowhere', 'three', 'alone', 'although', 'afterwards', 'except', 'among', 'moreover', 'therein', 'so', 'these', 'herself', 'call', 'co', 'amoungst', 'eleven', 'serious', 'take', 'detail', 'enough', 'somehow', 'into', 'than', 'more', 'besides', 'fill', 'mostly', 'becomes', 'beyond', 'again', 'otherwise', 'i', 'that', 'none', 'along', 'often', 'sometime', 'whereupon', 'then', 'upon', 'latter', 'across', 'nor', 'other', 'something', 'an', 'own', 'full', 'forty', 'this', 'already', 'being', 'off', 'namely', 'been', 'whether', 'your', 'is', 'cannot', 'onto', 'by', 'such', 'rather', 'beforehand', 'front'})\n"
     ]
    }
   ],
   "source": [
    "# Let's look at sklearn's stopwords.\n",
    "print(CountVectorizer(stop_words = 'english').get_stop_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` gives you the option to eliminate stopwords from your corpus when instantiating your vectorizer.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "```\n",
    "\n",
    "You can optionally pass your own list of stopwords that you'd like to remove.\n",
    "```python\n",
    "cvec = CountVectorizer(stop_words=['list', 'of', 'words', 'to', 'stop'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vocabulary size\n",
    "\n",
    "---\n",
    "One downside to `CountVectorizer` is the size of its vocabulary (`cvec.get_feature_names()`) can get really large. We're creating one column for every unique token in your corpus of data!\n",
    "\n",
    "There are three hyperparameters to help you control this.\n",
    "\n",
    "1. You can set `max_features` to only include the $N$ most popular vocabulary words in the corpus.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(max_features=1_000) # Only the top 1,000 words from the entire corpus will be saved\n",
    "```\n",
    "\n",
    "2. You can tell `CountVectorizer` to only consider words that occur in **at least** some number of documents.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(min_df=2) # A word must occur in at least two documents from the corpus\n",
    "```\n",
    "\n",
    "3. Conversely, you can tell `CountVectorizer` to only consider words that occur in **at most** some percentage of documents.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(max_df=.98) # Ignore words that occur in > 98% of the documents from the corpus\n",
    "```\n",
    "\n",
    "Both `max_df` and `min_df` can accept either an integer or a float.\n",
    "- An integer tells us the number of documents.\n",
    "- A float tells us the percentage of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why might we want to control these vocabulary size hyperparameters?**\n",
    "    \n",
    "- If we have too many features, our models may take a **very** long time to fit.\n",
    "- Control for overfitting/underfitting.\n",
    "- Words in 99% of documents or words occuring in only one document might not be very informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Range\n",
    "---\n",
    "\n",
    "`CountVectorizer` has the ability to capture $n$-word phrases, also called $n$-grams. Consider the following:\n",
    "\n",
    "> The quick brown fox jumped over the lazy dog.\n",
    "\n",
    "In the example sentence, the 2-grams are:\n",
    "- 'the quick'\n",
    "- 'quick brown'\n",
    "- 'brown fox'\n",
    "- 'fox jumped'\n",
    "- 'jumped over'\n",
    "- 'over the'\n",
    "- 'the lazy'\n",
    "- 'lazy dog'\n",
    "\n",
    "The `ngram_range` determines what $n$-grams should be considered as features.\n",
    "\n",
    "```python\n",
    "cvec = CountVectorizer(ngram_range=(1,2)) # Captures every 1-gram and every 2-gram\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many 3-grams would be generated from the phrase \"the quick brown fox jumped over the lazy dog?\"**\n",
    "\n",
    "- Seven 3-grams.\n",
    "    - 'the quick brown'\n",
    "    - 'quick brown fox'\n",
    "    - 'brown fox jumped'\n",
    "    - 'fox jumped over'\n",
    "    - 'jumped over the'\n",
    "    - 'over the lazy'\n",
    "    - 'the lazy dog'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Why might we want to change ngram_range to something other than (1,1)?</summary>\n",
    "\n",
    "- We can work with multi-word phrases like \"not good\" or \"very hot.\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "---\n",
    "\n",
    "We may want to test lots of different values of hyperparameters in our CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Is CountVectorizer an estimator or a transformer?</summary>\n",
    "    \n",
    "- A transformer.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Why do we need a pipeline to GridSearch over our CountVectorizer hyperparameters?</summary>\n",
    "    \n",
    "- The CountVectorizer is a transformer.\n",
    "- Transformers have .fit() and .transform() methods, but cannot do .predict().\n",
    "- In order to GridSearch over hyperparameters, we need some way to score our model performance.\n",
    "- A pipeline stacks together one or more transformers with an estimator at the end. The estimator allows us to .predict() and get a score!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Go until jurong point, crazy.. Available only ...\n",
      "1                        Ok lar... Joking wif u oni...\n",
      "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3    U dun say so early hor... U c already then say...\n",
      "4    Nah I don't think he goes to usf, he lives aro...\n",
      "Name: message, dtype: object 0    0\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    0\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X[:5], y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline accuracy\n",
    "\n",
    "We need to calculate baseline accuracy in order to tell if our model is better than null model (predicting the plurality class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.865688\n",
       "1    0.134312\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes\n",
    "\n",
    "I am only going to scratch the surface of this algorithm. For more details on it, check out the resources in the [README](../README.md)!\n",
    "\n",
    "Naïve Bayes relies on [Bayes theorem](https://www.mathsisfun.com/data/bayes-theorem.html), which we will officially cover in a later week. Right now, just know that we rely on our prior knowledge to calculate probabilities.\n",
    "\n",
    "In order to understand Bayes theorem, we need to remember conditional probabilities. A quick example to understand this intuitively:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>If you pick a card from a standard 52-card deck, what is the probability of drawing a queen given the card is a heart?</summary>\n",
    "    \n",
    "I have told you the condition: that the card is a heart. Therefore, we only have 13 options to choose from, since there are 13 hearts in a deck of cards. Out of these, only 1 card is a queen (there is one queen in each suit), so the probability of drawing a queen given the card is a heart is 1/13.\n",
    "    \n",
    "It is important to note here that the probability of drawing a queen given the card is a heart is not the same as the probability of drawing a heart given the card is a queen! This would be 1/4.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know $P(B|A)$, Bayes theorem allows us to calculate the probability of $P(A|B)$ by relating the probability of $P(A|B)$ to $P(B|A)$. \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Bayes' Theorem: } P(A|B) &=& \\frac{P(B|A)P(A)}{P(B)}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "- Let $A$ be that a message is spam.\n",
    "- Let $B$ represent the words used in the message.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Bayes' Theorem: } P(A|B) &=& \\frac{P(B|A)P(A)}{P(B)} \\\\\n",
    "\\Rightarrow P(\\text{message is spam}|\\text{words in message}) &=& \\frac{P(\\text{words in message}|\\text{message is spam})P(\\text{message is spam})}{P(\\text{words in message})}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "We want to calculate the probability that a post is spam **given** the words that are in the message! Our model can learn this from the training data.\n",
    "\n",
    "**Naïve Bayes** makes the assumption that all features are independent of one another (this is why it is called *naïve*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Why is this assumption not realistic with our data?</summary>\n",
    "    \n",
    "Text data is never independent! Certain words can change the context of a sentence when used with other words. The way language works, we have words that are more or less likely to follow other words.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite this assumption not being realistic with NLP data, we still use Naïve Bayes pretty frequently.\n",
    "- It's a very fast modeling algorithm (which is great especially when we have lots of features and/or lots of data!).\n",
    "- It is often an excellent classifier, outperforming more complicated models.\n",
    "\n",
    "There are three common types of Naive Bayes models: Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes.\n",
    "- How do we pick which of the three models to use? It depends on our $X$ variable.\n",
    "    - [Bernoulli Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB): when we have 0/1 variables.\n",
    "    - [Multinomial Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB): when our variables are positive integers.\n",
    "    - [Gaussian Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB): when our features are Normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a pipeline up with two stages:\n",
    "# 1. CountVectorizer (transformer)\n",
    "# 2. Multinomial Naive Bayes (estimator)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `GridSearchCV`\n",
    "---\n",
    "\n",
    "At this point, you could use your `pipeline` object as a model:\n",
    "\n",
    "```python\n",
    "# Estimate how your model will perform on unseen data\n",
    "cross_val_score(pipe, X_train, y_train, cv=3).mean() \n",
    "\n",
    "# Fit your model\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Training score\n",
    "pipe.score(X_train, y_train)\n",
    "\n",
    "# Test score\n",
    "pipe.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "Since we want to tune over the `CountVectorizer`, we'll load our `pipeline` object into `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit: 2000, 3000, 4000, 5000\n",
    "# Minimum number of documents needed to include token: 2, 3\n",
    "# Maximum number of documents needed to include token: 90%, 95%\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [2_000, 3_000, 4_000, 5_000],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "\n",
    "gs = GridSearchCV(pipe, # what object are we optimizing?\n",
    "                  param_grid=pipe_params, # what parameters values are we searching?\n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How many models are we fitting here?</summary>\n",
    "\n",
    "- 4 max_features\n",
    "- 2 min_df\n",
    "- 2 max_df\n",
    "- 2 ngram_range\n",
    "- 5-fold CV\n",
    "- 4 * 2 * 2 * 2 * 5 = 160 models\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             param_grid={'cvec__max_df': [0.9, 0.95],\n",
       "                         'cvec__max_features': [2000, 3000, 4000, 5000],\n",
       "                         'cvec__min_df': [2, 3],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2)]})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit GridSearch to training data.\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9852668942077514\n"
     ]
    }
   ],
   "source": [
    "# What's the best score?\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.991159924993303"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score model on training set.\n",
    "gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9847743338771071"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score model on testing set.\n",
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Is accuracy the best score here?</summary>\n",
    "\n",
    "Since we are classifying whether or not a message is spam, I care more about minimizing false positives here (maximizing for specificity). I prefer for my important emails to go to my inbox (true negatives) and potentially have a few spam messages go to my inbox (false negative) than miss an important email that was incorrectly classified as spam (false positive). \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 1 0]\n",
      "1587 5 23 224\n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "preds = gs.predict(X_test)\n",
    "print(preds)\n",
    "\n",
    "# Save confusion matrix values\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View confusion matrix\n",
    "plot_confusion_matrix(gs, X_test, y_test, cmap='Blues', values_format='d');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity: 0.996859296482412\n"
     ]
    }
   ],
   "source": [
    "# Calculate the specificity\n",
    "\n",
    "spec = tn / (tn + fp)\n",
    "\n",
    "print('Specificity:', spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Congratulations! We've used `CountVectorizer` to transform our text data into something we can pass into a model.\n",
    "\n",
    "But what if we want to do something more than just count up the occurrence of each token?\n",
    "\n",
    "## Term Frequency-Inverse Document Frequency (TF-IDF) Vectorizer\n",
    "\n",
    "---\n",
    "\n",
    "When modeling, which word do you think tends to be the most helpful?\n",
    "- Words that are common across all documents.\n",
    "- Words that are rare across all documents.\n",
    "- Words that are rare across some documents, and common across some documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Answer:</summary>\n",
    "\n",
    "- Words that are common in certain documents but rare in other documents tend to be more informative than words that are common in all documents or rare in all documents.\n",
    "- Example: If we were examining poetry over time, the word \"thine\" might be common in some documents but rare in most documents. The word \"thine\" is probably pretty informative in this case.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is a score that tells us which words are important to one document, relative to all other documents. Words that occur often in one document but don't occur in many documents contain more predictive power.\n",
    "\n",
    "Variations of the TF-IDF score are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n",
    "- If you want to see how it can be calculated, check out [the Wikipedia page](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) and [`sklearn`](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting) page.\n",
    "\n",
    "<img src=\"./images/tfidfvectorizer.png\" alt=\"drawing\" width=\"750\"/>\n",
    "\n",
    "[Source](https://towardsdatascience.com/nlp-learning-series-part-2-conventional-methods-for-text-classification-40f2839dd061)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Practice Using the `TfidfVectorizer`\n",
    "\n",
    "---\n",
    "\n",
    "`sklearn` provides a TF-IDF vectorizer that works similarly to the CountVectorizer.\n",
    "- The arguments `stop_words`, `max_features`, `min_df`, `max_df`, and `ngram_range` also work here.\n",
    "\n",
    "As you did above, instantiate the default `TfidfVectorizer`, then fit the spam and ham data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the transformer.\n",
    "tvec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the top words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert training data to dataframe\n",
    "X_train_df = pd.DataFrame(tvec.fit_transform(X_train).todense(), \n",
    "                          columns=tvec.get_feature_names())\n",
    "\n",
    "# plot top occuring words\n",
    "X_train_df.sum().sort_values(ascending=False).head(10).plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Using the `TfidfVectorizer`\n",
    "\n",
    "Let's set up a pipeline using tf-idf and Multinomial Naive Bayes.\n",
    "\n",
    "<details><summary>What's the problem with this?</summary>\n",
    "\n",
    "- Technically, we are supposed to have positive integers to use Multinomial Naive Bayes. Tf-idf does not give us positive integers.\n",
    "- However, it will still work. Even the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) says \"The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a pipeline with tf-idf vectorizer and multinomial naive bayes\n",
    "\n",
    "pipe_tvec = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit: 2000, 3000, 4000, 5000\n",
    "# No stop words and english stop words\n",
    "# Check (individual tokens) and also check (individual tokens and 2-grams).\n",
    "\n",
    "pipe_tvec_params = {\n",
    "    'tvec__max_features': [2_000, 3_000, 4_000, 5_000],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "    'tvec__ngram_range': [(1,1), (1,2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Instantiate GridSearchCV.\n",
    "gs_tvec = GridSearchCV(pipe_tvec, # what object are we optimizing?\n",
    "                        param_grid = pipe_tvec_params, # what parameters values are we searching?\n",
    "                        cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('nb', MultinomialNB())]),\n",
       "             param_grid={'tvec__max_features': [2000, 3000, 4000, 5000],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'tvec__stop_words': [None, 'english']})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Fit GridSearch to training data.\n",
    "gs_tvec.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9884811143852129"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score model on training set.\n",
    "gs_tvec.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9815116911364872"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score model on testing set.\n",
    "gs_tvec.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "y_pred = gs_tvec.predict(X_test)\n",
    "\n",
    "# Save confusion matrix values\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "# Calculate the specificity\n",
    "spec = tn / (tn + fp)\n",
    "print('Specificity:', spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (BONUS) How is the information from vectorizers stored efficiently?\n",
    "\n",
    "When you CountVectorize the training text messages, you get 3,733 rows and 6,935 features... this is 25,888,355 entries. That's a lot of data to store in a dataframe!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>How many of these values are zero?</summary>\n",
    "\n",
    "- Over 99% of all values are zero!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of storing all those zeroes, `sklearn` automatically stores these as a sparse matrix. It saves **a lot** of space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)\n",
    "\n",
    "cvec = CountVectorizer()\n",
    "\n",
    "X_train = cvec.fit_transform(X_train)\n",
    "\n",
    "print(type(X_train))\n",
    "print(X_train[0])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
