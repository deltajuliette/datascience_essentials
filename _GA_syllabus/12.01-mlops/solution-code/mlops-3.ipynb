{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5297817d-fa17-417b-89a7-4b0853bbd525",
   "metadata": {},
   "source": [
    "# Batch Predictions\n",
    "- In the previous two mlops lessons we implemented a real time prediction endpoint using Flask and Docker\n",
    "- This is great as long as our predictions need to run on a small number of samples every time but require fast processing speed. For example, to publish on Rapid API and users are expected to hit the API with 10 rows of data at a time.\n",
    "- For most applications this is the way to go since you can expect each user to only require predictions for their own data.\n",
    "- The real time APIs can be scaled to support very high concurrency (many users hitting the API at the same time but each user only has few rows to predict) by using technologies like Kubernetes.\n",
    "- However, for some special use cases, especially within Data Engineering, you might need to run model predictions for several thousands or millions of data points at the same time. \n",
    "    - These applications will generally run at a regular cadence, such as once per week or once per day\n",
    "    - They can also tolerate a longer running time. Though the running time should be longer than the interval between 2 runs. For example, if predicting on a million data points takes 2 days to run, we cannot schedule this task to run every day!\n",
    "    \n",
    "<details><summary>Based on your current Python knowledge, how would you run predictions on several rows of data if you can only feed one row of data to the model at a time?</summary>\n",
    "\n",
    "- You would use a `for` loop!\n",
    "- Loop over the rows one by one, feed it to the model, get predictions and save the predictions to a new column\n",
    "![](../images/for_loop.png)\n",
    "</details>\n",
    "\n",
    "<details><summary>If one row takes 1second to get predictions, how long will the above strategy take to complete all the predictions for `N` rows?</summary>\n",
    "\n",
    "- If each row takes 1s and we have total `N` rows, since we feed one row at a time, total time to complete all predicions is `N` seconds!\n",
    "- If `N` is small this is not an issue. However, if `N` is 1 Million, then predictions will take 11.5 days to complete!\n",
    "</details>\n",
    "\n",
    "<details><summary>How can we speed this up?</summary>\n",
    "\n",
    "- Parallelisation!\n",
    "- Instead of setting up one machine running our model, we can setup `N` machines and each machine is fed 1 row of data\n",
    "- All the `N` machines make predictions concurrently\n",
    "- Total prediction time is only 1 second!\n",
    "- This is also known as horizontal scaling\n",
    "- In practice, setting up `N` machines is too expensive especially if `N` is large. So instead we decide how fast we want to the predictions to complete, say 1 day. Then calculate the minimum number of machines needed to complete the predictions within this time. \n",
    "- It's not an exact calculation, so we always add some buffer.\n",
    "- In this way, we feed `batches` of rows into each machine at the same time.\n",
    "![](../images/spark.png)\n",
    "</details>\n",
    "\n",
    "<details><summary>Sounds great! How do we do this?</summary>\n",
    "\n",
    "- As always, there are many ways. All of them have one thing in common: the ability to coordinate execution of code across a `cluster` of machines\n",
    "- Some popular methods are:\n",
    "    1. [Apache Spark](https://spark.apache.org/)\n",
    "    1. [Apache Beam](https://beam.apache.org/)\n",
    "    1. [Dask](https://dask.org/)\n",
    "- Apache Spark is by far the most popular way to do this! Though I personally prefer Apache Beam. In my opinion, Apache Beam will be more popular in the future since its easier to implement than Spark. \n",
    "- Spark is moving in the right direction too! From Spark 3.2 onwards, there is a new feature called [Pandas API on Spark](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html) which is inspired by Dask to make it easier to program in Pyspark using syntax very similar to Pandas! But this feature is still very new, and not many companies would've adopted it yet. So let's implement a Spark program to run model predictions on many images using the traditional way of doing it in Spark\n",
    "</details>\n",
    "\n",
    "<details><summary> How about Kubernetes? </summary>\n",
    "    \n",
    "- Kubernetes is another way to parallelise predictions but it does it slightly differently\n",
    "    - Generally used when you expect a lot (many millions or billions) of users to access your API at the same time. But each user only posts a small number of rows each. Think of Google search. Each user only searches one search at a time, but many billion users are running the search concurrently. The key difference is the parallelisation should provide response in **REALTIME!**\n",
    "    - K8S is still very complicated to manage in production. Typically managed by a dedicated Cloud Engineering team. Almost never involves a Data Scientist.\n",
    "    - DO NOT attempt to manage a K8S cluster without a dedicated Cloud Engineering team!\n",
    "    - For all DS purposes, we just need to wrap our flask app in a Docker container, and it can then run on either Cloud Run (medium traffic) or K8S (extremely large traffic)\n",
    "![](../images/k8s.png)\n",
    "</details>\n",
    "\n",
    "<details><summary>I'm totally confused!</summary>\n",
    "\n",
    "<img src=\"../images/confused_meme.png\" style=\"width: 300px;\"/>\n",
    "    \n",
    "- It's normal to be confused with all the options available out there. \n",
    "- That's why most companies have a dedicated team called *Solution Architect* to design the architecture for each service developed by the company. \n",
    "- Personally, I came up with this simple flowchart to help me decide what technology to use. Hope you find it useful!\n",
    "\n",
    "![](../images/technology_selection.png)\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e75320-de76-4010-b675-dbc9e5ac9e25",
   "metadata": {},
   "source": [
    "# Spark\n",
    "![](../images/spark.png)\n",
    "- Spark is a technology to orchestrate running an application across multiple machines at the same time\n",
    "- Fundamentally, if each row of data can be predicted independent of all the other rows, which is the case for any ML model, then we can theoretically feed each row to a different machine!\n",
    "- In reality, we take our large dataframe and break it up into `N` *partitions*. Each *partition* containing `X` rows.\n",
    "- The rows in each *partition* is then processed by one machine.\n",
    "- This is how we're going to use Spark\n",
    "- Similar to Docker, Spark has tremendous depth and can do a lot of fancy things. But if we stick to the basics as mentioned above, we can achieve our use case of running predicitions on millions of rows without a real time constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448278e5-b8ae-43de-b661-ea550c0b2212",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734a3f79-dbfc-45d1-b9d0-d657d4a2d042",
   "metadata": {},
   "source": [
    "## Create a Spark Cluster running on your local computer!\n",
    "1. This step is only because we do not have access to a real Spark cluster. If you're working in a company, you'd skip this step and directly use your company's Spark cluster instead\n",
    "1. On your Docker desktop, go to Settings -> Resources and set Memory: 4.00 GB \n",
    "1. Install [Docker Compose](https://docs.docker.com/compose/install/)\n",
    "1. We'll be modifying the instructions from this [Github repo](https://github.com/cluster-apps-on-docker/spark-standalone-cluster-on-docker)\n",
    "    ![](../images/cluster-architecture.png)\n",
    "1. Run the following command from terminal: `curl -LO https://raw.githubusercontent.com/cluster-apps-on-docker/spark-standalone-cluster-on-docker/master/docker-compose.yml --ssl-no-revoke`\n",
    "1. It downloads a `docker-compose.yml` file.\n",
    "1. The default setting is to only provide 512MB RAM for each worker node. Let's update it to 4GB. Open the newly downloaded `docker-compose.yml` and update `SPARK_WORKER_MEMORY=4G` and `SPARK_WORKER_CORES=2`\n",
    "1. Let's update the jupyterlab service to forward to port `8889` instead of `8888`\n",
    "1. Your `docker-compose.yml` should now look like this\n",
    "    ```\n",
    "    ---\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    # -- Docs: https://github.com/cluster-apps-on-docker/spark-standalone-cluster-on-docker --\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    version: \"3.6\"\n",
    "    volumes:\n",
    "      shared-workspace:\n",
    "        name: \"hadoop-distributed-file-system\"\n",
    "        driver: local\n",
    "    services:\n",
    "      jupyterlab:\n",
    "        image: andreper/jupyterlab:3.0.0-spark-3.0.0\n",
    "        container_name: jupyterlab\n",
    "        ports:\n",
    "          - 8889:8888\n",
    "          - 4040:4040\n",
    "        volumes:\n",
    "          - shared-workspace:/opt/workspace\n",
    "      spark-master:\n",
    "        image: andreper/spark-master:3.0.0\n",
    "        container_name: spark-master\n",
    "        ports:\n",
    "          - 8080:8080\n",
    "          - 7077:7077\n",
    "        volumes:\n",
    "          - shared-workspace:/opt/workspace\n",
    "      spark-worker-1:\n",
    "        image: andreper/spark-worker:3.0.0\n",
    "        container_name: spark-worker-1\n",
    "        environment:\n",
    "          - SPARK_WORKER_CORES=2\n",
    "          - SPARK_WORKER_MEMORY=4G\n",
    "        ports:\n",
    "          - 8081:8081\n",
    "        volumes:\n",
    "          - shared-workspace:/opt/workspace\n",
    "        depends_on:\n",
    "          - spark-master\n",
    "      spark-worker-2:\n",
    "        image: andreper/spark-worker:3.0.0\n",
    "        container_name: spark-worker-2\n",
    "        environment:\n",
    "          - SPARK_WORKER_CORES=2\n",
    "          - SPARK_WORKER_MEMORY=4G\n",
    "        ports:\n",
    "          - 8082:8081\n",
    "        volumes:\n",
    "          - shared-workspace:/opt/workspace\n",
    "        depends_on:\n",
    "          - spark-master\n",
    "    ...\n",
    "    ```\n",
    "1. Run this command from a terminal to start up the local Spark cluster! `docker-compose up`\n",
    "1. Open JupyterLab running on the Spark Cluster: http://localhost:8889/. \n",
    "1. **Upload this notebook into the Spark Cluster using Jupyter Lab**\n",
    "1. **Upload the model file `cats_vs_dogs.h5` into the Spark Cluster using Jupyter Lab**\n",
    "1. You can check the SparkUI as well: http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8120a9-8727-43e6-a473-106ae53c97e7",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270b396e-787d-4a16-9d34-ba409b6f9c90",
   "metadata": {},
   "source": [
    "# Spark Code\n",
    "- All the code below this line needs to be run from Jupyterlab within your Spark Cluster: http://localhost:8889/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692764f2-9ee8-4da2-a600-d72f1a60bf5f",
   "metadata": {},
   "source": [
    "## Download data on your Spark cluster\n",
    "1. This step is only because we do not have access to a real Spark cluster. If you're working in a company, you'd skip this step and directly use the data already present on your company's Spark cluster instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c94896-5e1d-4f45-b24d-eb6c02a01249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unzip the data\n",
    "! curl https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip --output ./data/cats_and_dogs_filtered.zip\n",
    "! unzip data/cats_and_dogs_filtered.zip -d ./data/\n",
    "! rm data/cats_and_dogs_filtered.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b0ab8-d3de-46ae-bb58-fec42bbdeb7d",
   "metadata": {},
   "source": [
    "## Install the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c76aec-24cc-46c5-904f-0b0fe03e72d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the necessary libraries\n",
    "!pip install pillow pandas tensorflow-cpu pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3562d74b-516f-49b2-b40d-36f51794d79d",
   "metadata": {},
   "source": [
    "## Find the paths of all the image files we want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c761c7b-bf52-4814-862c-7f21962a8eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the paths of all the images\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Path to all the downloaded images\n",
    "img_path = Path('data/cats_and_dogs_filtered')\n",
    "\n",
    "# Find list of all files in the path\n",
    "images = [path for path in img_path.glob('**/*.jpg')]\n",
    "\n",
    "# Load the file names to a dataframe\n",
    "image_df = pd.DataFrame(images, columns=['img_path'])\n",
    "image_df.to_csv('data/images.csv', index=False)\n",
    "\n",
    "print(image_df.shape)\n",
    "image_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff53f032-791e-443e-a1a4-56579c69d565",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a999b00a-bdb0-49c5-862e-80e67fd82f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Spark works in a session. Let's create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# To prevent memory errors\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4501da49-0a8b-4537-82b1-6d22f31fb5fd",
   "metadata": {},
   "source": [
    "## Read the image paths csv file as a Spark Dataframe. \n",
    "- Spark Dataframe is different from a Pandas dataframe but they both share lots of similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8ca29f-cefa-4c1a-a150-64c336125f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.csv('data/images.csv', header=True)\n",
    "\n",
    "# Print number of rows\n",
    "print(spark_df.count())\n",
    "\n",
    "# Equivalent of pandas_df.head()\n",
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812b507-6f09-44d0-a1ff-016d0031c0be",
   "metadata": {},
   "source": [
    "## Repartition the Spark Dataframe\n",
    "- Spark dataframe derives its power by its ability to process chunks of rows in parallel\n",
    "- Each chunk of row is a partition\n",
    "- To ensure we exploit the Spark dataframe's power, let's repartition our dataframe into 10 partitions. You can use more if you have more worker nodes!\n",
    "\n",
    "![](../images/spark.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cda52a-52bd-4793-9f2b-19b467b67a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Initial number of partitions: {spark_df.rdd.getNumPartitions()}')\n",
    "\n",
    "spark_df = spark_df.repartition(10)\n",
    "print(f'Final number of partitions: {spark_df.rdd.getNumPartitions()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ebc3cf-9fbe-465a-9a19-8b533a783720",
   "metadata": {},
   "source": [
    "## Create the PandasUDF (Pandas user defined function)\n",
    "- Spark dataframe provides a special type of operation called `PandasUDF`\n",
    "- PandasUDF allows us to apply any Python function on a Spark dataframe\n",
    "- In this case, we'll apply the data preprocessing and model predictions on the spark dataframe using a PandasUDF\n",
    "- The syntax is a little confusing and documentation is poor. Ends up you need to trial and error a bit to find out how to implement it. The good new is that once you implement for one model, you can reuse the code for any other model! \n",
    "- For more info on PandasUDF, check out this [Link](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6f6916-6197-4a63-b3da-94d2b2abf78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction code\n",
    "@F.pandas_udf('string', F.PandasUDFType.SCALAR_ITER)\n",
    "def predict(image_paths: pd.Series) -> pd.Series:\n",
    "    # Load the model\n",
    "    trf_model = load_model('cats_vs_dogs.h5')\n",
    "    \n",
    "    # Iterate over each Partition\n",
    "    for images in image_paths:\n",
    "        # Load all the images within the partition and preprocess them\n",
    "        test_images = [image.load_img(img, target_size=(224, 224)) for img in images]\n",
    "        test_images = [image.img_to_array(img).tolist() for img in test_images]\n",
    "        test_images = preprocess_input(np.array(test_images))\n",
    "        \n",
    "        # Run predictions\n",
    "        result = trf_model.predict(test_images, batch_size=5)\n",
    "        result = ['Dog' if pred[0]>0.5 else 'Cat' for pred in result]\n",
    "        \n",
    "        # Yield the results as a pandas series\n",
    "        yield pd.Series(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2b90a9-0875-4984-adf6-a32a5f7f5733",
   "metadata": {},
   "source": [
    "## Run the PandasUDF on the Spark dataframe and save results to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0637bce9-8eac-413b-a1c0-00e33407f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pandasUDF on our Spark dataframe\n",
    "spark_df = spark_df.withColumn('prediction', predict('img_path'))\n",
    "\n",
    "# Write results to csv files\n",
    "spark_df.write.csv('output', mode='overwrite', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482e1eaa-e21d-41b5-bc9f-be1bac5049e8",
   "metadata": {},
   "source": [
    "# What's next?\n",
    "- We only ran on 3000 rows. You can run the same code on 300 Million rows by just adding more worker nodes.\n",
    "- Spark is mostly in the realm of Data Engineering.\n",
    "- But as a DS, it's handy to know at least how to run predictions using Spark.\n",
    "- Spark has lots more depth and we only scratched the surface to solve a very specific and narrow problem (model predictions).\n",
    "- If you want to learn more about Spark, recommend this book: https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d0beb2-0337-46eb-bd7e-b93f145befef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
